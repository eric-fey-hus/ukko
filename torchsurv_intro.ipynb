{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "In this notebook, we use `TorchSurv` to train a model that predicts relative risk of breast cancer recurrence. We use a public data set, the [German Breast Cancer Study Group 2 (GBSG2)](https://paperswithcode.com/dataset/gbsg2). After training the model, we evaluate the predictive performance using evaluation metrics implemented in `TorchSurv`.\n",
    "\n",
    "\n",
    "We first load the dataset using the package [lifelines](https://lifelines.readthedocs.io/en/latest/). The GBSG2 dataset contains features and recurrence free survival time (in days) for 686 women undergoing hormonal treatment. \n",
    "\n",
    "### Dependencies\n",
    "\n",
    "To run this notebook, dependencies must be installed. the recommended method is to use our developpment conda environment (**preffered**). Instruction can be found [here](https://opensource.nibr.com/torchsurv/devnotes.html#set-up-a-development-environment-via-conda) to install all optional dependancies. The other method is to install only required packages using the command line below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install only required packages (optional)\n",
    "# %pip install lifelines\n",
    "# %pip install matplotlib\n",
    "# %pip install sklearn\n",
    "# %pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "On local machine (Carbon X1) installed from repo into conda env: _ptorch_:\n",
    "\n",
    "```sh\n",
    "git clone https://github.com/Novartis/torchsurv.git\n",
    "cd torchsurv\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lifelines\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Our package\n",
    "from torchsurv.loss.cox import neg_partial_log_likelihood\n",
    "from torchsurv.loss.weibull import neg_log_likelihood, log_hazard, survival_function\n",
    "from torchsurv.metrics.brier_score import BrierScore\n",
    "from torchsurv.metrics.cindex import ConcordanceIndex\n",
    "from torchsurv.metrics.auc import Auc\n",
    "from torchsurv.stats.kaplan_meier import KaplanMeierEstimator\n",
    "\n",
    "# PyTorch boilerplate - see https://github.com/Novartis/torchsurv/blob/main/docs/notebooks/helpers_introduction.py\n",
    "#from helpers_introduction import Custom_dataset, plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant parameters accross models\n",
    "# Detect available accelerator; Downgrade batch size if only CPU available\n",
    "if any([torch.cuda.is_available(), torch.backends.mps.is_available()]):\n",
    "    print(\"CUDA-enabled GPU/TPU is available.\")\n",
    "    BATCH_SIZE = 128  # batch size for training\n",
    "else:\n",
    "    print(\"No CUDA-enabled GPU found, using CPU.\")\n",
    "    BATCH_SIZE = 64# 32  # batch size for training\n",
    "\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Custom_dataset(Dataset):\n",
    "    \"\"\" \"Custom dataset for the GSBG2 brain cancer dataset\"\"\"\n",
    "\n",
    "    # defining values in the constructor\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "\n",
    "    # Getting data size/length\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    # Getting the data samples\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.df.iloc[idx]\n",
    "        # Targets\n",
    "        event = torch.tensor(sample[\"cens\"]).bool()\n",
    "        time = torch.tensor(sample[\"time\"]).float()\n",
    "        # Predictors\n",
    "        x = torch.tensor(sample.drop([\"cens\", \"time\"]).values).float()\n",
    "        return x, (event, time)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_losses(train_losses, val_losses, title: str = \"Cox\") -> None:\n",
    "\n",
    "    train_losses = torch.stack(train_losses) / train_losses[0]\n",
    "    val_losses = torch.stack(val_losses) / val_losses[0]\n",
    "\n",
    "    plt.plot(train_losses, label=\"training\")\n",
    "    plt.plot(val_losses, label=\"validation\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Normalized loss\")\n",
    "    plt.title(title)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Dataset overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GBSG2 dataset\n",
    "df = lifelines.datasets.load_gbsg2()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "The dataset contains the categorical features: \n",
    "\n",
    "- `horTh`: hormonal therapy, a factor at two levels (yes and no).\n",
    "- `age`:  age of the patients in years.\n",
    "- `menostat`: menopausal status, a factor at two levels pre (premenopausal) and post (postmenopausal).\n",
    "- `tsize`: tumor size (in mm).\n",
    "- `tgrade`: tumor grade, a ordered factor at levels I < II < III.\n",
    "- `pnodes`: number of positive nodes.\n",
    "- `progrec`: progesterone receptor (in fmol).\n",
    "- `estrec`: estrogen receptor (in fmol).\n",
    "\n",
    "Additionally, it contains our survival targets:\n",
    "\n",
    "- `time`: recurrence free survival time (in days).\n",
    "- `cens`: censoring indicator (0- censored, 1- event).\n",
    "\n",
    "One common approach is to use a [one hot encoder](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) to convert them into numerical features. We then seperate the dataframes into features `X` and labels `y`. The following code also partitions the labels and features into training and testing cohorts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onehot = pd.get_dummies(df, columns=[\"horTh\", \"menostat\", \"tgrade\"]).astype(\"float\")\n",
    "df_onehot.drop(\n",
    "    [\"horTh_no\", \"menostat_Post\", \"tgrade_I\"],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")\n",
    "df_onehot.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_onehot, test_size=0.3)\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.3)\n",
    "print(\n",
    "    f\"(Sample size) Training:{len(df_train)} | Validation:{len(df_val)} |Testing:{len(df_test)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Let us setup the dataloaders for training, validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dataloader_train = DataLoader(\n",
    "    Custom_dataset(df_train), batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "dataloader_val = DataLoader(\n",
    "    Custom_dataset(df_val), batch_size=len(df_val), shuffle=False\n",
    ")\n",
    "dataloader_test = DataLoader(\n",
    "    Custom_dataset(df_test), batch_size=len(df_test), shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "x, (event, time) = next(iter(dataloader_train))\n",
    "num_features = x.size(1)\n",
    "\n",
    "print(f\"x (shape)    = {x.shape}\")\n",
    "print(f\"num_features = {num_features}\")\n",
    "print(f\"event        = {event.shape}\")\n",
    "print(f\"time         = {time.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ccompute baseline (for Cox PH)\n",
    "cph = lifelines.CoxPHFitter()\n",
    "cph.fit(df_train, duration_col='time', event_col='cens')\n",
    "#cph.fit(df_train, duration_col='time', event_col='cens', strata=[\"menostat_Pre\"])\n",
    "\n",
    "cph.print_summary()  # print the results\n",
    "cph.plot()  # plot the coefficients\n",
    "\n",
    "cph.baseline_hazard_.plot()\n",
    "cph.baseline_cumulative_hazard_.plot()\n",
    "cph.baseline_survival_.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check proportional hazards assumption\n",
    "cph.check_assumptions(df_train, p_value_threshold=0.05, show_plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Section 1: Cox proportional hazards model\n",
    "\n",
    "In this section, we use the [Cox proportional hazards model](../_autosummary/torchsurv.loss.cox.html). Given covariate $x_{i}$, the hazard of patient $i$ has the form\n",
    "$$\n",
    "\\lambda (t|x_{i}) =\\lambda_{0}(t)\\theta(x_{i})\n",
    "$$\n",
    "The baseline hazard $\\lambda_{0}(t)$ is identical across subjects (i.e., has no dependency on $i$). The subject-specific risk of event occurrence is captured through the relative hazards $\\{\\theta(x_{i})\\}_{i = 1, \\dots, N}$.\n",
    "\n",
    "We train a multi-layer perceptron (MLP) to model the subject-specific risk of event occurrence, i.e., the log relative hazards $\\log\\theta(x_{i})$. Patients with lower recurrence time are assumed to have higher risk of event. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Section 1.1: MLP model for log relative hazards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_model = torch.nn.Sequential(\n",
    "    torch.nn.BatchNorm1d(num_features),  # Batch normalization\n",
    "    torch.nn.Linear(num_features, 32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.Linear(32, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.Linear(64, 1),  # Estimating log hazards for Cox models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Section 1.2: MLP model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Init optimizer for Cox\n",
    "optimizer = torch.optim.Adam(cox_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Initiate empty list to store the loss on the train and validation sets\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = torch.tensor(0.0)\n",
    "    for i, batch in enumerate(dataloader_train):\n",
    "        x, (event, time) = batch\n",
    "        optimizer.zero_grad()\n",
    "        log_hz = cox_model(x)  # shape = (16, 1)\n",
    "        loss = neg_partial_log_likelihood(log_hz, event, time, reduction=\"mean\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach()\n",
    "\n",
    "    if epoch % (EPOCHS // 10) == 0:\n",
    "        print(f\"Epoch: {epoch:03}, Training loss: {epoch_loss:0.2f}\")\n",
    "\n",
    "    # Reccord loss on train and test sets\n",
    "    epoch_loss /= i + 1\n",
    "    train_losses.append(epoch_loss)\n",
    "    with torch.no_grad():\n",
    "        x, (event, time) = next(iter(dataloader_val))\n",
    "        val_losses.append(\n",
    "            neg_partial_log_likelihood(cox_model(x), event, time, reduction=\"mean\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "We can visualize the training and validation losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, \"Cox\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Section 1.3: Cox proportional hazards model evaluation\n",
    "\n",
    "We evaluate the predictive performance of the model using \n",
    "\n",
    "* the [concordance index](../_autosummary/torchsurv.metrics.cindex.html) (C-index), which measures the the probability that a model correctly predicts which of two comparable samples will experience an event first based on their estimated risk scores,\n",
    "* the [Area Under the Receiver Operating Characteristic Curve](../_autosummary/torchsurv.metrics.auc.html) (AUC), which measures the probability that a model correctly predicts which of two comparable samples will experience an event by time t based on their estimated risk scores.\n",
    "\n",
    "We cannot use the Brier score because this model is not able to estimate the survival function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "We start by evaluating the subject-specific relative hazards on the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_model.eval()\n",
    "with torch.no_grad():\n",
    "    # test event and test time of length n\n",
    "    x, (event, time) = next(iter(dataloader_test))\n",
    "    log_hz = cox_model(x)  # log hazard of length n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_hz.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "We obtain the concordance index, and its confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concordance index\n",
    "cox_cindex = ConcordanceIndex()\n",
    "print(\"Cox model performance:\")\n",
    "print(f\"Concordance-index   = {cox_cindex(log_hz, event, time)}\")\n",
    "print(f\"Confidence interval = {cox_cindex.confidence_interval()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "We can also test whether the observed concordance index is greater than 0.5. The statistical test is specified with H0: c-index = 0.5 and Ha: c-index > 0.5. The p-value of the statistical test is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H0: cindex = 0.5, Ha: cindex > 0.5\n",
    "print(\"p-value = {}\".format(cox_cindex.p_value(alternative=\"greater\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "For time-dependent prediction (e.g., 5-year mortality), the C-index is not a proper measure. Instead, it is recommended to use the AUC. The probability to correctly predicts which of two comparable patients will experience an event by 5-year based on their estimated risk scores is the AUC evaluated at 5-year (1825 days) obtained with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_auc = Auc()\n",
    "\n",
    "new_time = torch.tensor(1825.0)\n",
    "\n",
    "# auc evaluated at new time = 1825, 5 year\n",
    "print(f\"AUC 5-yr             = {cox_auc(log_hz, event, time, new_time=new_time)}\")\n",
    "print(f\"AUC 5-yr (conf int.) = {cox_auc.confidence_interval()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "As before, we can test whether the observed Auc at 5-year is greater than 0.5. The statistical test is specified with H0: auc = 0.5 and Ha: auc > 0.5. The p-value of the statistical test is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"AUC (p_value) = {cox_auc.p_value()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## Section 2: Weibull accelerated failure time (AFT) model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "In this section, we use the [Weibull accelerated failure (AFT) model](../_autosummary/torchsurv.loss.weibull.html). Given covariate $x_{i}$, the hazard of patient $i$ at time $t$ has the form\n",
    "$$\n",
    "\\lambda (t|x_{i}) = \\frac{\\rho(x_{i}) } {\\lambda(x_{i}) } + \\left(\\frac{t}{\\lambda(x_{i})}\\right)^{\\rho(x_{i}) - 1}\n",
    "$$\n",
    "\n",
    "Given the hazard form, it can be shown that the event density follows a Weibull distribution parametrized by scale $\\lambda(x_{i})$ and shape $\\rho(x_{i})$. The subject-specific risk of event occurrence at time $t$ is captured through the hazards $\\{\\lambda (t|x_{i})\\}_{i = 1, \\dots, N}$. We train a multi-layer perceptron (MLP) to model the subject-specific log scale, $\\log \\lambda(x_{i})$, and the log shape, $\\log\\rho(x_{i})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### Section 2.1: MLP model for log scale and log shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same architecture than Cox model, beside outputs dimension\n",
    "weibull_model = torch.nn.Sequential(\n",
    "    torch.nn.BatchNorm1d(num_features),  # Batch normalization\n",
    "    torch.nn.Linear(num_features, 32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.Linear(32, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.Linear(64, 2),  # Estimating log parameters for Weibull model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### Section 2.2: MLP model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Init optimizer for Weibull\n",
    "optimizer = torch.optim.Adam(weibull_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Initialize empty list to store loss on train and validation sets\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = torch.tensor(0.0)\n",
    "    for i, batch in enumerate(dataloader_train):\n",
    "        x, (event, time) = batch\n",
    "        optimizer.zero_grad()\n",
    "        log_params = weibull_model(x)  # shape = (16, 2)\n",
    "        loss = neg_log_likelihood(log_params, event, time, reduction=\"mean\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach()\n",
    "\n",
    "    if epoch % (EPOCHS // 10) == 0:\n",
    "        print(f\"Epoch: {epoch:03}, Training loss: {epoch_loss:0.2f}\")\n",
    "\n",
    "    # Reccord losses for the following figure\n",
    "    train_losses.append(epoch_loss)\n",
    "    with torch.no_grad():\n",
    "        x, (event, time) = next(iter(dataloader_val))\n",
    "        val_losses.append(\n",
    "            neg_log_likelihood(weibull_model(x), event, time, reduction=\"mean\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "We can visualize the training and validation losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, \"Weibull\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### Section 2.3: Weibull AFT model evaluation\n",
    "\n",
    "We evaluate the predictive performance of the model using \n",
    "\n",
    "* the [C-index](../_autosummary/torchsurv.metrics.cindex.html), which measures the the probability that a model correctly predicts which of two comparable samples will experience an event first based on their estimated risk scores,\n",
    "* the [AUC](../_autosummary/torchsurv.metrics.auc.html), which measures the probability that a model correctly predicts which of two comparable samples will experience an event by time t based on their estimated risk scores, and\n",
    "* the [Brier score](../_autosummary/torchsurv.metrics.brier_score.html), which measures the model's calibration by calculating the mean square error between the estimated survival function and the empirical (i.e., in-sample) event status."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "We start by obtaining the subject-specific log hazard and survival probability at every time $t$ observed on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "weibull_model.eval()\n",
    "with torch.no_grad():\n",
    "    # event and time of length n\n",
    "    x, (event, time) = next(iter(dataloader_test))\n",
    "    log_params = weibull_model(x)  # shape = (n,2)\n",
    "\n",
    "# Compute the log hazards from weibull log parameters\n",
    "log_hz = log_hazard(log_params, time)  # shape = (n,n)\n",
    "\n",
    "# Compute the survival probability from weibull log parameters\n",
    "surv = survival_function(log_params, time)  # shape = (n,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "We can evaluate the concordance index, its confidence interval and the p-value of the statistical test testing whether the c-index is greater than 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "time2 = [0,1,2,5, 10,20, 50, 100]\n",
    "time2 = range(0,501,1)\n",
    "#surv2 = survival_function(log_params, time2, all_times = False) \n",
    "surv2 = torch.empty(206)\n",
    "surv_list = []\n",
    "for t in time2:\n",
    "    survt = survival_function(log_params, time = torch.tensor(t))\n",
    "    #print(survt.shape)\n",
    "    surv_list.append(survt.unsqueeze(1))\n",
    "\n",
    "surv2 = torch.cat(surv_list, dim=1).transpose(0, 1)\n",
    "print(surv2.shape)\n",
    "#display(surv2)\n",
    "#plt.plot(time, surv2);\n",
    "for i in range(x.shape[1]):\n",
    "    plt.step(time2, surv2[:,i].numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concordance index\n",
    "weibull_cindex = ConcordanceIndex()\n",
    "print(\"Weibull model performance:\")\n",
    "print(f\"Concordance-index   = {weibull_cindex(log_hz, event, time)}\")\n",
    "print(f\"Confidence interval = {weibull_cindex.confidence_interval()}\")\n",
    "\n",
    "# H0: cindex = 0.5, Ha: cindex >0.5\n",
    "print(f\"p-value             = {weibull_cindex.p_value(alternative = 'greater')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "For time-dependent prediction (e.g., 5-year mortality), the C-index is not a proper measure. Instead, it is recommended to use the AUC. The probability to correctly predicts which of two comparable patients will experience an event by 5-year based on their estimated risk scores is the AUC evaluated at 5-year (1825 days) obtained with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_time = torch.tensor(1825.0)\n",
    "\n",
    "# subject-specific log hazard at \\5-yr\n",
    "log_hz_t = log_hazard(log_params, time=new_time)  # shape = (n)\n",
    "weibull_auc = Auc()\n",
    "\n",
    "# auc evaluated at new time = 1825, 5 year\n",
    "print(f\"AUC 5-yr             = {weibull_auc(log_hz_t, event, time, new_time=new_time)}\")\n",
    "print(f\"AUC 5-yr (conf int.) = {weibull_auc.confidence_interval()}\")\n",
    "print(f\"AUC 5-yr (p value)   = {weibull_auc.p_value(alternative='greater')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "Lastly, we can evaluate the time-dependent Brier score and the integrated Brier score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "brier_score = BrierScore()\n",
    "\n",
    "# brier score at first 5 times\n",
    "print(f\"Brier score             = {brier_score(surv, event, time)[:5]}\")\n",
    "print(f\"Brier score (conf int.) = {brier_score.confidence_interval()[:,:5]}\")\n",
    "\n",
    "# integrated brier score\n",
    "print(f\"Integrated Brier score  = {brier_score.integral()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "We can test whether the time-dependent Brier score is smaller than what would be expected if the survival model was not providing accurate predictions beyond random chance. We use a bootstrap permutation test and obtain the p-value with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H0: bs = bs0, Ha: bs < bs0; where bs0 is the expected brier score if the survival model was not providing accurate predictions beyond random chance.\n",
    "\n",
    "# p-value for brier score at first 5 times\n",
    "print(f\"Brier score (p-val)        = {brier_score.p_value(alternative = 'less')[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "## Section 3: Models comparison\n",
    "\n",
    "We can compare the predictive performance of the Cox proportional hazards model against the Weibull AFT model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "### Section 3.1: Concordance index\n",
    "The statistical test is formulated as follows, H0: cindex cox = cindex weibull, Ha: cindex cox > cindex weibull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cox cindex     = {cox_cindex.cindex}\")\n",
    "print(f\"Weibull cindex = {weibull_cindex.cindex}\")\n",
    "print(\"p-value        = {}\".format(cox_cindex.compare(weibull_cindex)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### Section 3.2: AUC at 5-year\n",
    "\n",
    "The statistical test is formulated as follows, H0: 5-yr auc cox = 5-yr auc weibull, Ha: 5-yr auc cox > 5-yr auc weibull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cox 5-yr AUC     = {cox_auc.auc}\")\n",
    "print(f\"Weibull 5-yr AUC = {weibull_auc.auc}\")\n",
    "print(\"p-value          = {}\".format(cox_auc.compare(weibull_auc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "## Section 4: Kaplan Meier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Kaplan-Meier estimator\n",
    "km = KaplanMeierEstimator()\n",
    "\n",
    "# Use our observed testing dataset\n",
    "event = torch.tensor(df_test[\"cens\"].values).bool()\n",
    "time = torch.tensor(df_test[\"time\"].values)\n",
    "\n",
    "# Compute the estimator\n",
    "km(event, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot estimate\n",
    "km.plot_km()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the survival values at each time step\n",
    "km.print_survival_table()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
