{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchsurf ukko AML model\n",
    "\n",
    "Notebook to develop the torchsurf-ukko model with AML data\n",
    "\n",
    "Kernals to use:\n",
    "\n",
    "- Carbon X1:  pytorch\n",
    "- HUS Dell: \n",
    "- CSC: Python 3 (ipykernel)\n",
    "- ecare4meb2 ML: python 3.10 pytorch and tensorflow (seems to be brocken on gpu, torch.cuda not installed correctly)\n",
    "\n",
    "Note on Acamedic ML environments\n",
    "- azureml has torch with cuda and works:\n",
    "    - CUDA Driver Version: 12.4\n",
    "    - torch 2.6.0\n",
    "- azureml_py38_PT_TF was broken. fixed with:\n",
    "  - First, install torchsurv : `conda install conda-forge::torchsurv`.  \n",
    "    This will break torch.\n",
    "  - Then reinstall the correct torch and cuda:\n",
    "    ```sh\n",
    "    pip3 install --force-reinstall torch torchvision totoraudio --index-url https://download.pytorch.org/whl/cu124\n",
    "    ```\n",
    "  - For ukko development you also need pytest: `conda install pytest`\n",
    "  - And for runnign ukko:\n",
    "    - `conda install anaconda::scikit-learn`\n",
    "    - `conda install conda-forge::lifelines`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if torcha nd cuda is availabe and wich version\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Driver Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import ukko \n",
    "import importlib\n",
    "# For preprocessing\n",
    "#print(\"Loading sklearn\")\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn_pandas import DataFrameMapper \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Our package\n",
    "from torchsurv.loss.cox import neg_partial_log_likelihood\n",
    "from torchsurv.loss.weibull import neg_log_likelihood, log_hazard, survival_function\n",
    "from torchsurv.metrics.brier_score import BrierScore\n",
    "from torchsurv.metrics.cindex import ConcordanceIndex\n",
    "from torchsurv.metrics.auc import Auc\n",
    "#from torchsurv.stats.kaplan_meier import KaplanMeierEstimator\n",
    "\n",
    "print(\"Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For interactive plots execute:\n",
    "\n",
    "In jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In VSCode, JupyterLab:\n",
    "\n",
    "- you need to have `ipykernel` installed, eg. from conda-forge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #conda install conda-forge::ipykernal \n",
    "\n",
    "# # Import and configure interactive plotting\n",
    "# import ipywidgets as widgets\n",
    "# from IPython.display import display\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Enable interactive mode\n",
    "# %matplotlib widget\n",
    "\n",
    "# # Test widget functionality\n",
    "# def test_interactive():\n",
    "#     slider = widgets.FloatSlider(\n",
    "#         value=0.4,\n",
    "#         min=0.0,\n",
    "#         max=1.0,\n",
    "#         step=0.01,\n",
    "#         description='Test:',\n",
    "#         continuous_update=False\n",
    "#     )\n",
    "#     display(slider)\n",
    "#     return \"Widget test complete\"\n",
    "\n",
    "# test_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "datapath = \"../../HUS-OMOP/AML_model/data/\"\n",
    "datafile = \"df_xy_all_omop_2025_01_21.csv\"\n",
    "os.listdir(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load tidy data\n",
    "print(\"Loading AML data\")\n",
    "df_xy = pd.read_csv(datapath + datafile)\n",
    "print(f\"Data: {df_xy.shape}\")\n",
    "display(df_xy.head(5))\n",
    "\n",
    "# create train, validation and test datasets: IMPUTE nan: -1\n",
    "df_train = df_xy.fillna(-1)\n",
    "df_test = df_train.sample(n=250, random_state=42)\n",
    "df_train = df_train.drop(df_test.index)\n",
    "df_val = df_train.sample(n=250, random_state=42)\n",
    "df_train = df_train.drop(df_val.index)\n",
    "\n",
    "print(f\"Train: {df_train.shape}\")\n",
    "print(f\"Val  : {df_val.shape}\")\n",
    "print(f\"Test : {df_test.shape}\")\n",
    "\n",
    "display(df_train.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load tidy data\n",
    "# print(\"Loading tidy data\")\n",
    "# df_xy = pd.read_csv(\"data/df_xy_synth_v1.csv\")\n",
    "\n",
    "# # create train, validation and test datasets: IMPUTE nan: -1\n",
    "# df_train = df_xy.fillna(-1)\n",
    "# df_test = df_train.sample(n=200, random_state=42)\n",
    "# df_train = df_train.drop(df_test.index)\n",
    "# df_val = df_train.sample(n=200, random_state=42)\n",
    "# df_train = df_train.drop(df_val.index)\n",
    "\n",
    "# print(f\"Train: {df_train.shape}\")\n",
    "# print(f\"Val  : {df_val.shape}\")\n",
    "# print(f\"Test : {df_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up for torchsurv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Detect available accelerator; Downgrade batch size if only CPU available\n",
    "if any([torch.cuda.is_available(), torch.backends.mps.is_available()]):\n",
    "    print(\"CUDA-enabled GPU/TPU is available.\")\n",
    "    BATCH_SIZE = 128  # batch size for training\n",
    "else:\n",
    "    print(\"No CUDA-enabled GPU found, using CPU.\")\n",
    "    BATCH_SIZE = 128# 32  # batch size for training\n",
    "\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class torchsurv_dataset(Dataset):\n",
    "    \"\"\" \"Custom dataset for torcsurv use in df format\"\"\"\n",
    "\n",
    "    # defining values in the constructor\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "\n",
    "    # Getting data size/length\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    # Getting the data samples\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.df.iloc[idx]\n",
    "        # Targets\n",
    "        event = torch.tensor(sample[\"OSS_status\"]).bool()\n",
    "        time = torch.tensor(sample[\"OSS_days\"]).float()\n",
    "        # Predictors\n",
    "        x = torch.tensor(sample.drop([\"person_id\", \"OSS_status\", \"OSS_days\"]).values).float()\n",
    "        return x, (event, time)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_losses(train_losses, val_losses, title: str = \"Cox\") -> None:\n",
    "\n",
    "    train_losses = torch.stack(train_losses) / train_losses[0]\n",
    "    val_losses = torch.stack(val_losses) / val_losses[0]\n",
    "\n",
    "    plt.plot(train_losses, label=\"training\")\n",
    "    plt.plot(val_losses, label=\"validation\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Normalized loss\")\n",
    "    plt.title(title)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dataloader_train = DataLoader(\n",
    "    torchsurv_dataset(df_train), batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "dataloader_val = DataLoader(\n",
    "    torchsurv_dataset(df_val), batch_size=len(df_val), shuffle=False\n",
    ")\n",
    "dataloader_test = DataLoader(\n",
    "    torchsurv_dataset(df_test), batch_size=len(df_test), shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "x, (event, time) = next(iter(dataloader_train))\n",
    "num_features = x.size(1)\n",
    "\n",
    "print(f\"x (shape)    = {x.shape}\")\n",
    "print(f\"num_features = {num_features}\")\n",
    "print(f\"event        = {event.shape}\")\n",
    "print(f\"time         = {time.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Artificial testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ukko.data)\n",
    "\n",
    "#Note: Move this class to ukko.data later\n",
    "class SineWaveDatasetSurvival(ukko.data.SineWaveDataset):\n",
    "    def __init__(self, n_samples, weibull_shape=1, weibull_scale=1, seed=42):\n",
    "        \"\"\"\n",
    "        Creates sine wave dataset with survival times based on first feature's frequency, amplidue, or phase\n",
    "        \n",
    "        Args:\n",
    "            n_samples: Number of samples in dataset\n",
    "            weibull_shape: Shape parameter (k) for Weibull distribution\n",
    "            weibull_scale: Scale parameter (lambda) for Weibull distribution\n",
    "            seed: Random seed\n",
    "        \"\"\"\n",
    "        # Init parent class with fixed parameters\n",
    "        super().__init__(\n",
    "            n_samples=n_samples,\n",
    "            n_features=1,\n",
    "            sequence_length=10,\n",
    "            prediction_length=1,\n",
    "            base_freq=0.1,\n",
    "            noise_level=0.0,\n",
    "            seed=seed\n",
    "        )\n",
    "        \n",
    "        # Generate Weibull distributed survival times based on first feature frequency\n",
    "        np.random.seed(seed)\n",
    "        base_times = np.random.weibull(weibull_shape, n_samples) * weibull_scale\n",
    "        \n",
    "        # Scale times based on first feature's frequency. \n",
    "        # Note:\n",
    "        #   - 1-p Weibull distribution is obtained: X = (-ln(U))^(1/k), where U is uniform [0,1] and k is the shape parameter.\n",
    "        #   - 2-p Weibull inlcuding scale lambda is then: X = lambda * (-ln(U))^(1/k)\n",
    "        freq_0 = np.array(self.f1f)  # Get frequencies of first feature, each smaple should havea a different frequency, but does not.\n",
    "        A_samples = np.array(self.f1A) \n",
    "        #self.survival_times = np.multiply(base_times, A_samples)\n",
    "        #self.survival_times = np.multiply(base_times, 2+self.groundtruth[:,0,1].numpy())\n",
    "        #self.survival_times = 2+self.groundtruth[:,0,1].numpy()\n",
    "        # make survival times deterministic:\n",
    "        self.survival_times = 10*A_samples\n",
    "        print(A_samples)\n",
    "        \n",
    "        # Generate random censoring\n",
    "        # self.censoring = np.random.binomial(1, 0.3, n_samples)  # 30% censoring\n",
    "        # Generate censoring (30% censored)\n",
    "        self.events = np.random.binomial(1, 0.7, n_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, _ = super().__getitem__(idx)\n",
    "        event = torch.tensor(self.events[idx]).bool()\n",
    "        time = torch.tensor(self.survival_times[idx]).float()\n",
    "        return x, (event, time)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Create dataset\n",
    "    dataset = SineWaveDatasetSurvival(\n",
    "        n_samples=500,\n",
    "        weibull_shape=5.0,\n",
    "        weibull_scale=10.0\n",
    "    )\n",
    "    \n",
    "    # Get first sample\n",
    "    x, (censoring, time) = dataset[0]\n",
    "    \n",
    "    # Plot example\n",
    "    fig = plt.figure(figsize=(15, 4))\n",
    "    \n",
    "    # Plot features\n",
    "    plt.subplot(121)\n",
    "    for f in range(dataset.n_features):\n",
    "        plt.plot(x[f], label=f'Feature {f}')\n",
    "    plt.title('Features')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot survival time distribution\n",
    "    plt.subplot(122)\n",
    "    plt.hist(dataset.survival_times, bins=20)\n",
    "    plt.title('Survival Time Distribution')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create artifical datasets for testing\n",
    "train_dataset = SineWaveDatasetSurvival(n_samples=500, seed=42)\n",
    "val_dataset = SineWaveDatasetSurvival(n_samples=500, seed=43)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Verify the data\n",
    "print(\"Training samples:\", len(train_dataset))\n",
    "print(\"Validation samples:\", len(val_dataset))\n",
    "\n",
    "# Check first batch\n",
    "x, (event, time) = next(iter(train_loader))\n",
    "print(\"\\nFirst batch shapes:\")\n",
    "print(f\"Features shape: {x.shape}\")\n",
    "print(f\"Events shape: {event.shape}\")\n",
    "print(f\"Times shape: {time.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.groundtruth[0,0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model coxPH from lifelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Define functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_risk_groups(model, data_loader, device='cpu'):\n",
    "    \"\"\"Get risk scores for all patients and split into high/low risk groups\"\"\"\n",
    "    model.eval()\n",
    "    all_risks = []\n",
    "    all_times = []\n",
    "    all_events = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, durations, events in data_loader:\n",
    "            x = x.float().to(device)\n",
    "            hazard_ratios = model(x)\n",
    "            # Ensure 1D arrays\n",
    "            all_risks.append(hazard_ratios.cpu().numpy().flatten())\n",
    "            all_times.append(durations.numpy().flatten())\n",
    "            all_events.append(events.numpy().flatten())\n",
    "    \n",
    "    # Concatenate all predictions\n",
    "    risk_scores = np.concatenate(all_risks)\n",
    "    times = np.concatenate(all_times)\n",
    "    events = np.concatenate(all_events)\n",
    "    \n",
    "    # Split into high/low risk groups using median\n",
    "    median_risk = np.median(risk_scores)\n",
    "    high_risk = risk_scores >= median_risk\n",
    "    \n",
    "    return risk_scores, times, events, high_risk\n",
    "\n",
    "from lifelines import KaplanMeierFitter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_risk_stratification(times, events, high_risk, title=\"Risk Stratification\"):\n",
    "    \"\"\"Plot Kaplan-Meier curves for high and low risk groups\"\"\"\n",
    "    \n",
    "    # Initialize KM estimator\n",
    "    kmf1 = KaplanMeierFitter()\n",
    "    kmf2 = KaplanMeierFitter()\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot high risk group\n",
    "    mask = high_risk.astype(bool)  # Ensure boolean mask\n",
    "    kmf1.fit(times[mask], events[mask], label='High Risk')\n",
    "    kmf1.plot_survival_function(show_censors = True, censor_styles={'marker': 'x', 'ms': 15})\n",
    "    \n",
    "    # Plot low risk group\n",
    "    mask = ~high_risk.astype(bool)  # Ensure boolean mask\n",
    "    kmf2.fit(times[mask], events[mask], label='Low Risk')\n",
    "    kmf2.plot_survival_function(show_censors = True, censor_styles={'marker': 'x', 'ms': 15})\n",
    "    \n",
    "    # Add at-risk counts\n",
    "    lifelines.plotting.add_at_risk_counts(kmf1, kmf2)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time (days)')\n",
    "    plt.ylabel('Survival Probability')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Add log-rank test\n",
    "    from lifelines.statistics import logrank_test\n",
    "    log_rank = logrank_test(times[high_risk], times[~high_risk],\n",
    "                           events[high_risk], events[~high_risk])\n",
    "    plt.text(0.05, 0.05, f'Log-rank p-value: {log_rank.p_value:.3e}',\n",
    "             transform=plt.gca().transAxes)\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lifelines\n",
    "from lifelines import CoxPHFitter\n",
    "cph = CoxPHFitter()\n",
    "cph.fit(df_train.drop(columns = [\"person_id\"]), duration_col='OSS_days', event_col='OSS_status')\n",
    "\n",
    "#cph.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df_train.shape)\n",
    "risk = cph.predict_partial_hazard(df_train.drop(columns = [\"person_id\"]))\n",
    "print (risk)\n",
    "print (np.median(risk))\n",
    "high_risk = risk >= np.median(risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "risk_threshholds = np.quantile(risk, [0.333, 0.666])\n",
    "risk_strat = (risk > risk_threshholds[0]).astype(int) + (risk > risk_threshholds[1]).astype(int)\n",
    "risk_strat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plot_risk_stratification(df_train['OSS_days'], df_train['OSS_status'], high_risk, \n",
    "                             title=\"CPH from lifelines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Classical loghazard model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cox_model = torch.nn.Sequential(\n",
    "    torch.nn.BatchNorm1d(num_features),  # Batch normalization\n",
    "    torch.nn.Linear(num_features, 32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.Linear(32, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(),\n",
    "    torch.nn.Linear(64, 1),  # Estimating log hazards for Cox models\n",
    ")\n",
    "cox_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Init optimizer for Cox\n",
    "optimizer = torch.optim.Adam(cox_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Initiate empty list to store the loss on the train and validation sets\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = torch.tensor(0.0)\n",
    "    for i, batch in enumerate(dataloader_train):\n",
    "        x, (event, time) = batch\n",
    "        optimizer.zero_grad()\n",
    "        log_hz = cox_model(x)  # shape = (16, 1)\n",
    "        loss = neg_partial_log_likelihood(log_hz, event, time, reduction=\"mean\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach()\n",
    "\n",
    "    #if epoch % (EPOCHS // 10) == 0:\n",
    "    #    print(f\"Epoch: {epoch:03}, Training loss: {epoch_loss:0.2f}\")\n",
    "\n",
    "    # Reccord loss on train and test sets\n",
    "    train_losses.append(epoch_loss)\n",
    "    with torch.no_grad():\n",
    "        x, (event, time) = next(iter(dataloader_val))\n",
    "        epoch_val_loss = neg_partial_log_likelihood(cox_model(x), event, time, reduction=\"mean\")\n",
    "        val_losses.append(\n",
    "          epoch_val_loss\n",
    "        )\n",
    "\n",
    "    if epoch % (EPOCHS // 10) == 0:\n",
    "        print(f\"Epoch: {epoch:03}, Training loss: {epoch_loss:0.2f}. Validation loss: {epoch_val_loss:0.2f}\")\n",
    "\n",
    "    epoch_loss /= i + 1\n",
    "    \n",
    "\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, \"Cox\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: ukko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importlib.reload(ukko)\n",
    "importlib.reload(ukko.utils)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Test:\n",
    "\n",
    "# Convert AML data to multiindex df\n",
    "df_x, data_3d = ukko.utils.convert_to_3d_df(df_train.iloc[:,3:].fillna(-1))\n",
    "df_y = df_train.iloc[:,:3]\n",
    "#display(df_x)\n",
    "#display(df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_x.columns.get_level_values(0).to_frame().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ukkosurv_dataset(Dataset):\n",
    "    \"\"\" \"Custom dataset for ukko-torcsurv use in df format\"\"\"\n",
    "\n",
    "    # defining values in the constructor\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        #self.df = df\n",
    "        df_x, data_3d = ukko.utils.convert_to_3d_df(df.iloc[:,3:].fillna(-1))\n",
    "        df_y = df.iloc[:,:3]\n",
    "        \n",
    "        self.df_y = df_y        # Dataframe with survival data, e.g. OSS_status, OSS_days\n",
    "        self.data_3d = data_3d  # numpy array with 3D feature data: patients, features, time \n",
    "\n",
    "\n",
    "    # Getting data size/length\n",
    "    def __len__(self):\n",
    "        return len(self.data_3d)\n",
    "\n",
    "    # Getting the data samples\n",
    "    def __getitem__(self, idx):\n",
    "        y = self.df_y.iloc[idx,:]\n",
    "        # Targets\n",
    "        event = torch.tensor(y[\"OSS_status\"]).bool()\n",
    "        time = torch.tensor(y[\"OSS_days\"]).float()\n",
    "        # Predictors\n",
    "        x = torch.tensor(self.data_3d[idx,:,:]).float()\n",
    "        return x, (event, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "BATCH_SIZE = 600\n",
    "BATCH_SIZE = 512#len(df_train)\n",
    "dataloader_train = DataLoader(\n",
    "    ukkosurv_dataset(df_train.sample(512)), batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "dataloader_val = DataLoader(\n",
    "    ukkosurv_dataset(df_val), batch_size=len(df_val), shuffle=False\n",
    ")\n",
    "dataloader_test = DataLoader(\n",
    "    ukkosurv_dataset(df_test), batch_size=len(df_test), shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#help(dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "x, (event, time) = next(iter(dataloader_train))\n",
    "num_features, num_timepoints = x.size(1), x.size(2)\n",
    "\n",
    "print(f\"x (shape)      = {x.shape}\")\n",
    "print(f\"num_features   = {num_features}\")\n",
    "print(f\"num_timepoints = {num_timepoints}\")\n",
    "print(f\"event          = {event.shape}\")\n",
    "print(f\"time           = {time.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.1 Artifical dataset for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create artifical datasets for testing\n",
    "train_dataset = SineWaveDatasetSurvival(n_samples=500, seed=42)\n",
    "val_dataset = SineWaveDatasetSurvival(n_samples=500, seed=43)\n",
    "\n",
    "# Asing paramters for model accordingly:\n",
    "num_features   = train_dataset.n_features\n",
    "num_timepoints = train_dataset.sequence_length\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 500\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Verify the data\n",
    "print(\"Training samples:\", len(train_dataset))\n",
    "print(\"Validation samples:\", len(val_dataset))\n",
    "\n",
    "# Check first batch\n",
    "x, (event, time) = next(iter(dataloader_train))\n",
    "print(\"\\nFirst batch shapes:\")\n",
    "print(f\"Features shape: {x.shape}\")\n",
    "print(f\"Events shape: {event.shape}\")\n",
    "print(f\"Times shape: {time.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importlib.reload(ukko.core)\n",
    "\n",
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del optimizer\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "# Initialize model\n",
    "# DualAttentionRegressor1(self, n_features, time_steps, d_model=128, n_heads=8, dropout=0.1, n_modules=1)\n",
    "model = ukko.core.DualAttentionRegressor1(\n",
    "    n_features=num_features,\n",
    "    time_steps=num_timepoints,\n",
    "    d_model=16,\n",
    "    n_heads=4,\n",
    "    dropout=0.2,\n",
    "    n_modules=2\n",
    ")\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sanity check:\n",
    "\n",
    "def sanity_check(model, dataloader):\n",
    "    \"\"\"\n",
    "    Perform basic model sanity checks\n",
    "    \"\"\"\n",
    "    # Get a single batch\n",
    "    batch, (event, time) = next(iter(dataloader))\n",
    "    \n",
    "    print(\"Input shapes:\")\n",
    "    print(f\"Batch: {batch.shape}\")\n",
    "    print(f\"Event: {event.shape}\")\n",
    "    print(f\"Time: {time.shape}\")\n",
    "    \n",
    "    # Run forward pass\n",
    "    try:\n",
    "        predictions, feat_attn, time_attn = model(batch)\n",
    "        print(\"\\nOutput shapes:\")\n",
    "        print(f\"Predictions: {predictions.shape}\")\n",
    "        print(f\"Feature attention: {feat_attn.shape}\")\n",
    "        print(f\"Time attention: {time_attn.shape}\")\n",
    "        \n",
    "        print(\"\\nValue ranges:\")\n",
    "        print(f\"Predictions min/max: {predictions.min():.3f}/{predictions.max():.3f}\")\n",
    "        print(f\"Feature attention min/max: {feat_attn.min():.3f}/{feat_attn.max():.3f}\")\n",
    "        print(f\"Time attention min/max: {time_attn.min():.3f}/{time_attn.max():.3f}\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error during forward pass: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Run sanity check\n",
    "success = sanity_check(model, dataloader_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = sum(p.numel() for p in model.parameters())\n",
    "model_memory = model_parameters * 4 / (1024 ** 2)  # in MB\n",
    "print(f\"Model_memory {model_memory} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_model_parameters(model):\n",
    "    \"\"\"Analyze model parameters and their shapes\"\"\"\n",
    "    total_params = 0\n",
    "    print(\"Model Parameter Analysis:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Layer':<50} {'Shape':<20} {'Parameters':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        total_params += num_params\n",
    "        print(f\"{name:<50} {str(list(param.shape)):<20} {num_params:<10,d}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Total Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Run the analysis\n",
    "analyze_model_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importlib.reload(ukko.core)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "EPOCHS = 200\n",
    "LEARNING_RATE = 1e-3\n",
    "    \n",
    "# Initialize model\n",
    "model = ukko.core.DualAttentionRegressor1(\n",
    "    n_features=num_features,\n",
    "    time_steps=num_timepoints,\n",
    "    d_model=8,\n",
    "    n_heads=4,\n",
    "    dropout=0.2,\n",
    "    n_modules=2,\n",
    "    n_kv_heads=2 \n",
    ")\n",
    "\n",
    "BATCH_SIZE = 128#len(df_train)\n",
    "dataloader_train = DataLoader(\n",
    "    ukkosurv_dataset(df_train.sample(512)), batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "# Init optimizer for Cox\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Initiate empty list to store the loss on the train and validation sets\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Get device and move model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = torch.tensor(0.0)\n",
    "    for i, batch in enumerate(dataloader_train):\n",
    "        x, (event, time) = batch\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        log_hz, feature_weights, time_weights = model(x)  # shape = (batchsize, 1)\n",
    "        loss = neg_partial_log_likelihood(log_hz.to(device), event.to(device), time.to(device), reduction=\"mean\")\n",
    "        #print(loss.is_cuda)\n",
    "        #print(epoch_loss.is_cuda)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().to(\"cpu\")\n",
    "\n",
    "    # Reccord loss on train and test sets\n",
    "    epoch_loss /= i + 1\n",
    "    train_losses.append(epoch_loss)\n",
    "    with torch.no_grad():\n",
    "        x, (event, time) = next(iter(dataloader_val))\n",
    "        x = x.to(device)\n",
    "        log_hz, feature_weights, time_weights = model(x)\n",
    "        val_losses.append(\n",
    "            neg_partial_log_likelihood(log_hz, event, time, reduction=\"mean\").detach().to(\"cpu\")\n",
    "        )\n",
    "\n",
    "    # Display progress\n",
    "    if epoch % 5 == 0: #(EPOCHS // 5) == 0:\n",
    "        print(f\"Epoch: {epoch:03}, Training loss: {train_losses[-1]:0.2f}, Validation loss: {val_losses[-1]:0.2f}\")\n",
    "\n",
    "plot_losses(train_losses, val_losses, \"Cox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, \"Cox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.to(\"cpu\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # test event and test time of length n\n",
    "    x, (event, time) = next(iter(dataloader_train))\n",
    "    x = x.to(\"cpu\")\n",
    "    log_hz, feature_weights, time_weights = model(x)  # log hazard of length n\n",
    "\n",
    "# Concordance index\n",
    "cox_cindex = ConcordanceIndex()\n",
    "print(\"Cox model performance:\")\n",
    "print(f\"Concordance-index   = {cox_cindex(log_hz, event, time)}\")\n",
    "print(f\"Confidence interval = {cox_cindex.confidence_interval()}\")\n",
    "\n",
    "# plot log hazard vs time\n",
    "plt.figure(figsize=(10, 5))\n",
    "# change to scatter plot and color by event \n",
    "plt.scatter(time, log_hz, c=event, cmap='coolwarm', alpha=0.5)\n",
    "plt.xlabel(\"Surival Time\")\n",
    "plt.ylabel(\"Log hazard\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, \"Cox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk = log_hz.detach().cpu().numpy()\n",
    "risk_threshholds = np.quantile(risk, [0.333, 0.666])\n",
    "risk_threshholds = np.quantile(risk, [0.5, 1])\n",
    "risk_strat = (risk > risk_threshholds[0]).astype(int) + (risk  > risk_threshholds[1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cpu\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # test event and test time of length n\n",
    "    x = torch.tensor(ukkosurv_dataset(df_train).data_3d).float()\n",
    "    #x, (event, time) = next(iter(dataloader_train))\n",
    "    x = x.to(\"cpu\")\n",
    "    log_hz, feature_weights, time_weights = model(x)\n",
    "\n",
    "risk = log_hz.detach().cpu().numpy()\n",
    "high_risk = risk >= np.median(risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "fig = plot_risk_stratification(df_train['OSS_days'], df_train['OSS_status'], high_risk, \n",
    "                             title=\"CPH from lifelines\")\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_val)\n",
    "\n",
    "uk_val = ukkosurv_dataset(df_val)\n",
    "uk_val.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "model = model.to(\"cpu\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # test event and test time of length n\n",
    "    x, (event, time) = next(iter(dataloader_val))\n",
    "    x = x.to(\"cpu\")\n",
    "    log_hz, feature_weights, time_weights = model(x)\n",
    "    print(time[:4])\n",
    "    print(event[:4])\n",
    "\n",
    "print(time.numpy()[:4])\n",
    "print(event[:4].numpy())\n",
    "\n",
    "print(df_val['OSS_days'][:4].to_numpy())\n",
    "print(df_val['OSS_status'][:4].to_numpy())\n",
    "\n",
    "\n",
    "risk = log_hz.detach().cpu().numpy()\n",
    "high_risk = risk >= np.median(risk)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "fig = plot_risk_stratification(time, event, high_risk, \n",
    "                             title=\"ukko\")\n",
    "fig = plot_risk_stratification(df_val['OSS_days'], df_val['OSS_status'], high_risk, \n",
    "                             title=\"ukko\")\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Weibull-ukko \n",
    "\n",
    "This means 2 outputs of model: shape and scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Weibull model\n",
    "\n",
    "importlib.reload(ukko.core)\n",
    "\n",
    "# Initialize model\n",
    "# DualAttentionRegressor1(self, n_features, time_steps, d_model=128, n_heads=8, dropout=0.1, n_modules=1)\n",
    "model = ukko.core.DualAttentionRegressor1(\n",
    "    n_features=num_features,\n",
    "    time_steps=num_timepoints,\n",
    "    d_model=16,\n",
    "    n_heads=4,\n",
    "    dropout=0.2,\n",
    "    n_modules=2,\n",
    "    n_outputs=2 # output for log shape and log scale of Weibull\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training of Weibull model\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "EPOCHS = 60\n",
    "LEARNING_RATE = 1e-2\n",
    "\n",
    "# Init optimizer for Cox\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Initiate empty list to store the loss on the train and validation sets\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Get device and move model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = torch.tensor(0.0)\n",
    "    for i, batch in enumerate(dataloader_train):\n",
    "        x, (event, time) = batch\n",
    "        x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        log_params, feature_weights, time_weights = model(x)  \n",
    "        loss = neg_log_likelihood(log_params, event, time, reduction=\"mean\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach()\n",
    "\n",
    "    # Reccord loss on train and test sets\n",
    "    epoch_loss /= i + 1\n",
    "    train_losses.append(epoch_loss)\n",
    "    with torch.no_grad():\n",
    "        x, (event, time) = next(iter(dataloader_val))\n",
    "        log_params, feature_weights, time_weights = model(x)\n",
    "        val_losses.append(\n",
    "            neg_log_likelihood(log_params, event, time, reduction=\"mean\")\n",
    "        )\n",
    "\n",
    "    # Display progress\n",
    "    #if epoch % (EPOCHS // 10) == 0:\n",
    "    print(f\"Epoch: {epoch:03}, Training loss: {train_losses[-1]:0.2f}, Validation loss: {val_losses[-1]:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # test event and test time of length n\n",
    "    x, (event, time) = next(iter(dataloader_train))\n",
    "    log_params, feature_weights, time_weights = model(x)  # log hazard of length n\n",
    "\n",
    "# Additional step for Weibull:\n",
    "# Compute the log hazards from weibull log parameters\n",
    "log_hz = log_hazard(log_params, time)  \n",
    "# Compute the survival probability from weibull log parameters\n",
    "surv = survival_function(log_params, time)  \n",
    "\n",
    "# Concordance index\n",
    "weibull_cindex = ConcordanceIndex()\n",
    "print(\"Weibull model performance:\")\n",
    "print(f\"Concordance-index   = {weibull_cindex(log_hz, event, time)}\")\n",
    "print(f\"Confidence interval = {weibull_cindex.confidence_interval()}\")\n",
    "\n",
    "# H0: cindex = 0.5, Ha: cindex >0.5\n",
    "print(f\"p-value             = {weibull_cindex.p_value(alternative = 'greater')}\")\n",
    "\n",
    "# plot log hazard vs time\n",
    "plt.figure(figsize=(10, 5))\n",
    "# change to scatter plot and color by event \n",
    "plt.scatter(time, log_params[:,0], c=event, cmap='coolwarm', alpha=0.5)\n",
    "plt.xlabel(\"Surival Time\")\n",
    "plt.ylabel(\"Shape parameter (log scale)\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "# change to scatter plot and color by event \n",
    "plt.scatter(time, log_params[:,1], c=event, cmap='coolwarm', alpha=0.5)\n",
    "plt.xlabel(\"Surival Time\")\n",
    "plt.ylabel(\"Scale parameter (log scale)\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "# change to scatter plot and color by event \n",
    "plt.scatter(log_params[:,0], log_params[:,1], c=event, cmap='coolwarm', alpha=0.5)\n",
    "plt.xlabel(\"Shape parameter (log scale)\")\n",
    "plt.ylabel(\"Scale parameter (log scale)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_hz.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Hyperparameter search for ukko PH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_model_simple(\n",
    "            model,\n",
    "            dataloader_train,\n",
    "            dataloader_val,\n",
    "            optimizer = None,\n",
    "            n_epochs = 100,\n",
    "            learning_rate = 0.01,\n",
    "            device='cuda'\n",
    "        ):\n",
    "\n",
    "    # Initialize optimizer if not provided\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initiate empty list to store the loss on the train and validation sets\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_epoch = 0\n",
    "    best_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Get device and move model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0.0 #torch.tensor(0.0)\n",
    "        model = model.to(device)\n",
    "        for i, batch in enumerate(dataloader_train):\n",
    "            x, (event, time) = batch\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            event = event.to(device, non_blocking=True)\n",
    "            time = time.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            log_hz, feature_weights, time_weights = model(x)  # shape = (batchsize, 1)\n",
    "            loss = neg_partial_log_likelihood(log_hz, event, time, reduction=\"mean\")\n",
    "            #print(loss.is_cuda)\n",
    "            #print(epoch_loss.is_cuda)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() #.detach().to(\"cpu\").item()\n",
    "\n",
    "            # Free up memory for unused tensors\n",
    "            del x, event, time, log_hz, feature_weights, time_weights, loss\n",
    "            \n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Reccord loss on train and test sets\n",
    "        epoch_loss /= i + 1\n",
    "        train_losses.append(epoch_loss)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x, (event, time) = next(iter(dataloader_val))\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            event = event.to(device, non_blocking=True)\n",
    "            time = time.to(device, non_blocking=True)\n",
    "            log_hz, feature_weights, time_weights = model(x)\n",
    "            val_loss = neg_partial_log_likelihood(log_hz, event, time, reduction=\"mean\").item() #.detach().to(\"cpu\")\n",
    "            val_losses.append(val_loss)\n",
    "            # Save best model based on validation loss\n",
    "            if val_loss < best_loss:\n",
    "                best_epoch = epoch\n",
    "                best_loss = val_loss\n",
    "                best_model_state = copy.deepcopy(model.to('cpu').state_dict())\n",
    "        \n",
    "        del x, event, time, log_hz, feature_weights, time_weights    \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Display progress\n",
    "        if epoch % 10 == 0: #(n_epochs // 5) == 0:\n",
    "            print(f\"    Epoch: {epoch:03}, Training loss: {train_losses[-1]:0.2f}, Validation loss: {val_losses[-1]:0.2f}\")\n",
    "    \n",
    "    # Load best model if validation was used\n",
    "    if dataloader_val and best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    return model, train_losses, val_losses, best_loss, best_epoch #if dataloader_val else avg_train_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cuda memory management:\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def print_cuda_tensors():\n",
    "    \"\"\"\n",
    "    Prints a list of all tensors currently residing on the CUDA device.\n",
    "    \"\"\"\n",
    "    print(\"--- CUDA Tensors ---\")\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) and obj.is_cuda:\n",
    "                # Calculate memory in megabytes (MB)\n",
    "                mem_bytes = obj.element_size() * obj.nelement()\n",
    "                mem_mb = mem_bytes / (1024 ** 2)\n",
    "                \n",
    "                print(f\"Type: {obj.type():<30s} | Size: {str(list(obj.size())):<20s} | Mem: {mem_mb:.2f} MB\")\n",
    "        except:\n",
    "            # Some objects may fail attribute lookups\n",
    "            pass\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Usage ---\n",
    "# Create some tensors on the CPU and GPU\n",
    "a = torch.randn(100, 100)\n",
    "b = torch.randn(1000, 1000, device='cuda')\n",
    "c = torch.zeros((5, 2048, 2048), dtype=torch.float16, device='cuda')\n",
    "\n",
    "# Now let's see what's on the GPU\n",
    "print_cuda_tensors()\n",
    "\n",
    "# Output might look like this:\n",
    "# --- CUDA Tensors ---\n",
    "# Type: torch.cuda.FloatTensor          | Size: [1000, 1000]         | Mem: 3.81 MB\n",
    "# Type: torch.cuda.HalfTensor           | Size: [5, 2048, 2048]      | Mem: 40.00 MB\n",
    "# --------------------\n",
    "\n",
    "# If we delete a tensor, it will disappear from the list after Python's garbage collection\n",
    "del c\n",
    "# gc.collect() # Optionally force garbage collection\n",
    "print_cuda_tensors()\n",
    "\n",
    "# Output now only shows tensor 'b':\n",
    "# --- CUDA Tensors ---\n",
    "# Type: torch.cuda.FloatTensor          | Size: [1000, 1000]         | Mem: 3.81 MB\n",
    "# --------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import itertools\n",
    "\n",
    "def hyperparameter_search(model_class, dataloader_train, dataloader_val, param_grid):\n",
    "    \"\"\"\n",
    "    Perform grid search over hyperparameters\n",
    "    \n",
    "    Args:\n",
    "        model_class: The model class to instantiate\n",
    "        dataloader_train: DataLoader for training data\n",
    "        dataloader_val: DataLoader for validation data\n",
    "        param_grid: Dictionary of parameters to search, e.g.:\n",
    "            {\n",
    "                'learning_rate': [0.1, 0.01, 0.001],\n",
    "                'd_model': [8, 16, 32],\n",
    "                'n_heads': [4, 8],\n",
    "                'n_modules':[1,2,3],\n",
    "                'dropout': [0.1, 0.2]\n",
    "            }\n",
    "    \"\"\"\n",
    "    # Generate all combinations of parameters\n",
    "    param_combinations = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_params = None\n",
    "    best_model_state = None\n",
    "    best_epoch = 0\n",
    "    \n",
    "    # Get model/data dimensions\n",
    "    x, (event, time) = next(iter(dataloader_train))\n",
    "    num_features, num_timepoints = x.size(1), x.size(2)\n",
    "\n",
    "    # list of dicts for results:\n",
    "    results = []\n",
    "    # init log file\n",
    "    open('hyperparameter_results.txt', 'w').close()\n",
    "    \n",
    "    # Try each parameter combination\n",
    "    for params in param_combinations:\n",
    "        # Initialize model with current parameters\n",
    "        model = model_class(\n",
    "            n_features=num_features,\n",
    "            time_steps=num_timepoints,\n",
    "            d_model=params['d_model'],\n",
    "            n_heads=params['n_heads'],\n",
    "            n_modules=params['n_modules'],\n",
    "            dropout=params['dropout']\n",
    "        )\n",
    "        # Notes: \n",
    "        # model = ukko.core.DualAttentionRegressor1(\n",
    "        #     n_features=num_features,\n",
    "        #     time_steps=num_timepoints,\n",
    "        #     d_model=16,\n",
    "        #     n_heads=4,\n",
    "        #     dropout=0.2,\n",
    "        #     n_modules=2\n",
    "        # )\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "        \n",
    "        # Train model\n",
    "        trained_model, train_losses, val_losses, val_loss, best_epoch_i = train_model_simple(\n",
    "            model=model,\n",
    "            dataloader_train = dataloader_train,\n",
    "            dataloader_val = dataloader_val, \n",
    "            optimizer=optimizer,\n",
    "            n_epochs=500,\n",
    "            device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "\n",
    "        # Keep result\n",
    "        results.append(\n",
    "            {\n",
    "            'params': params,\n",
    "            'val_loss': val_loss #.detach().cpu().numpy()\n",
    "            }\n",
    "        )\n",
    "        with open('hyperparameter_results.txt', 'a') as f:\n",
    "            f.write(f\"Params: {params} | val_loss: {val_loss}\\n\")\n",
    "            \n",
    "        # Update best parameters if current model is better\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_params = params\n",
    "            best_model_state = trained_model.state_dict()\n",
    "            best_epoch = best_epoch_i\n",
    "\n",
    "        #print(\"--------------------\")\n",
    "        #print_cuda_tensors()\n",
    "\n",
    "        # Cleanup\n",
    "        del model\n",
    "        del optimizer\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        #print_cuda_tensors()\n",
    "        #print(\"--------------------\")\n",
    "            \n",
    "        print(f\"Params: {params}\")\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(\"--------------------\")\n",
    "    \n",
    "    return best_params, best_val_loss, best_model_state, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'd_model': [8, 16, 32],\n",
    "    'n_heads': [4, 8],\n",
    "    'n_modules': [1, 2, 3],\n",
    "    'dropout': [0.1, 0.2]\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'd_model': [8, 16],\n",
    "    'n_heads': [4, 8],\n",
    "    'n_modules': [1, 2, 3],\n",
    "    'dropout': [0.1, 0.2]\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01],\n",
    "    'd_model': [16],\n",
    "    'n_heads': [8],\n",
    "    'n_modules': [1, 2, 3],\n",
    "    'dropout': [0.1, 0.2]\n",
    "}\n",
    "\n",
    "BATCH_SIZE = 512#len(df_train) 1024 out of memory\n",
    "dataloader_train = DataLoader(\n",
    "    ukkosurv_dataset(df_train), batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "# Perform hyperparameter search\n",
    "best_params, best_val_loss, best_model_state, results = hyperparameter_search(\n",
    "    model_class = ukko.core.DualAttentionRegressor1,\n",
    "    dataloader_train = dataloader_train, \n",
    "    dataloader_val = dataloader_val,\n",
    "    param_grid = param_grid\n",
    ")\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best parameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Load best model\n",
    "# best_modep_param = {\n",
    "    \n",
    "# }\n",
    "best_model = ukko.core.DualAttentionRegressor1(**best_params)\n",
    "best_model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.01, 0.005],\n",
    "    'd_model': [8, 16],\n",
    "    'n_heads': [2, 4, 8],\n",
    "    'n_modules': [2, 3],\n",
    "    'dropout': [0.1, 0.2]\n",
    "}\n",
    "\n",
    "BATCH_SIZE = 512#len(df_train)\n",
    "dataloader_train = DataLoader(\n",
    "    ukkosurv_dataset(df_train), batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "# Perform hyperparameter search\n",
    "best_params, best_val_loss, best_model_state, results = hyperparameter_search(\n",
    "    model_class = ukko.core.DualAttentionRegressor1,\n",
    "    dataloader_train = dataloader_train, \n",
    "    dataloader_val = dataloader_val,\n",
    "    param_grid = param_grid\n",
    ")\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best parameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Load best model\n",
    "# best_modep_param = {\n",
    "    \n",
    "# }\n",
    "best_model = ukko.core.DualAttentionRegressor1(**best_params)\n",
    "best_model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "#df.explode('params')\n",
    "pd.concat([df[\"params\"].apply(pd.Series), df.val_loss], axis=1).sort_values('val_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Honey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = [0.0011, 0.0026, 0.0062, 0.0173]\n",
    "t = [19.5, 20.16, 20.75, 22.5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_str_to_hours(time_str):\n",
    "    \"\"\"Convert time string in format 'XhYY' to decimal hours\n",
    "    \n",
    "    Args:\n",
    "        time_str (str): Time in format like '2h13' or '3h45'\n",
    "    \n",
    "    Returns:\n",
    "        float: Decimal hours\n",
    "    \"\"\"\n",
    "    h, m = time_str.split('h')\n",
    "    return float(h) + float(m)/60\n",
    "\n",
    "y = np.array([0.0011, 0.0026, 0.0062, 0.0173, 0.0315, 0.34])\n",
    "h = ['19h31', '20h10', '20h45', '22h25', '23h00', '25h38']\n",
    "\n",
    "#y = np.array([0.0026, 0.0062, 0.0173, 0.0315])\n",
    "#h = ['20h10', '20h45', '22h25', '23h00']\n",
    "\n",
    "\n",
    "# Convert time strings to decimal hours\n",
    "t = [time_str_to_hours(time) for time in h]\n",
    "print(\"Times in decimal hours:\", [f\"{x:.2f}\" for x in t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Define exponential function\n",
    "def exp_func(x, a, b):\n",
    "    return a * np.exp(b * x)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "#t = np.array([19.5, 20.16, 20.75, 22.45])\n",
    "#y = np.array([0.0011, 0.0026, 0.0062, 0.0175])\n",
    "\n",
    "# Fit exponential function\n",
    "popt, pcov = curve_fit(exp_func, t, y)\n",
    "a, b = popt\n",
    "\n",
    "# Generate points for smooth curve\n",
    "t_smooth = np.linspace(min(t), 27, 100)\n",
    "y_fit = exp_func(t_smooth, a, b)\n",
    "\n",
    "# Find where y reaches 0.4\n",
    "t_at_04 = np.log(0.4/a)/b\n",
    "\n",
    "def hours_to_hm(hours):\n",
    "    h = int(hours)\n",
    "    m = int((hours - h) * 60)\n",
    "    return f\"{h}h {m}min\"\n",
    "\n",
    "# Add this line after finding t_at_04\n",
    "time_hm = hours_to_hm(t_at_04 - 24)\n",
    "print(f\"Time to reach 0.4: {time_hm}\")\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(t, y, color='blue', label='Data points')\n",
    "plt.plot(t_smooth, y_fit, 'r-', label=f'Fit: {a:.2e}*exp({b:.2f}x)')\n",
    "\n",
    "plt.axvline(x=t_at_04, color='green', linestyle='--', label=f't = {time_hm}')\n",
    "\n",
    "plt.yscale('log')  \n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Exponential Fit')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Fitted function: y = {a:.2e} * exp({b:.2f} * x)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - Pytorch and Tensorflow",
   "language": "python",
   "name": "python38-azureml-pt-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
