{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# The loss function (loghazard) has issues, so we need to look into that.\n",
    "\n",
    "**For now it might be good to use pytorch.loss**, which implements partial log liklihood and Weibull losses. \n",
    "\n",
    "---\n",
    "\n",
    "### **Context: Why Breslow’s Method?**\n",
    "\n",
    "In the Cox model, the **partial likelihood** is used to estimate the regression coefficients without specifying the baseline hazard function. However, when **ties** occur — that is, **multiple events happen at the same time point** — the original Cox partial likelihood (which assumes distinct event times) becomes problematic.\n",
    "\n",
    "This is where **Breslow’s method** comes in: it provides a way to **approximate the partial likelihood** in the presence of tied event times.\n",
    "\n",
    "---\n",
    "\n",
    "### **Breslow’s Approximation: The Idea**\n",
    "\n",
    "Suppose at time \\( t_j \\), **\\( d_j \\) individuals** experience the event (i.e., a tie). Let:\n",
    "\n",
    "- \\( R_j \\): the **risk set** just before time \\( t_j \\)\n",
    "- \\( \\beta \\): the vector of regression coefficients\n",
    "- \\( x_i \\): covariate vector for individual \\( i \\)\n",
    "\n",
    "The **exact partial likelihood** for tied events is computationally intensive. Breslow proposed a simpler approximation:\n",
    "\n",
    "\\[\n",
    "L_B(\\beta) = \\prod_{j} \\frac{\\exp\\left( \\sum_{i \\in D_j} \\beta^T x_i \\right)}{\\left[ \\sum_{i \\in R_j} \\exp(\\beta^T x_i) \\right]^{d_j}}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( D_j \\): the set of individuals who fail at time \\( t_j \\)\n",
    "- The denominator is raised to the power \\( d_j \\), treating the tied events as if they occurred **sequentially but with the same risk set**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Interpretation**\n",
    "\n",
    "- **Numerator**: Product of exponentiated linear predictors for those who failed at \\( t_j \\)\n",
    "- **Denominator**: Risk set sum raised to the number of events — this is the **approximation** part\n",
    "\n",
    "This method **underestimates the denominator** slightly compared to the exact method, especially when the number of ties is large.\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison with Other Methods**\n",
    "\n",
    "| Method         | Accuracy with Ties | Computational Cost | Notes |\n",
    "|----------------|--------------------|--------------------|-------|\n",
    "| **Breslow**    | Low (for many ties) | Low                | Simple, fast |\n",
    "| **Efron**      | Higher              | Moderate           | Better approximation |\n",
    "| **Exact**      | Highest             | High               | Computationally expensive |\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use Breslow’s Method**\n",
    "\n",
    "- **Few ties**: Breslow is often sufficient and computationally efficient.\n",
    "- **Many ties**: Prefer **Efron’s method** or **exact partial likelihood**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchsurv.loss.cox as tscox #import neg_partial_log_likelihood\n",
    "\n",
    "# Own implementation (super simple, double check!). Assumes events are ordered already.\n",
    "\n",
    "def naive_partial_likelihood(log_hazard, event_indicator, event_time):\n",
    "    risk_set = torch.flip(torch.cumsum(torch.flip(torch.exp(log_hazard), [0]), dim=0), [0])\n",
    "    partial_likelihood = torch.sum(log_hazard[event_indicator == 1] - torch.log(risk_set[event_indicator == 1]))\n",
    "    return partial_likelihood\n",
    "\n",
    "def breslow_partial_likelihood(log_hazard, event_indicator, event_time):\n",
    "    unique_times, counts = torch.unique(event_time[event_indicator == 1], return_counts=True)\n",
    "    print(unique_times, counts)\n",
    "    partial_likelihood = 0\n",
    "    for t, d in zip(unique_times, counts):\n",
    "        risk_set = torch.sum(torch.exp(log_hazard[event_time >= t]))\n",
    "        partial_likelihood += torch.sum(log_hazard[(event_time == t) & (event_indicator == 1)]) - d * torch.log(risk_set)\n",
    "        print(t, d, risk_set)\n",
    "    return partial_likelihood\n",
    "\n",
    "def efron_partial_likelihood(log_hazard, event_indicator, event_time):\n",
    "    unique_times, counts = torch.unique(event_time[event_indicator == 1], return_counts=True)\n",
    "    partial_likelihood = 0\n",
    "    for t, d in zip(unique_times, counts):\n",
    "        risk_set = torch.sum(torch.exp(log_hazard[event_time >= t]))\n",
    "        tied_hazard_sum = torch.sum(torch.exp(log_hazard[(event_time == t) & (event_indicator == 1)]))\n",
    "        for k in range(d):\n",
    "            partial_likelihood += (torch.sum(log_hazard[(event_time == t) & (event_indicator == 1)]) - torch.log(risk_set - k * tied_hazard_sum / d))\n",
    "    return partial_likelihood\n",
    "\n",
    "# Example usage\n",
    "log_hazard = torch.tensor([0.5, 1.0, 1.0, 1.5, 2.0])\n",
    "event_indicator = torch.tensor([1, 1, 0, 1, 1])\n",
    "event_time = torch.tensor([1, 2, 2, 2, 3])\n",
    "\n",
    "log_hazard = torch.tensor([0.5, 1.0, 1.5, 2.0])\n",
    "event_indicator = torch.tensor([1, 1, 1, 1])\n",
    "event_time = torch.tensor([1, 2, 2, 3])\n",
    "\n",
    "reduction = 'mean' # or 'sum', depending on your needs\n",
    "torchsurv_ll_breslow = tscox.neg_partial_log_likelihood(log_hazard, event_indicator, event_time, ties_method='breslow', reduction = reduction)\n",
    "torchsurv_ll_efron = tscox.neg_partial_log_likelihood(log_hazard, event_indicator, event_time, ties_method='efron', reduction = reduction)\n",
    "partial_ll_naive = naive_partial_likelihood(log_hazard, event_indicator, event_time)\n",
    "partial_ll_breslow = breslow_partial_likelihood(log_hazard, event_indicator, event_time)\n",
    "partial_ll_efron = efron_partial_likelihood(log_hazard, event_indicator, event_time)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"TorchSurv Breslow Partial Likelihood:\", torchsurv_ll_breslow.item())\n",
    "print(\"TorchSurv Efron Partial Likelihood:  \", torchsurv_ll_efron.item())\n",
    "print(\"---\")\n",
    "print(\"Naive Partial Likelihood:  \", partial_ll_naive.item())\n",
    "print(\"Breslow Partial Likelihood:\", partial_ll_breslow.item())\n",
    "print(\"Efron Partial Likelihood:  \", partial_ll_efron.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Survival Analysis with TorchSurv and Artificial Data\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Generating artificial survival data\n",
    "2. Training a TorchSurv LogHazard model\n",
    "3. Evaluating and visualizing the results using stratification plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable to handle OpenMP runtime conflict\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "# Also set this for the current session\n",
    "if os.name == 'nt':  # Windows\n",
    "    try:\n",
    "        import ctypes\n",
    "        ctypes.CDLL('mkl_rt.dll')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchsurv.loss.cox as tscox\n",
    "\n",
    "# Import ukko survival functions\n",
    "from ukko.survival import generate_survival_data_LL, plot_KM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ukko.survival\n",
    "importlib.reload(ukko.survival)\n",
    "\n",
    "def generate_data(n_samples=1000, n_features=3, n_informative=1, \n",
    "                 shape=5, scale=10.0, censoring=0.3, random_seed=42, \n",
    "                 nonlinear = True):\n",
    "    \"\"\"Generate synthetic survival data with log-logistic distribution.\"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    survival_data, true_coefficients = ukko.survival.generate_survival_data_LL(\n",
    "        n_samples, n_features, n_informative,\n",
    "        shape, scale, censoring,\n",
    "        nonlinear = nonlinear\n",
    "    )\n",
    "    feature_cols = [col for col in survival_data.columns if col.startswith('feature_')]\n",
    "    \n",
    "    return survival_data, true_coefficients, feature_cols\n",
    "\n",
    "def prepare_data_for_pytorch(survival_data, feature_cols):\n",
    "    \"\"\"Prepare survival data for PyTorch.\"\"\"\n",
    "    # Prepare data for PyTorch\n",
    "    X = torch.FloatTensor(survival_data[feature_cols].values)\n",
    "    times = torch.FloatTensor(survival_data['observed_time'].values)\n",
    "    events = torch.FloatTensor(survival_data['event_observed'].values)\n",
    "    # Create data loader\n",
    "    dataset = TensorDataset(X, times, events)\n",
    "    batch_size = X.shape[0]  # Use the entire dataset as one batch\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "# Generate linear data:\n",
    "survival_data_lin, true_coefficients_lin, feature_cols_lin = \\\n",
    "  generate_data(nonlinear=False)\n",
    "train_loader_lin = prepare_data_for_pytorch(survival_data_lin, feature_cols_lin)\n",
    "\n",
    "# Generate nonlinear data:\n",
    "survival_data_nonlin, true_coefficients_nonlin, feature_cols_nonlin = \\\n",
    "  generate_data(nonlinear=True)\n",
    "train_loader_nonlin = prepare_data_for_pytorch(survival_data_nonlin, feature_cols_nonlin)\n",
    "plt.title(\"Generated Nonlinear Survival Data\")\n",
    "\n",
    "# Plot initial KM curve for the entire dataset\n",
    "plot_KM(survival_data_nonlin)\n",
    "\n",
    "print(\"\\nData shape:\", survival_data_nonlin.shape)\n",
    "print(\"Number of events:\", survival_data_nonlin['event_observed'].sum())\n",
    "print(\"Censoring rate:\", 1 - survival_data_nonlin['event_observed'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratify_and_plot(survival_data, risk_scores, n_groups=4, model_name=''):\n",
    "    \"\"\"Stratify patients into multiple risk groups and plot KM curves.\"\"\"\n",
    "    survival_data = survival_data.copy()\n",
    "    survival_data['predicted_risk'] = risk_scores\n",
    "    \n",
    "    # Calculate quantile cutoffs\n",
    "    quantiles = np.linspace(0, 1, n_groups + 1)\n",
    "    cutoffs = np.quantile(survival_data['predicted_risk'], quantiles)\n",
    "    \n",
    "    # Plot KM curves\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create color map for groups\n",
    "    try:\n",
    "        colors = plt.cm.managua(np.linspace(0, 1, n_groups))\n",
    "    except:\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, n_groups))\n",
    "    \n",
    "    for i in range(n_groups):\n",
    "        if i == 0:\n",
    "            mask = survival_data['predicted_risk'] <= cutoffs[1]\n",
    "            group_label = f'Lowest Risk (Q1)'\n",
    "        elif i == n_groups - 1:\n",
    "            mask = survival_data['predicted_risk'] > cutoffs[-2]\n",
    "            group_label = f'Highest Risk (Q{n_groups})'\n",
    "        else:\n",
    "            mask = (survival_data['predicted_risk'] > cutoffs[i]) & \\\n",
    "                   (survival_data['predicted_risk'] <= cutoffs[i+1])\n",
    "            group_label = f'Q{i+1}'\n",
    "        \n",
    "        group = survival_data[mask]\n",
    "        \n",
    "        kmf = KaplanMeierFitter()\n",
    "        kmf.fit(durations=group['observed_time'],\n",
    "                event_observed=group['event_observed'],\n",
    "                label=group_label)\n",
    "        kmf.plot_survival_function(show_censors=True, ci_show=False, color=colors[i])\n",
    "        \n",
    "        print(f\"{group_label} size: {len(group)}\")\n",
    "    \n",
    "    plt.title(f'Kaplan-Meier Survival Curves by Risk Group ({model_name})')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Survival Probability')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return cutoffs\n",
    "\n",
    "def plot_training_loss(result):\n",
    "  model, history, best_loss = result\n",
    "  # Plot training loss\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  plt.plot(history['train_loss'])\n",
    "  plt.axhline(y=best_loss, color='r', linestyle='--', label='Best Loss')\n",
    "  plt.title('Training Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LogHazard model\n",
    "class LogHazardModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Define ukko loghazard model for 1-timepoint data using dual attention mechanism\n",
    "from ukko.core import DualAttentionRegressor1\n",
    "class UkkoLogHazardModel(nn.Module):\n",
    "    \"\"\"A PyTorch model for survival analysis using a dual attention mechanism.\"\"\"\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.attention_net = DualAttentionRegressor1(\n",
    "            n_features=n_features,\n",
    "            time_steps=1,  # Since we're using instantaneous inputs\n",
    "            d_model=64,\n",
    "            n_heads=4,\n",
    "            dropout=0.1,\n",
    "            n_modules=3\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Add time dimension expected by DualAttentionRegressor\n",
    "        x = x.unsqueeze(2)  # [batch_size, n_features, 1]\n",
    "        \n",
    "        # Get predictions from attention network\n",
    "        predictions, feat_weights, time_weights = self.attention_net(x)\n",
    "        \n",
    "        return predictions, feat_weights, time_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Compare linear and ukko loghazard models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Linear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 500\n",
    "\n",
    "import importlib\n",
    "import ukko.core\n",
    "from ukko.core import train_model_simple\n",
    "#from ukko.core import UkkoLogHazardModel, train_model_simple\n",
    "importlib.reload(ukko.core)\n",
    "\n",
    "# Linear Model:\n",
    "model_lin = LogHazardModel(len(feature_cols_lin))\n",
    "optimizer_lin = torch.optim.Adam(model_lin.parameters(), lr=0.001)\n",
    "reduction = 'mean'\n",
    "\n",
    "# Ukko LogHazard Model:\n",
    "model = UkkoLogHazardModel(len(feature_cols_lin))\n",
    "#model = ukko.core.UkkoLogHazardModel(len(feature_cols))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "reduction = 'mean'\n",
    "\n",
    "# Train the linear model\n",
    "print(\"Training Linear Model...\")\n",
    "#model, history, best_loss = train_model_simple(\n",
    "result_lin = train_model_simple(\n",
    "    model_lin, \n",
    "    train_loader_lin, \n",
    "    optimizer_lin,\n",
    "    n_epochs=n_epochs,\n",
    "    device='cuda',\n",
    "    reduction=reduction\n",
    ")\n",
    "\n",
    "print(\"Training Ukko LogHazard Model...\")\n",
    "# Train the linear model\n",
    "result_ukko = train_model_simple(\n",
    "    model, \n",
    "    train_loader_lin, \n",
    "    optimizer,\n",
    "    n_epochs=n_epochs,\n",
    "    device='cuda',\n",
    "    reduction=reduction\n",
    ")\n",
    "print(\" \")\n",
    "\n",
    "plot_training_loss(result_lin)\n",
    "plot_training_loss(result_ukko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Model performance evaluation:\n",
    "X =  train_loader_lin.dataset.tensors[0].cpu()\n",
    "\n",
    "# Generate risk scores using the trained linear model\n",
    "model_lin.eval().to('cpu')\n",
    "with torch.no_grad():\n",
    "    risk_scores_lin = model_lin(X).numpy().flatten()\n",
    "# Create stratification plot\n",
    "cutoffs_lin = stratify_and_plot(survival_data_lin, risk_scores_lin, n_groups=5,\n",
    "                            model_name='Linear Model')\n",
    "\n",
    "# Ukko Model performance evaluation:\n",
    "# Generate risk scores using the trained ukko model\n",
    "model.eval().cpu()\n",
    "with torch.no_grad():\n",
    "    risk_scores, feat_weights, time_weights = model(X)\n",
    "    risk_scores = risk_scores.numpy()\n",
    "# Create stratification plot\n",
    "cutoffs = stratify_and_plot(survival_data_lin, risk_scores, n_groups=5, \n",
    "                            model_name='Ukko LogHazard Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Non-linear data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ukko.core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2000\n",
    "\n",
    "# Linear Model:\n",
    "model_lin = LogHazardModel(len(feature_cols_nonlin))\n",
    "optimizer_lin = torch.optim.Adam(model_lin.parameters(), lr=0.001)\n",
    "reduction = 'mean'\n",
    "\n",
    "# Ukko LogHazard Model:\n",
    "model = UkkoLogHazardModel(len(feature_cols_nonlin))\n",
    "#model = ukko.core.UkkoLogHazardModel(len(feature_cols))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)\n",
    "reduction = 'mean'\n",
    "\n",
    "# Train the linear model\n",
    "print(\"Training Linear Model...\")\n",
    "#model, history, best_loss = train_model_simple(\n",
    "result_lin = train_model_simple(\n",
    "    model_lin, \n",
    "    train_loader_nonlin, \n",
    "    optimizer_lin,\n",
    "    n_epochs=n_epochs // 10,\n",
    "    device='cuda',\n",
    "    reduction=reduction\n",
    ")\n",
    "\n",
    "print(\"Training Ukko LogHazard Model...\")\n",
    "# Train the ukko model\n",
    "result_ukko = train_model_simple(\n",
    "    model, \n",
    "    train_loader_nonlin, \n",
    "    optimizer,\n",
    "    n_epochs=n_epochs,\n",
    "    device='gpu',\n",
    "    reduction=reduction\n",
    ")\n",
    "print(\" \")\n",
    "\n",
    "plot_training_loss(result_lin)\n",
    "plot_training_loss(result_ukko)\n",
    "\n",
    "# Linear Model performance evaluation:\n",
    "X =  train_loader_nonlin.dataset.tensors[0].cpu()\n",
    "\n",
    "# Generate risk scores using the trained model\n",
    "model_lin.eval().to('cpu')\n",
    "with torch.no_grad():\n",
    "    risk_scores_lin = model_lin(X).numpy().flatten()\n",
    "# Create stratification plot\n",
    "cutoffs_lin = stratify_and_plot(survival_data_nonlin, risk_scores_lin, n_groups=5,\n",
    "                            model_name='Linear Model')\n",
    "\n",
    "# Ukko Model performance evaluation:\n",
    "# Generate risk scores using the trained model\n",
    "model.eval().to('cpu')\n",
    "with torch.no_grad():\n",
    "    risk_scores, feat_weights, time_weights = model(X)\n",
    "    risk_scores = risk_scores.numpy()\n",
    "\n",
    "# Create stratification plot\n",
    "cutoffs = stratify_and_plot(survival_data_nonlin, risk_scores, n_groups=5, \n",
    "                            model_name='Ukko LogHazard Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 1. Simple loghazard model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PyTorch\n",
    "X = torch.FloatTensor(survival_data[feature_cols].values)\n",
    "times = torch.FloatTensor(survival_data['observed_time'].values)\n",
    "events = torch.FloatTensor(survival_data['event_observed'].values)\n",
    "\n",
    "# Create data loader\n",
    "dataset = TensorDataset(X, times, events)\n",
    "batch_size = X.shape[0]  # Use the entire dataset as one batch\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = LogHazardModel(len(feature_cols))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_epoch(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X_batch, times_batch, events_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get log hazards\n",
    "        log_hazards = model(X_batch)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = tscox.neg_partial_log_likelihood(log_hazards, events_batch==1, times_batch, ties_method='efron', reduction = reduction)\n",
    "        \n",
    "        # # Basic implementation of cox partial likelihood\n",
    "        # log_risk = log_hazards\n",
    "        # risk = torch.exp(log_risk)\n",
    "        \n",
    "        # # Sort by time\n",
    "        # sorted_times, sort_idx = torch.sort(times_batch, descending=True)\n",
    "        # risk = risk[sort_idx]\n",
    "        # events = events_batch[sort_idx]\n",
    "        \n",
    "        # # Calculate loss using cox partial likelihood\n",
    "        # log_cumsum = torch.log(torch.cumsum(risk, dim=0) + 1e-8)\n",
    "        # loss = -torch.mean((log_risk[events == 1] - log_cumsum[events == 1]))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 100\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss = train_epoch(model, train_loader, optimizer)\n",
    "    losses.append(loss)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}, Loss: {loss:.4f}')\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 2. Ukko model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 2. Ukko Model\n",
    "\n",
    "Now we'll train a DualAttentionRegressor model from the ukko package and compare its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ukko.core\n",
    "#from ukko.core import UkkoLogHazardModel, train_model_simple\n",
    "importlib.reload(ukko.core)\n",
    "\n",
    "# Example usage (replace existing training loop):\n",
    "model = LogHazardModel(len(feature_cols))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "reduction = 'mean'\n",
    "\n",
    "# Usage: UkkoLogHazardModel:\n",
    "model = ukko.core.UkkoLogHazardModel(len(feature_cols))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)\n",
    "reduction = 'mean'\n",
    "\n",
    "# Train the model\n",
    "model, history, best_loss = train_model_simple(\n",
    "    model, train_loader, optimizer,\n",
    "    n_epochs=10,\n",
    "    device='gpu',\n",
    "    reduction=reduction\n",
    ")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history['train_loss'])\n",
    "plt.axhline(y=best_loss, color='r', linestyle='--', label='Best Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Model performance evaluation:\n",
    "# Generate risk scores using the trained model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    risk_scores, feat_weights, time_weights = model(X)\n",
    "    risk_scores = risk_scores.numpy()\n",
    "\n",
    "# Create stratification plot\n",
    "cutoffs = stratify_and_plot(survival_data, risk_scores, n_groups=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate risk scores using the trained model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    risk_scores, feat_weights, time_weights = model(X)\n",
    "    risk_scores = risk_scores.numpy()\n",
    "\n",
    "# Create stratification plot\n",
    "cutoffs = stratify_and_plot(survival_data, risk_scores, n_groups=5)\n",
    "\n",
    "# Visualize attention weights\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Feature attention weights - reshape for visualization\n",
    "feat_attention = feat_weights.detach().numpy()\n",
    "feat_attention = feat_attention.reshape(-1, len(feature_cols))  # Reshape to [batch*heads, n_features]\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(feat_attention, aspect='auto')\n",
    "plt.title('Feature Attention Weights')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Batch × Heads')\n",
    "\n",
    "# Time attention weights - should be 1×1 for our case\n",
    "plt.subplot(122)\n",
    "time_attention = time_weights.detach().numpy()\n",
    "time_attention = time_attention.reshape(-1, 1)  # Reshape to [batch*heads, 1]\n",
    "\n",
    "plt.imshow(time_attention, aspect='auto')\n",
    "plt.title('Time Attention Weights')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Batch × Heads')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print average feature attention weights\n",
    "mean_attention = feat_attention.mean(axis=0)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(range(len(feature_cols)), mean_attention)\n",
    "plt.title('Average Feature Importance')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Mean Attention Weight')\n",
    "plt.xticks(range(len(feature_cols)), feature_cols, rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "# Appendix - depreciated stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ukko.core import DualAttentionRegressor1\n",
    "import copy \n",
    "\n",
    "\n",
    "# Define the Ukko LogHazard model\n",
    "class UkkoLogHazardModel(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.attention_net = DualAttentionRegressor1(\n",
    "            n_features=n_features,\n",
    "            time_steps=1,  # Since we're using instantaneous inputs\n",
    "            d_model=64,\n",
    "            n_heads=4,\n",
    "            dropout=0.1,\n",
    "            n_modules=3\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Add time dimension expected by DualAttentionRegressor\n",
    "        x = x.unsqueeze(2)  # [batch_size, n_features, 1]\n",
    "        \n",
    "        # Get predictions from attention network\n",
    "        predictions, feat_weights, time_weights = self.attention_net(x)\n",
    "        \n",
    "        return predictions, feat_weights, time_weights\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = UkkoLogHazardModel(len(feature_cols))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 600  # Increased epochs since we're tracking best model\n",
    "losses = []\n",
    "best_loss = float('inf')\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Adjust learning rate after 300 epochs\n",
    "    if epoch == 300:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.001\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X_batch, times_batch, events_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get log hazards\n",
    "        log_hazards, feat_weights, time_weights = model(X_batch)\n",
    "        \n",
    "        # Calculate loss using TorchSurv's Cox partial likelihood\n",
    "        loss = tscox.neg_partial_log_likelihood(log_hazards, events_batch==1, times_batch, ties_method='efron', reduction='mean')\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.4f}, Best Loss: {best_loss:.4f}')\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses)\n",
    "plt.axhline(y=best_loss, color='r', linestyle='--', label='Best Loss')\n",
    "plt.title('Training Loss - Ukko LogHazard Model')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Load best model for evaluation\n",
    "print('\\nLoading best model with loss:', best_loss)\n",
    "model.load_state_dict(best_model_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
