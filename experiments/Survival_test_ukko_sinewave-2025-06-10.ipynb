{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "\n",
    "Test ukko model using LONGITUDONAL FEATIRES. \n",
    "\n",
    "Create a time-dependent survival dataset that combines log-logistic survival times with sinusoidal features.  \n",
    "OPtions:\n",
    "\n",
    "- [x] amplitude modulation\n",
    "- [ ] frequency modulation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "  ## Amplitude modulation\n",
    "\n",
    "This approach creates an interpretable relationship between survival times and feature patterns, where:\n",
    "\n",
    "- Longer survival times result in higher amplitude in the first feature\n",
    "- The temporal patterns in the features could be used to predict survival\n",
    "- The censoring mechanism affects both the survival times and the feature patterns\n",
    "\n",
    "SurvivalSineWaveDataset:\n",
    "\n",
    "- Use generate_survival_data_LL() for survival times and censoring\n",
    "- Keep both dynamic (sinusoidal) and static features from the survival data\n",
    "- Use normalized observed times for amplitude modulation of first feature\n",
    "- Use static features to modulate amplitudes of other features\n",
    "\n",
    "plot_survival_features to show:\n",
    "- First feature (amplitude modulated by survival time)\n",
    "- Other dynamic features (modulated by static features)\n",
    "- Original static features from generate_survival_data_LL\n",
    "- Added example usage and visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable to handle OpenMP runtime conflict\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "# Also set this for the current session\n",
    "if os.name == 'nt':  # Windows\n",
    "    try:\n",
    "        import ctypes\n",
    "        ctypes.CDLL('mkl_rt.dll')\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchsurv.loss.cox as tscox\n",
    "\n",
    "# Import ukko functions\n",
    "from ukko.survival import generate_survival_data_LL, plot_KM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SurvivalSineWaveDataset(Dataset):\n",
    "    def __init__(self, n_samples, n_features, sequence_length, \n",
    "                 shape=5, scale=10.0, censoring=0.3,\n",
    "                 base_freq=0.1, noise_level=0.1, seed=42,\n",
    "                 nonlinear=False):\n",
    "        \"\"\"\n",
    "        Creates survival dataset with sinusoidal time-dependent features.\n",
    "        The true survival time modulates the amplitude of the first feature.\n",
    "        Uses generate_survival_data_LL for survival times generation.\n",
    "        \n",
    "        Args:\n",
    "            n_samples: Number of samples\n",
    "            n_features: Number of features for sinusoidal patterns (actual will be n_features + original features)\n",
    "            sequence_length: Length of time series\n",
    "            shape: Shape parameter for log-logistic distribution\n",
    "            scale: Scale parameter for log-logistic distribution\n",
    "            censoring: Censoring rate\n",
    "            base_freq: Base frequency of sine waves\n",
    "            noise_level: Standard deviation of Gaussian noise\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.n_features = n_features\n",
    "        self.true_coefficients = np.random.uniform(0.5, 2.0, n_features) #placeholder for true coefficients\n",
    "        \n",
    "        # Generate survival data using ukko function\n",
    "        survival_data, true_coefficients = generate_survival_data_LL(\n",
    "            n_samples=n_samples,\n",
    "            n_features=n_features,  # These will be our static features\n",
    "            n_informative_features=1, #min(3, n_features),\n",
    "            loglogistic_shape=shape,\n",
    "            loglogistic_scale=scale,\n",
    "            target_censoring_percentage=censoring\n",
    "        )\n",
    "        self.true_coefficients = true_coefficients\n",
    "\n",
    "        # Extract survival information\n",
    "        observed_times = survival_data['observed_time'].values\n",
    "        events = survival_data['event_observed'].values\n",
    "        static_features = survival_data[[f'feature_{i}' for i in range(n_features)]].values\n",
    "        \n",
    "        # Normalize survival times to use as amplitude modulation\n",
    "        max_time = np.max(observed_times)\n",
    "        normalized_times = observed_times / max_time\n",
    "        \n",
    "        # Plot normalized survival times\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(normalized_times, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "        plt.title('Histogram of Normalized Survival Times')\n",
    "        print(\"Mean of normalized times:\", np.mean(normalized_times))\n",
    "\n",
    "        # Create time points for sequences\n",
    "        t = np.linspace(0, sequence_length * 2 * np.pi, sequence_length)\n",
    "        \n",
    "        # Generate base frequencies and phases\n",
    "        freq_array = np.random.uniform(0.5, 2.0, n_features)\n",
    "        phase_array = np.random.uniform(0, 2 * np.pi, n_features)\n",
    "        \n",
    "        # Generate data for each sample\n",
    "        data = []\n",
    "        ampl = []\n",
    "        for i in range(n_samples):\n",
    "            sample = []\n",
    "            for f in range(n_features):\n",
    "                if f == 0:\n",
    "                    # Modulate amplitude with survival time\n",
    "                    amplitude = normalized_times[i]\n",
    "                    if nonlinear:\n",
    "                      # Define Gaussian function:\n",
    "                      def gaussian(x, a=1, b=1, c=1):\n",
    "                          \"\"\"\n",
    "                          Gaussian function with parameters a (peak height), b (possition), and c (width).\n",
    "                          \"\"\"\n",
    "                          return a * np.exp( - (x - b) ** 2 / (2 * c ** 2))\n",
    "                      def nonlinearity(x):\n",
    "                          return gaussian(x, a=1, b=0.25, c=0.15)\n",
    "                      # Apply Gaussian function to amplitude\n",
    "                      amplitude = nonlinearity(normalized_times[i])\n",
    "                      #print(f\"nomalized_times[i]: {normalized_times[i]}, amplitude: {amplitude}\")\n",
    "                    amplitude0 = amplitude\n",
    "                else:\n",
    "                    # Use static features as base amplitudes\n",
    "                    amplitude = (static_features[i, f] + 1) / 2  # Normalize to 0-1 range\n",
    "                    #amplitude0 = []\n",
    "                \n",
    "                # Generate sine wave with noise\n",
    "                sine_wave = amplitude * np.sin(freq_array[f] * base_freq * t + phase_array[f])\n",
    "                noise = np.random.normal(0, noise_level, len(t))\n",
    "                feature_data = sine_wave + noise\n",
    "                \n",
    "                sample.append(feature_data)\n",
    "            data.append(sample)\n",
    "            ampl.append(amplitude0)\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        self.data = torch.FloatTensor(data)  # [n_samples, n_features, sequence_length]\n",
    "        self.static_features = torch.FloatTensor(static_features)  # Keep original features\n",
    "        self.amplitude = torch.FloatTensor(ampl)  # Store amplitude for first feature\n",
    "        self.observed_times = torch.FloatTensor(observed_times)\n",
    "        self.events = torch.FloatTensor(events)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        static = self.static_features[idx]\n",
    "        time = self.observed_times[idx]\n",
    "        event = self.events[idx]\n",
    "        return x, static, time, event\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_survival_features(dataset, sample_indices=None):\n",
    "    \"\"\"Plot survival times and corresponding feature patterns.\"\"\"\n",
    "    \n",
    "    # deal with fact that dataset might be smaller than 4 samples\n",
    "    if len(dataset) < 4:\n",
    "        nsamples = len(dataset)\n",
    "    else:\n",
    "        nsamples = 4\n",
    "\n",
    "    # Take samples\n",
    "    if sample_indices is None:\n",
    "        sample_indices = np.random.choice(len(dataset), nsamples, replace=False)\n",
    "        \n",
    "    fig, axes = plt.subplots(len(sample_indices), 3, figsize=(15, 3*len(sample_indices)))\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        x, static, time, event = dataset[idx]\n",
    "        \n",
    "        # Plot first feature (amplitude modulated by survival)\n",
    "        axes[i,0].plot(x[0].numpy())\n",
    "        axes[i,0].set_title(f'Sample {idx}: Time={time:.1f}, Event={event:.0f}')\n",
    "        axes[i,0].set_ylabel('Feature 1 (Survival)')\n",
    "        axes[i,0].grid(True)\n",
    "        \n",
    "        # Plot other dynamic features\n",
    "        for j in range(1, dataset.n_features):\n",
    "            axes[i,1].plot(x[j].numpy(), label=f'Feature {j+1}')\n",
    "        axes[i,1].set_title('Other Dynamic Features')\n",
    "        axes[i,1].grid(True)\n",
    "        if i == 0:\n",
    "            axes[i,1].legend()\n",
    "        \n",
    "        # Plot static features\n",
    "        axes[i,2].bar(range(len(static)), static.numpy())\n",
    "        axes[i,2].set_title('Static Features')\n",
    "        axes[i,2].set_ylim(-2, 2)\n",
    "        axes[i,2].grid(True)\n",
    "            \n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "    fig2, ax = plt.subplots(1,2,figsize=(10, 3) )\n",
    "    # plot amplitude of featuer 1 against survival time\n",
    "    # Note: toch.max retunrs a tuple (values, indices), we need to take values\n",
    "    ax[0].scatter(dataset.observed_times.numpy(), dataset.data[:, 0, :].max(dim=1)[0].numpy(), alpha=0.1)\n",
    "    ax[0].set_xlabel('Observed Survival Time')\n",
    "    ax[0].set_ylabel('Mean Amplitude of Feature 1')\n",
    "    ax[0].set_title('Amplitude of Feature 1 vs Survival Time')\n",
    "    # plot first static_feature agains tsurvival time\n",
    "    ax[1].scatter(dataset.observed_times.numpy(), dataset.static_features[:, 0].numpy(), alpha=0.1)\n",
    "    ax[1].set_xlabel('Observed Survival Time')\n",
    "    ax[1].set_ylabel('First Static Feature')\n",
    "    ax[1].set_title('First Static Feature vs Survival Time') \n",
    "    \n",
    "    return fig, fig2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear sine wave dataset with survival times\n",
    "dataset_lin = SurvivalSineWaveDataset(\n",
    "    n_samples=2000,\n",
    "    n_features=3,\n",
    "    sequence_length=20,\n",
    "    shape=5,\n",
    "    scale=10.0,\n",
    "    censoring=0.3,\n",
    "    noise_level=0.0,\n",
    "    nonlinear=False\n",
    ")\n",
    "\n",
    "# Plot examples\n",
    "fig = plot_survival_features(dataset_lin, sample_indices=[0, 1, 2, 3, 4])\n",
    "plt.show()\n",
    "\n",
    "# Plot KM curve for the dataset\n",
    "survival_df_lin = pd.DataFrame({\n",
    "    'observed_time': dataset_lin.observed_times.numpy(),\n",
    "    'event_observed': dataset_lin.events.numpy()\n",
    "})\n",
    "plot_KM(survival_df_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nonliear sine wave dataset with survival times\n",
    "dataset = SurvivalSineWaveDataset(\n",
    "    n_samples=2000,\n",
    "    n_features=3,\n",
    "    sequence_length=20,\n",
    "    shape=5,\n",
    "    scale=10.0,\n",
    "    censoring=0.3,\n",
    "    noise_level=0.0,\n",
    "    nonlinear=True\n",
    ")\n",
    "\n",
    "#print(dataset)\n",
    "\n",
    "# Plot examples\n",
    "fig = plot_survival_features(dataset, sample_indices=[0, 1, 2, 3, 4])\n",
    "plt.show()\n",
    "\n",
    "# Plot KM curve for the dataset\n",
    "survival_df = pd.DataFrame({\n",
    "    'observed_time': dataset.observed_times.numpy(),\n",
    "    'event_observed': dataset.events.numpy()\n",
    "})\n",
    "plot_KM(survival_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data\n",
    "dataset.static_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 2. Survival Analysis with ukko Model\n",
    "\n",
    "Now we'll implement a survival analysis using the ukko dual attention architecture. The key components are:\n",
    "\n",
    "1. UkkoSurvivalModel class that combines the dual attention network with a survival head\n",
    "2. Training loop with Cox partial likelihood loss\n",
    "3. Risk stratification and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ukko.core import DualAttentionRegressor1\n",
    "import copy\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchsurv.loss.cox as tscox\n",
    "\n",
    "# Define the Ukko Survival model\n",
    "class UkkoSurvivalModel(nn.Module):\n",
    "    def __init__(self, sequence_length, n_features, \n",
    "                 d_model=64, n_heads=4, dropout=0.1, n_modules=3):\n",
    "        super().__init__()\n",
    "        self.attention_net = DualAttentionRegressor1(\n",
    "            n_features=n_features,\n",
    "            time_steps=sequence_length,\n",
    "            d_model=d_model, #128,\n",
    "            n_heads=n_heads,\n",
    "            dropout=dropout,\n",
    "            n_modules=n_modules\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Get predictions from attention network\n",
    "        predictions, feat_weights, time_weights = self.attention_net(x)\n",
    "        return predictions, feat_weights, time_weights\n",
    "\n",
    "# DEPRECIATED: Generating a dataloader using TensorDataset:\n",
    "#              Instead use `SurvivalSineWaveDataset` directly with DataLoader\n",
    "# # Prepare data for training\n",
    "# X = torch.FloatTensor(dataset.data.numpy())\n",
    "# times = torch.FloatTensor(dataset.observed_times.numpy())\n",
    "# events = torch.FloatTensor(dataset.events.numpy())\n",
    "# # Create data loader\n",
    "# survival_dataset = TensorDataset(X, times, events)\n",
    "# batch_size = 32\n",
    "# train_loader = DataLoader(survival_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "batch_size =  dataset.__len__()\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, n_epochs=30, device='cuda', patience=5):\n",
    "    \"\"\"\n",
    "    Train the survival model with validation monitoring.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to train\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        optimizer: PyTorch optimizer\n",
    "        n_epochs: Number of training epochs\n",
    "        device: Device to train on ('cuda' or 'cpu')\n",
    "        patience: Number of epochs to wait for improvement before early stopping\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained model (best version)\n",
    "        history: Dict containing training and validation losses\n",
    "        best_val_loss: Best validation loss achieved\n",
    "    \"\"\"\n",
    "    # Setup device\n",
    "    device = torch.device(device if torch.cuda.is_available() and device=='cuda' else 'cpu')\n",
    "    model = model.to(device)\n",
    "    print(f'Training on {device}')\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "      print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    def evaluate(model, data_loader):\n",
    "        \"\"\"Evaluate model on given data loader\"\"\"\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, static, times_batch, events_batch in data_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                times_batch = times_batch.to(device)\n",
    "                events_batch = events_batch.to(device)\n",
    "                \n",
    "                log_hazards, _, _ = model(X_batch)\n",
    "                loss = tscox.neg_partial_log_likelihood(\n",
    "                    log_hazards, events_batch==1, times_batch, \n",
    "                    ties_method='efron', reduction='mean'\n",
    "                )\n",
    "                total_loss += loss.item()\n",
    "        return total_loss / len(data_loader)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for X_batch, static, times_batch, events_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            times_batch = times_batch.to(device)\n",
    "            events_batch = events_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            log_hazards, feat_weights, time_weights = model(X_batch)\n",
    "            \n",
    "            loss = tscox.neg_partial_log_likelihood(\n",
    "                log_hazards, events_batch==1, times_batch, \n",
    "                ties_method='efron', reduction='mean'\n",
    "            )\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Calculate average losses\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        val_loss = evaluate(model, val_loader)\n",
    "        \n",
    "        # Store losses\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f'Epoch {epoch+1}/{n_epochs} Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Best Val Loss: {best_val_loss:.4f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f'\\nEarly stopping after {epoch + 1} epochs')\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, history, best_val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pred data loader:\n",
    "\n",
    "# Create train/val split\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Calculate split sizes\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.5 * total_size)\n",
    "val_size = total_size - train_size\n",
    "print(f\"Total samples: {total_size}, Train size: {train_size}, Val size: {val_size}\")\n",
    "\n",
    "# Create splits\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset_lin, #survival_dataset, \n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = train_dataset.__len__() #64\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = UkkoSurvivalModel(sequence_length=dataset.sequence_length, n_features=dataset.n_features, \n",
    "                          d_model=64, n_heads=4, dropout=0.2, n_modules=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Train model\n",
    "model, history, best_val_loss = train_model(\n",
    "    model, train_loader, val_loader, optimizer,\n",
    "    n_epochs=3000,\n",
    "    patience=3000\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.axhline(y=best_val_loss, color='r', linestyle='--', label='Best Val Loss')\n",
    "plt.title('Training History - Ukko Survival Model')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratify_and_plot(survival_data, risk_scores, n_groups=4):\n",
    "    \"\"\"Stratify patients into multiple risk groups and plot KM curves.\"\"\"\n",
    "    survival_data = survival_data.copy()\n",
    "    survival_data['predicted_risk'] = risk_scores\n",
    "    \n",
    "    # Calculate quantile cutoffs\n",
    "    quantiles = np.linspace(0, 1, n_groups + 1)\n",
    "    cutoffs = np.quantile(survival_data['predicted_risk'], quantiles)\n",
    "    \n",
    "    # Plot KM curves\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create color map for groups\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, n_groups))\n",
    "    \n",
    "    for i in range(n_groups):\n",
    "        if i == 0:\n",
    "            mask = survival_data['predicted_risk'] <= cutoffs[1]\n",
    "            group_label = f'Lowest Risk (Q1)'\n",
    "        elif i == n_groups - 1:\n",
    "            mask = survival_data['predicted_risk'] > cutoffs[-2]\n",
    "            group_label = f'Highest Risk (Q{n_groups})'\n",
    "        else:\n",
    "            mask = (survival_data['predicted_risk'] > cutoffs[i]) & \\\n",
    "                   (survival_data['predicted_risk'] <= cutoffs[i+1])\n",
    "            group_label = f'Q{i+1}'\n",
    "        \n",
    "        group = survival_data[mask]\n",
    "        \n",
    "        kmf = KaplanMeierFitter()\n",
    "        kmf.fit(durations=group['observed_time'],\n",
    "                event_observed=group['event_observed'],\n",
    "                label=group_label)\n",
    "        kmf.plot_survival_function(show_censors=True, ci_show=False, color=colors[i])\n",
    "        \n",
    "        print(f\"{group_label} size: {len(group)}\")\n",
    "    \n",
    "    plt.title('Kaplan-Meier Survival Curves by Risk Group')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Survival Probability')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "    \n",
    "    return fig, cutoffs\n",
    "\n",
    "# Model performance evaluation:\n",
    "# For TensorDataset use .tesnsors, for SurvivalSineWaveDataset use .data\n",
    "# X =  train_loader.dataset.tensors[0].cpu() \n",
    "indices = train_loader.dataset.indices\n",
    "X = train_loader.dataset.dataset.data[indices].data.cpu()  # get features of training set\n",
    "print(f\"X.shape: {X.shape}\")\n",
    "#.data.cpu()\n",
    "\n",
    "# Generate risk scores using the trained model\n",
    "model.eval().to('cpu')\n",
    "with torch.no_grad():\n",
    "    risk_scores, feat_weights, time_weights = model(X)\n",
    "    risk_scores = risk_scores.numpy()\n",
    "\n",
    "# Create stratification plot\n",
    "sur_df = pd.DataFrame({\n",
    "    'observed_time': dataset.observed_times[indices].numpy(),\n",
    "    'event_observed': dataset.events[indices].numpy()\n",
    "})\n",
    "fig, cutoffs = stratify_and_plot(sur_df, risk_scores, n_groups=5)\n",
    "fig.gca().set_title('Kaplan-Meier Curves by Risk Group - Train Set')\n",
    "\n",
    "# Val set:\n",
    "indices = val_loader.dataset.indices\n",
    "X = val_loader.dataset.dataset.data[indices].data.cpu()  # get features of training set\n",
    "print(f\"X.shape: {X.shape}\")\n",
    "#.data.cpu()\n",
    "\n",
    "# Generate risk scores using the trained model\n",
    "model.eval().to('cpu')\n",
    "with torch.no_grad():\n",
    "    risk_scores, feat_weights, time_weights = model(X)\n",
    "    risk_scores = risk_scores.numpy()\n",
    "\n",
    "# Create stratification plot\n",
    "sur_df = pd.DataFrame({\n",
    "    'observed_time': dataset.observed_times[indices].numpy(),\n",
    "    'event_observed': dataset.events[indices].numpy()\n",
    "})\n",
    "fig, cutoffs = stratify_and_plot(sur_df, risk_scores, n_groups=5)\n",
    "fig.gca().set_title('Kaplan-Meier Curves by Risk Group - Validation Set')\n",
    "\n",
    "# # Visualize attention weights\n",
    "# plt.figure(figsize=(15, 5))\n",
    "\n",
    "# # Feature attention weights - reshape for visualization\n",
    "# feat_attention = feat_weights.detach().numpy()\n",
    "# # print dim of feat_attention \n",
    "# print(\"Feature attention shape:\", feat_attention.shape)  # Should be [batch_size, n_features, sequence_length]\n",
    "\n",
    "# # Plot feature attention weights\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.imshow(feat_attention.T)\n",
    "# plt.title('Feature Attention Weights')\n",
    "# plt.xlabel('Sample')\n",
    "# plt.ylabel('Feature')\n",
    "# plt.colorbar()\n",
    "\n",
    "# # Time attention weights\n",
    "# time_attention = time_weights.detach().numpy().mean(axis=0)  # Average across batch\n",
    "\n",
    "# # Plot time attention weights\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(time_attention)\n",
    "# plt.title('Time Attention Weights (Average)')\n",
    "# plt.xlabel('Time Step')\n",
    "# plt.ylabel('Weight')\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Non Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pred data loader:\n",
    "\n",
    "# Create train/val split\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Calculate split sizes\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.5 * total_size)\n",
    "val_size = total_size - train_size\n",
    "print(f\"Total samples: {total_size}, Train size: {train_size}, Val size: {val_size}\")\n",
    "\n",
    "# Create splits\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset, #survival_dataset, \n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = train_dataset.__len__() #64\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = UkkoSurvivalModel(sequence_length=dataset.sequence_length, n_features=dataset.n_features, \n",
    "                          d_model=64, n_heads=4, dropout=0.2, n_modules=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Train model\n",
    "model, history, best_val_loss = train_model(\n",
    "    model, train_loader, val_loader, optimizer,\n",
    "    n_epochs=3000,\n",
    "    patience=3000\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.axhline(y=best_val_loss, color='r', linestyle='--', label='Best Val Loss')\n",
    "plt.title('Training History - Ukko Survival Model')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratify_and_plot(survival_data, risk_scores, n_groups=4):\n",
    "    \"\"\"Stratify patients into multiple risk groups and plot KM curves.\"\"\"\n",
    "    survival_data = survival_data.copy()\n",
    "    survival_data['predicted_risk'] = risk_scores\n",
    "    \n",
    "    # Calculate quantile cutoffs\n",
    "    quantiles = np.linspace(0, 1, n_groups + 1)\n",
    "    cutoffs = np.quantile(survival_data['predicted_risk'], quantiles)\n",
    "    \n",
    "    # Plot KM curves\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create color map for groups\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, n_groups))\n",
    "    \n",
    "    for i in range(n_groups):\n",
    "        if i == 0:\n",
    "            mask = survival_data['predicted_risk'] <= cutoffs[1]\n",
    "            group_label = f'Lowest Risk (Q1)'\n",
    "        elif i == n_groups - 1:\n",
    "            mask = survival_data['predicted_risk'] > cutoffs[-2]\n",
    "            group_label = f'Highest Risk (Q{n_groups})'\n",
    "        else:\n",
    "            mask = (survival_data['predicted_risk'] > cutoffs[i]) & \\\n",
    "                   (survival_data['predicted_risk'] <= cutoffs[i+1])\n",
    "            group_label = f'Q{i+1}'\n",
    "        \n",
    "        group = survival_data[mask]\n",
    "        \n",
    "        kmf = KaplanMeierFitter()\n",
    "        kmf.fit(durations=group['observed_time'],\n",
    "                event_observed=group['event_observed'],\n",
    "                label=group_label)\n",
    "        kmf.plot_survival_function(show_censors=True, ci_show=False, color=colors[i])\n",
    "        \n",
    "        print(f\"{group_label} size: {len(group)}\")\n",
    "    \n",
    "    plt.title('Kaplan-Meier Survival Curves by Risk Group')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Survival Probability')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "    \n",
    "    return fig, cutoffs\n",
    "\n",
    "# Model performance evaluation:\n",
    "# For TensorDataset use .tesnsors, for SurvivalSineWaveDataset use .data\n",
    "# X =  train_loader.dataset.tensors[0].cpu() \n",
    "indices = train_loader.dataset.indices\n",
    "X = train_loader.dataset.dataset.data[indices].data.cpu()  # get features of training set\n",
    "print(f\"X.shape: {X.shape}\")\n",
    "#.data.cpu()\n",
    "\n",
    "# Generate risk scores using the trained model\n",
    "model.eval().to('cpu')\n",
    "with torch.no_grad():\n",
    "    risk_scores, feat_weights, time_weights = model(X)\n",
    "    risk_scores = risk_scores.numpy()\n",
    "\n",
    "# Create stratification plot\n",
    "sur_df = pd.DataFrame({\n",
    "    'observed_time': dataset.observed_times[indices].numpy(),\n",
    "    'event_observed': dataset.events[indices].numpy()\n",
    "})\n",
    "fig, cutoffs = stratify_and_plot(sur_df, risk_scores, n_groups=5)\n",
    "fig.gca().set_title('Kaplan-Meier Curves by Risk Group - Train Set')\n",
    "\n",
    "# Val set:\n",
    "indices = val_loader.dataset.indices\n",
    "X = val_loader.dataset.dataset.data[indices].data.cpu()  # get features of training set\n",
    "print(f\"X.shape: {X.shape}\")\n",
    "#.data.cpu()\n",
    "\n",
    "# Generate risk scores using the trained model\n",
    "model.eval().to('cpu')\n",
    "with torch.no_grad():\n",
    "    risk_scores, feat_weights, time_weights = model(X)\n",
    "    risk_scores = risk_scores.numpy()\n",
    "\n",
    "# Create stratification plot\n",
    "sur_df = pd.DataFrame({\n",
    "    'observed_time': dataset.observed_times[indices].numpy(),\n",
    "    'event_observed': dataset.events[indices].numpy()\n",
    "})\n",
    "fig, cutoffs = stratify_and_plot(sur_df, risk_scores, n_groups=5)\n",
    "fig.gca().set_title('Kaplan-Meier Curves by Risk Group - Validation Set')\n",
    "\n",
    "# # Visualize attention weights\n",
    "# plt.figure(figsize=(15, 5))\n",
    "\n",
    "# # Feature attention weights - reshape for visualization\n",
    "# feat_attention = feat_weights.detach().numpy()\n",
    "# # print dim of feat_attention \n",
    "# print(\"Feature attention shape:\", feat_attention.shape)  # Should be [batch_size, n_features, sequence_length]\n",
    "\n",
    "# # Plot feature attention weights\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.imshow(feat_attention.T)\n",
    "# plt.title('Feature Attention Weights')\n",
    "# plt.xlabel('Sample')\n",
    "# plt.ylabel('Feature')\n",
    "# plt.colorbar()\n",
    "\n",
    "# # Time attention weights\n",
    "# time_attention = time_weights.detach().numpy().mean(axis=0)  # Average across batch\n",
    "\n",
    "# # Plot time attention weights\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(time_attention)\n",
    "# plt.title('Time Attention Weights (Average)')\n",
    "# plt.xlabel('Time Step')\n",
    "# plt.ylabel('Weight')\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.observed_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot relationship between survival time and first feature amplitude\n",
    "def plot_amplitude_survival_relationship(dataset,ax=None):\n",
    "    \"\"\"Plot scatter of survival times vs amplitude of first feature\"\"\"\n",
    "    \n",
    "    # Get amplitude of first feature for each sample (max absolute value)\n",
    "    amplitudes = dataset.data[:, 0].abs().max(dim=1)[0]\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Plot censored and uncensored separately\n",
    "    censored = dataset.events == 0\n",
    "    uncensored = dataset.events == 1\n",
    "    \n",
    "    ax.scatter(dataset.observed_times[censored], amplitudes[censored], \n",
    "                alpha=0.2, label='Censored', marker='s')\n",
    "    ax.scatter(dataset.observed_times[uncensored], amplitudes[uncensored], \n",
    "             alpha=0.2, label='Uncensored', marker='o')\n",
    "    ax.set_xlabel('Survival Time')\n",
    "    ax.set_ylabel('Feature 1 Amplitude')\n",
    "    ax.set_title('Relationship between Survival Time and Feature 1 Amplitude')\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add correlation coefficient for uncensored samples\n",
    "    corr = np.corrcoef(dataset.observed_times[uncensored].numpy(), \n",
    "                       amplitudes[uncensored].numpy())[0,1]\n",
    "    ax.text(0.05, 0.95, f'Correlation (uncensored): {corr:.3f}', \n",
    "             transform=ax.transAxes)\n",
    "\n",
    "\n",
    "def plot_risk_survival_relationship(risk_scores, dataset, ax=None):\n",
    "    \"\"\"Plot scatter of survival times vs predicted risk scores\"\"\"\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Plot censored and uncensored separately\n",
    "    censored = dataset.events == 0\n",
    "    uncensored = dataset.events == 1\n",
    "    \n",
    "    # Convert risk scores to numpy if they're tensors\n",
    "    if torch.is_tensor(risk_scores):\n",
    "        risk_scores = risk_scores.numpy()\n",
    "    \n",
    "    # Flatten risk scores if needed\n",
    "    if len(risk_scores.shape) > 1:\n",
    "        risk_scores = risk_scores.flatten()\n",
    "    \n",
    "    ax.scatter(dataset.observed_times[censored], risk_scores[censored], \n",
    "               alpha=0.2, label='Censored', marker='s')\n",
    "    ax.scatter(dataset.observed_times[uncensored], risk_scores[uncensored], \n",
    "               alpha=0.2, label='Uncensored', marker='o')\n",
    "    \n",
    "    ax.set_xlabel('Survival Time')\n",
    "    ax.set_ylabel('Predicted Risk Score')\n",
    "    ax.set_title('Relationship between Survival Time and Predicted Risk')\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add correlation coefficient for uncensored samples\n",
    "    corr = np.corrcoef(dataset.observed_times[uncensored].numpy(), \n",
    "                       risk_scores[uncensored])[0,1]\n",
    "    ax.text(0.05, 0.95, f'Correlation (uncensored): {corr:.3f}', \n",
    "            transform=ax.transAxes)\n",
    "    \n",
    "\n",
    "# Get risk scores from the trained model\n",
    "#model.eval()\n",
    "#with torch.no_grad():\n",
    "#    risk_scores, _, _ = model(X)\n",
    "\n",
    "# Create the plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "#\n",
    "plot_amplitude_survival_relationship(dataset, ax=ax1)\n",
    "#  assumes that model has been evaluated and risk_scores are available\n",
    "plot_risk_survival_relationship(risk_scores, dataset, ax=ax2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot relationship between survival time and first feature amplitude\n",
    "def plot_amplitude_survival_relationship(dataset):\n",
    "    \"\"\"Plot scatter of survival times vs amplitude of first feature and true coefficients\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # First subplot - Amplitude relationship\n",
    "    # Get amplitude of first feature for each sample (max absolute value)\n",
    "    amplitudes = dataset.data[:, 0].abs().max(dim=1)[0]\n",
    "    \n",
    "    # Plot censored and uncensored separately\n",
    "    censored = dataset.events == 0\n",
    "    uncensored = dataset.events == 1\n",
    "    \n",
    "    ax1.scatter(dataset.observed_times[censored], amplitudes[censored], \n",
    "                alpha=0.2, label='Censored', marker='s')\n",
    "    ax1.scatter(dataset.observed_times[uncensored], amplitudes[uncensored], \n",
    "                alpha=0.2, label='Uncensored', marker='o')\n",
    "    \n",
    "    ax1.set_xlabel('Survival Time')\n",
    "    ax1.set_ylabel('Feature 1 Amplitude')\n",
    "    ax1.set_title('Relationship between Survival Time\\nand Feature 1 Amplitude')\n",
    "    ax1.grid(True)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Add correlation coefficient for uncensored samples\n",
    "    corr = np.corrcoef(dataset.observed_times[uncensored].numpy(), \n",
    "                       amplitudes[uncensored].numpy())[0,1]\n",
    "    ax1.text(0.05, 0.95, f'Correlation (uncensored): {corr:.3f}', \n",
    "             transform=ax1.transAxes)\n",
    "    \n",
    "    # Second subplot - True coefficients relationship\n",
    "    first_coef = dataset.static_features[:, 0]  # Get first static feature\n",
    "    \n",
    "    ax2.scatter(dataset.observed_times[censored], first_coef[censored], \n",
    "                alpha=0.2, label='Censored', marker='s')\n",
    "    ax2.scatter(dataset.observed_times[uncensored], first_coef[uncensored], \n",
    "                alpha=0.2, label='Uncensored', marker='o')\n",
    "    \n",
    "    ax2.set_xlabel('Survival Time')\n",
    "    ax2.set_ylabel('First True Coefficient')\n",
    "    ax2.set_title('Relationship between Survival Time\\nand First True Coefficient')\n",
    "    ax2.grid(True)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Add correlation coefficient for uncensored samples\n",
    "    corr = np.corrcoef(dataset.observed_times[uncensored].numpy(), \n",
    "                       first_coef[uncensored].numpy())[0,1]\n",
    "    ax2.text(0.05, 0.95, f'Correlation (uncensored): {corr:.3f}', \n",
    "             transform=ax2.transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create the plot\n",
    "plot_amplitude_survival_relationship(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions:\n",
    "\n",
    "def inspect_dataloader(loader):\n",
    "    \"\"\"Inspect a DataLoader's contents and structure\"\"\"\n",
    "    \n",
    "    # Get basic info\n",
    "    print(\"DataLoader Info:\")\n",
    "    print(f\"Number of batches: {len(loader)}\")\n",
    "    print(f\"Batch size: {loader.batch_size}\")\n",
    "    print(f\"Dataset size: {len(loader.dataset)}\")\n",
    "    \n",
    "    # Get first batch\n",
    "    print(\"\\nFirst batch contents:\")\n",
    "    batch = next(iter(loader))\n",
    "    print(f\"Number of items in batch: {len(batch)}\")\n",
    "    \n",
    "    # Print each tensor's shape and type\n",
    "    for i, tensor in enumerate(batch):\n",
    "        print(f\"\\nItem {i}:\")\n",
    "        print(f\"Shape: {tensor.shape}\")\n",
    "        print(f\"Type: {tensor.dtype}\")\n",
    "        print(f\"First element:\\n{tensor[0]}\")\n",
    "\n",
    "# Test the loader\n",
    "#train_loader, val_loader = search._get_dataloaders(64)\n",
    "#inspect_dataloader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create search instance\n",
    "# search = HyperparameterSearch(dataset)\n",
    "# train_loader, val_loader = search._get_dataloaders(64)\n",
    "# inspect_dataloader(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import Dict, List, Union, Tuple\n",
    "import random\n",
    "\n",
    "class HyperparameterSearch:\n",
    "    def __init__(self, dataset, train_val_split=0.8, seed=42):\n",
    "        self.split_ratio = train_val_split\n",
    "        self.seed = seed\n",
    "        \n",
    "        # ToDO: This is suboptimal. filx later to use dataset direclty. \n",
    "        # Should return three elements, but does 4, inlcuding true coefficients\n",
    "        self.dataset = dataset\n",
    "        # Prepare data for training\n",
    "        #X = torch.FloatTensor(dataset.data.numpy())\n",
    "        #times = torch.FloatTensor(dataset.observed_times.numpy())\n",
    "        #events = torch.FloatTensor(dataset.events.numpy())\n",
    "        #self.dataset = TensorDataset(X, times, events)\n",
    "        \n",
    "        \n",
    "        # Define parameter search spaces\n",
    "        self.param_grid = {\n",
    "            # Model Architecture\n",
    "            'd_model': [16, 32, 64, 128],\n",
    "            'n_heads': [2, 4, 8],\n",
    "            'dropout': [0.1, 0.2, 0.3, 0.4],\n",
    "            'n_modules': [1, 2, 3, 4],\n",
    "            # Training\n",
    "            'learning_rate': [0.0001, 0.0005, 0.001, 0.002, 0.005],\n",
    "            'batch_size': [16, 32, 64, 128],\n",
    "            'patience': [5, 7, 10]\n",
    "        }\n",
    "    \n",
    "    def _get_dataloaders(self, batch_size: int) -> Tuple[DataLoader, DataLoader]:\n",
    "        \"\"\"Create train and validation dataloaders\"\"\"\n",
    "        # Create train/val split\n",
    "        total_size = len(self.dataset)\n",
    "        train_size = int(self.split_ratio * total_size)\n",
    "        val_size = total_size - train_size\n",
    "        \n",
    "        train_dataset, val_dataset = random_split(\n",
    "            self.dataset,\n",
    "            [train_size, val_size],\n",
    "            generator=torch.Generator().manual_seed(self.seed)\n",
    "        )\n",
    "        \n",
    "        return (\n",
    "            DataLoader(train_dataset, batch_size=batch_size, shuffle=True),\n",
    "            DataLoader(val_dataset, batch_size=batch_size)\n",
    "        )\n",
    "    \n",
    "    def _train_and_evaluate(self, params: Dict) -> float:\n",
    "        \"\"\"Train model with given parameters and return best validation loss\"\"\"\n",
    "        # Create model with architecture params\n",
    "        model = UkkoSurvivalModel(\n",
    "            sequence_length=self.dataset.sequence_length,\n",
    "            n_features=self.dataset.n_features,\n",
    "            d_model = params['d_model'],\n",
    "            n_heads = params['n_heads'],\n",
    "            dropout = params['dropout'],\n",
    "            n_modules = params['n_modules']\n",
    "        )\n",
    "        \n",
    "        # Get dataloaders\n",
    "        train_loader, val_loader = self._get_dataloaders(params['batch_size'])\n",
    "        \n",
    "        # Create optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "        \n",
    "        # Train model\n",
    "        trained_model, history, best_val_loss = train_model(\n",
    "            model, train_loader, val_loader, optimizer,\n",
    "            n_epochs=5,  # Maximum epochs\n",
    "            patience=params['patience']\n",
    "        )\n",
    "        \n",
    "        return best_val_loss\n",
    "    \n",
    "    def grid_search(self, max_combinations: int = None) -> List[Dict]:\n",
    "        \"\"\"Perform grid search over parameter space\"\"\"\n",
    "        # Get all combinations\n",
    "        keys = self.param_grid.keys()\n",
    "        values = self.param_grid.values()\n",
    "        combinations = list(itertools.product(*values))\n",
    "        \n",
    "        # If max_combinations specified, randomly sample combinations\n",
    "        if max_combinations and len(combinations) > max_combinations:\n",
    "            combinations = random.sample(combinations, max_combinations)\n",
    "        \n",
    "        results = []\n",
    "        total = len(combinations)\n",
    "        \n",
    "        for i, combo in enumerate(combinations, 1):\n",
    "            params = dict(zip(keys, combo))\n",
    "            val_loss = self._train_and_evaluate(params)\n",
    "            \n",
    "            result = {**params, 'val_loss': val_loss}\n",
    "            results.append(result)\n",
    "            \n",
    "            print(f'\\nProgress: {i}/{total}')\n",
    "            print(f'Parameters: {params}')\n",
    "            print(f'Validation Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Sort by validation loss\n",
    "        results.sort(key=lambda x: x['val_loss'])\n",
    "        return results\n",
    "    \n",
    "    def random_search(self, n_trials: int) -> List[Dict]:\n",
    "        \"\"\"Perform random search over parameter space\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i in range(n_trials):\n",
    "            # Randomly sample parameters\n",
    "            params = {\n",
    "                key: random.choice(values) \n",
    "                for key, values in self.param_grid.items()\n",
    "            }\n",
    "            print(params)\n",
    "            \n",
    "            val_loss = self._train_and_evaluate(params)\n",
    "            \n",
    "            result = {**params, 'val_loss': val_loss}\n",
    "            results.append(result)\n",
    "            \n",
    "            print(f'\\nTrial {i+1}/{n_trials}')\n",
    "            print(f'Parameters: {params}')\n",
    "            print(f'Validation Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Sort by validation loss\n",
    "        results.sort(key=lambda x: x['val_loss'])\n",
    "        return results\n",
    "\n",
    "# Example usage\n",
    "def run_hyperparameter_search(search_type: str, **kwargs):\n",
    "    \"\"\"Run hyperparameter search with specified method\n",
    "    \n",
    "    Args:\n",
    "        search_type: Either 'grid' or 'random'\n",
    "        **kwargs: Additional arguments for search method\n",
    "            - For grid search: max_combinations (optional)\n",
    "            - For random search: n_trials (required)\n",
    "    \"\"\"\n",
    "    # Create search instance\n",
    "    search = HyperparameterSearch(dataset)\n",
    "    \n",
    "    # Run specified search type\n",
    "    if search_type.lower() == 'grid':\n",
    "        results = search.grid_search(max_combinations=kwargs.get('max_combinations'))\n",
    "    elif search_type.lower() == 'random':\n",
    "        if 'n_trials' not in kwargs:\n",
    "            raise ValueError(\"n_trials must be specified for random search\")\n",
    "        results = search.random_search(n_trials=kwargs['n_trials'])\n",
    "    else:\n",
    "        raise ValueError(\"search_type must be either 'grid' or 'random'\")\n",
    "    \n",
    "    # Print best results\n",
    "    print('\\nBest Parameters:')\n",
    "    best_params = results[0]\n",
    "    for key, value in best_params.items():\n",
    "        print(f'{key}: {value}')\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Run random search with 10 trials\n",
    "results = run_hyperparameter_search('random', n_trials=5)\n",
    "\n",
    "# Example: Run grid search with max 20 combinations\n",
    "# results = run_hyperparameter_search('grid', max_combinations=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put results into a  df:\n",
    "results = pd.DataFrame(results)\n",
    "results.sort_values(by='val_loss', inplace=True)\n",
    "results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hyperparameter_results(results: List[Dict]):\n",
    "    \"\"\"Visualize hyperparameter search results.\n",
    "    \n",
    "    Args:\n",
    "        results: List of dictionaries containing parameters and validation loss\n",
    "    \"\"\"\n",
    "    # Convert results to DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate parameter importance (correlation with validation loss)\n",
    "    param_importance = {}\n",
    "    for param in df_results.columns:\n",
    "        if param != 'val_loss':\n",
    "            correlation = df_results[param].corr(df_results['val_loss'])\n",
    "            param_importance[param] = abs(correlation)\n",
    "    \n",
    "    # Sort parameters by importance\n",
    "    param_importance = dict(sorted(param_importance.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    # Create subplots\n",
    "    n_params = len(param_importance)\n",
    "    fig = plt.figure(figsize=(15, 5 * ((n_params + 1) // 2)))\n",
    "    \n",
    "    # 1. Parameter Importance Plot\n",
    "    plt.subplot(((n_params + 1) // 2), 2, 1)\n",
    "    plt.bar(range(len(param_importance)), param_importance.values())\n",
    "    plt.xticks(range(len(param_importance)), param_importance.keys(), rotation=45)\n",
    "    plt.title('Parameter Importance (Correlation with Validation Loss)')\n",
    "    plt.ylabel('Absolute Correlation')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 2. Individual Parameter vs Validation Loss\n",
    "    for i, (param, importance) in enumerate(param_importance.items(), 2):\n",
    "        plt.subplot(((n_params + 1) // 2), 2, i)\n",
    "        \n",
    "        if df_results[param].dtype in [np.float64, np.int64]:\n",
    "            # For numeric parameters, use scatter plot\n",
    "            plt.scatter(df_results[param], df_results['val_loss'], alpha=0.5)\n",
    "            plt.xlabel(param)\n",
    "            plt.ylabel('Validation Loss')\n",
    "        else:\n",
    "            # For categorical parameters, use box plot\n",
    "            df_results.boxplot(column='val_loss', by=param, ax=plt.gca())\n",
    "            plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.title(f'{param} vs Validation Loss')\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print top 5 configurations\n",
    "    print('\\nTop 5 Configurations:')\n",
    "    top_5 = df_results.nsmallest(5, 'val_loss')\n",
    "    for i, row in top_5.iterrows():\n",
    "        print(f'\\nRank {i+1}:')\n",
    "        for col in row.index:\n",
    "            print(f'{col}: {row[col]:.4f}' if col == 'val_loss' else f'{col}: {row[col]}')\n",
    "\n",
    "def plot_parameter_interactions(results: List[Dict], top_k: int = 3):\n",
    "    \"\"\"Visualize interactions between most important parameters.\n",
    "    \n",
    "    Args:\n",
    "        results: List of dictionaries containing parameters and validation loss\n",
    "        top_k: Number of most important parameters to analyze\n",
    "    \"\"\"\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate parameter importance\n",
    "    param_importance = {}\n",
    "    for param in df_results.columns:\n",
    "        if param != 'val_loss':\n",
    "            correlation = df_results[param].corr(df_results['val_loss'])\n",
    "            param_importance[param] = abs(correlation)\n",
    "    \n",
    "    # Get top k parameters\n",
    "    top_params = sorted(param_importance.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    top_param_names = [p[0] for p in top_params]\n",
    "    \n",
    "    # Create interaction plots\n",
    "    fig = plt.figure(figsize=(15, 5 * (top_k - 1)))\n",
    "    plot_idx = 1\n",
    "    \n",
    "    for i in range(top_k - 1):\n",
    "        for j in range(i + 1, top_k):\n",
    "            param1, param2 = top_param_names[i], top_param_names[j]\n",
    "            \n",
    "            plt.subplot((top_k - 1), 1, plot_idx)\n",
    "            scatter = plt.scatter(\n",
    "                df_results[param1],\n",
    "                df_results[param2],\n",
    "                c=df_results['val_loss'],\n",
    "                cmap='viridis',\n",
    "                alpha=0.6\n",
    "            )\n",
    "            plt.colorbar(scatter, label='Validation Loss')\n",
    "            plt.xlabel(param1)\n",
    "            plt.ylabel(param2)\n",
    "            plt.title(f'Interaction between {param1} and {param2}')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plot_idx += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Modify run_hyperparameter_search to include visualizations\n",
    "def run_hyperparameter_search(search_type: str, **kwargs):\n",
    "    \"\"\"Run hyperparameter search with specified method and visualize results\n",
    "    \n",
    "    Args:\n",
    "        search_type: Either 'grid' or 'random'\n",
    "        **kwargs: Additional arguments for search method\n",
    "            - For grid search: max_combinations (optional)\n",
    "            - For random search: n_trials (required)\n",
    "    \"\"\"\n",
    "    # Create search instance\n",
    "    search = HyperparameterSearch(dataset)\n",
    "    \n",
    "    # Run specified search type\n",
    "    if search_type.lower() == 'grid':\n",
    "        results = search.grid_search(max_combinations=kwargs.get('max_combinations'))\n",
    "    elif search_type.lower() == 'random':\n",
    "        if 'n_trials' not in kwargs:\n",
    "            raise ValueError(\"n_trials must be specified for random search\")\n",
    "        results = search.random_search(n_trials=kwargs['n_trials'])\n",
    "    else:\n",
    "        raise ValueError(\"search_type must be either 'grid' or 'random'\")\n",
    "    \n",
    "    # Visualize results\n",
    "    print('\\nParameter Importance and Individual Effects:')\n",
    "    plot_hyperparameter_results(results)\n",
    "    \n",
    "    print('\\nParameter Interactions:')\n",
    "    plot_parameter_interactions(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# results = run_hyperparameter_search('random', n_trials=20)\n",
    "# results = run_hyperparameter_search('grid', max_combinations=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
