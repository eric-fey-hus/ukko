{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80c45e9",
   "metadata": {},
   "source": [
    "Compare AdamW reg parameter for a fixed architecture that overfits with ADAM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbcf2db",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c42c968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import ukko \n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsurv.loss.cox import neg_partial_log_likelihood\n",
    "from torchsurv.loss.weibull import neg_log_likelihood, log_hazard, survival_function\n",
    "from torchsurv.metrics.brier_score import BrierScore\n",
    "from torchsurv.metrics.cindex import ConcordanceIndex\n",
    "from torchsurv.metrics.auc import Auc\n",
    "#from torchsurv.stats.kaplan_meier import KaplanMeierEstimator\n",
    "# for cuda cleanups:\n",
    "import gc\n",
    "\n",
    "print(\"Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d67398",
   "metadata": {},
   "source": [
    "### Custom function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12e552ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, val_losses, title: str = \"Cox\") -> None:\n",
    "\n",
    "    #train_losses = torch.stack(train_losses) / train_losses[0]\n",
    "    #val_losses = torch.stack(val_losses) / val_losses[0]\n",
    "    train_losses = np.array(train_losses)\n",
    "    val_losses = np.array(val_losses)\n",
    "    train_losses = train_losses / train_losses[0]\n",
    "    val_losses = val_losses / val_losses[0]\n",
    "\n",
    "    plt.plot(train_losses, label=\"training\")\n",
    "    plt.plot(val_losses, label=\"validation\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Normalized loss\")\n",
    "    plt.title(title)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class ukkosurv_dataset(Dataset):\n",
    "    \"\"\" \"Custom dataset for ukko-torcsurv use in df format\"\"\"\n",
    "\n",
    "    # defining values in the constructor\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        #self.df = df\n",
    "        df_x, data_3d = ukko.utils.convert_to_3d_df(df.iloc[:,3:].fillna(-1))\n",
    "        df_y = df_train.iloc[:,:3]\n",
    "        \n",
    "        self.df_y = df_y        # Dataframe with survival data, e.g. OSS_status, OSS_days\n",
    "        self.data_3d = data_3d  # numpy array with 3D feature data: patients, features, time \n",
    "\n",
    "\n",
    "    # Getting data size/length\n",
    "    def __len__(self):\n",
    "        return len(self.data_3d)\n",
    "\n",
    "    # Getting the data samples\n",
    "    def __getitem__(self, idx):\n",
    "        y = self.df_y.iloc[idx,:]\n",
    "        # Targets\n",
    "        event = torch.tensor(y[\"OSS_status\"]).bool()\n",
    "        time = torch.tensor(y[\"OSS_days\"]).float()\n",
    "        # Predictors\n",
    "        x = torch.tensor(self.data_3d[idx,:,:]).float()\n",
    "        return x, (event, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c311d1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function\n",
    "import copy\n",
    "def train_model_simple(\n",
    "            model,\n",
    "            dataloader_train,\n",
    "            dataloader_val,\n",
    "            optimizer = None,\n",
    "            n_epochs = 100,\n",
    "            learning_rate = 0.01,\n",
    "            device='cuda'\n",
    "        ):\n",
    "\n",
    "    dtype=torch.float32\n",
    "    \n",
    "    # Initialize optimizer if not provided\n",
    "    #if optimizer is None:\n",
    "    #    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initiate empty list to store the loss on the train and validation sets\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_epoch = 0\n",
    "    best_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Get device and move model\n",
    "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #model = model.to(device=device, dtype=dtype)\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0.0 #torch.tensor(0.0)\n",
    "        #model = model.to(dtype=dtype, device=device) #\n",
    "        model.train()\n",
    "        #print(f\"Model start   : {list(model.parameters())[3]}\")\n",
    "        for i, batch in enumerate(dataloader_train):\n",
    "            x, (event, time) = batch\n",
    "            #x = x.to(dtype=dtype, device=device, non_blocking=True)\n",
    "            #event = event.to(device, non_blocking=True)\n",
    "            #time = time.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            log_hz, feature_weights, time_weights = model(x)  # shape = (batchsize, 1)\n",
    "            loss = neg_partial_log_likelihood(log_hz, event, time, reduction=\"mean\")#.to(dtype=dtype, device=device, non_blocking=True)\n",
    "            #print(f\"loss dtype: {loss.dtype}\")\n",
    "            #print(f\"loss cuda:  {loss.is_cuda}\")\n",
    "            #print(f\"weights dtype: {feature_weights.dtype, time_weights.dtype}\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() #.detach().to(\"cpu\").item()\n",
    "            print(f\"Epoch {epoch}, Batch {i}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "            # check model paramters are updating\n",
    "            #print(f\"Model training: {model(x)[0][:3]}\")\n",
    "            #print(f\"Model training: {list(model.parameters())[3]}\")\n",
    "            #for name, param in model.named_parameters():\n",
    "            #  print(f\"{name} mean: {param.data.mean().item()}\")\n",
    "\n",
    "            # Free up memory for unused tensors\n",
    "            #del x, event, time, log_hz, feature_weights, time_weights, loss\n",
    "            \n",
    "            #gc.collect()\n",
    "            #torch.cuda.empty_cache()\n",
    "        \n",
    "        # Reccord loss on train and test sets\n",
    "        epoch_loss /= i + 1\n",
    "        train_losses.append(epoch_loss)\n",
    "        #model.eval()\n",
    "        #print(\"Eval mode\")\n",
    "        with torch.no_grad():\n",
    "            x, (event, time) = next(iter(dataloader_val))\n",
    "            #x = x.to(device, non_blocking=True)\n",
    "            #event = event.to(device, non_blocking=True)\n",
    "            #time = time.to(device, non_blocking=True)\n",
    "            log_hz, feature_weights, time_weights = model(x)\n",
    "            #print(f\"log_hz: {log_hz}\")\n",
    "            val_loss = neg_partial_log_likelihood(log_hz, event, time, reduction=\"mean\").item() #.detach().to(\"cpu\")\n",
    "            print(f\"Validation loss: {val_loss}\")\n",
    "            val_losses.append(val_loss)\n",
    "            # Save best model based on validation loss\n",
    "            if val_loss < best_loss:\n",
    "                best_epoch = epoch\n",
    "                best_loss = val_loss\n",
    "                best_model_state = copy.deepcopy(model.to('cpu').state_dict())\n",
    "        \n",
    "            # check model paramters are updating\n",
    "            #print(f\"Model val     : {model(x)[0][:3]}\")\n",
    "            #print(f\"Model val     : {list(model.parameters())[3]}\")\n",
    "            #for name, param in model.named_parameters():\n",
    "            #  print(f\"{name} mean: {param.data.mean().item()}\")\n",
    "\n",
    "\n",
    "\n",
    "        del x, event, time, log_hz, feature_weights, time_weights    \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Display progress\n",
    "        if epoch % 1 == 0: #(n_epochs // 5) == 0:\n",
    "            print(f\"    Epoch: {epoch:03}, Training loss: {train_losses[-1]:0.2f}, Validation loss: {val_losses[-1]:0.2f}\")\n",
    "    \n",
    "    # Load best model if validation was used\n",
    "    if dataloader_val and best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    return model, train_losses, val_losses, best_loss, best_epoch #if dataloader_val else avg_train_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdb9673",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "18a3245d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tidy data\n",
      "Train: (599, 273)\n",
      "Val  : (400, 273)\n",
      "Test : (1, 273)\n"
     ]
    }
   ],
   "source": [
    "#Load tidy data\n",
    "print(\"Loading tidy data\")\n",
    "df_xy = pd.read_csv(\"data/df_xy_synth_v1.csv\")\n",
    "\n",
    "# create train, validation and test datasets: IMPUTE nan: -1\n",
    "df_train = df_xy.fillna(-1)\n",
    "df_test = df_train.sample(n=1, random_state=42)\n",
    "df_train = df_train.drop(df_test.index)\n",
    "df_val = df_train.sample(n=400, random_state=42)\n",
    "df_train = df_train.drop(df_val.index)\n",
    "\n",
    "print(f\"Train: {df_train.shape}\")\n",
    "print(f\"Val  : {df_val.shape}\")\n",
    "print(f\"Test : {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55922177",
   "metadata": {},
   "source": [
    "# Ukko"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a548950a",
   "metadata": {},
   "source": [
    "### Dataloaders: train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4cf4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ukko)\n",
    "importlib.reload(ukko.utils)\n",
    "\n",
    "# Dataloader\n",
    "BATCH_SIZE = 300#512\n",
    "dataloader_train = DataLoader(\n",
    "    ukkosurv_dataset(df_train), batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "dataloader_val = DataLoader(\n",
    "    #ukkosurv_dataset(df_val), batch_size=len(df_val), shuffle=False\n",
    "    ukkosurv_dataset(df_train), batch_size=len(df_train), shuffle=False\n",
    ")\n",
    "dataloader_test = DataLoader(\n",
    "    ukkosurv_dataset(df_test), batch_size=len(df_test), shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdc1682",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2cec046e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 10, Number of timepoints: 27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DualAttentionRegressor1(\n",
       "  (modules_list): ModuleList(\n",
       "    (0-1): 2 x DualAttentionModule(\n",
       "      (input_projection): Linear(in_features=1, out_features=8, bias=True)\n",
       "      (pos_encoder): PositionalEncoding()\n",
       "      (feature_attention): GroupedQueryAttention(\n",
       "        (W_q): Linear(in_features=8, out_features=8, bias=True)\n",
       "        (W_k): Linear(in_features=8, out_features=4, bias=True)\n",
       "        (W_v): Linear(in_features=8, out_features=4, bias=True)\n",
       "        (W_o): Linear(in_features=8, out_features=8, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feature_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (feature_ff): FeedForward(\n",
       "        (linear1): Linear(in_features=8, out_features=2048, bias=True)\n",
       "        (linear2): Linear(in_features=2048, out_features=8, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (activation): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (feature_ff_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (time_attention): GroupedQueryAttention(\n",
       "        (W_q): Linear(in_features=8, out_features=8, bias=True)\n",
       "        (W_k): Linear(in_features=8, out_features=4, bias=True)\n",
       "        (W_v): Linear(in_features=8, out_features=4, bias=True)\n",
       "        (W_o): Linear(in_features=8, out_features=8, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (time_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (time_ff): FeedForward(\n",
       "        (linear1): Linear(in_features=8, out_features=2048, bias=True)\n",
       "        (linear2): Linear(in_features=2048, out_features=8, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (activation): LeakyReLU(negative_slope=0.01)\n",
       "      )\n",
       "      (time_ff_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (output_ff): FeedForward(\n",
       "    (linear1): Linear(in_features=8, out_features=2048, bias=True)\n",
       "    (linear2): Linear(in_features=2048, out_features=8, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (activation): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (output_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "  (feature_pool): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (regressor): Linear(in_features=8, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(ukko.core)\n",
    "\n",
    "# Get feature and time dimensions\n",
    "x, (event, time) = next(iter(dataloader_train))\n",
    "num_features, num_timepoints = x.size(1), x.size(2)\n",
    "print(f\"Number of features: {num_features}, Number of timepoints: {num_timepoints}\")\n",
    "\n",
    "# Initialize model\n",
    "# DualAttentionRegressor1(self, n_features, time_steps, d_model=128, n_heads=8, dropout=0.1, n_modules=1)\n",
    "def initmodel():\n",
    "  model = ukko.core.DualAttentionRegressor1(\n",
    "    n_features=num_features,\n",
    "    time_steps=num_timepoints,\n",
    "    d_model=8,\n",
    "    n_heads=4,\n",
    "    n_kv_heads=2,\n",
    "    dropout=0.2,\n",
    "    n_modules=2\n",
    "  )\n",
    "  return model\n",
    "\n",
    "model = initmodel()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0759f83",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3d5de8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules_list.0.input_projection.weight: 0.354489\n",
      "modules_list.0.input_projection.bias: -0.367049\n",
      "modules_list.0.feature_attention.W_q.weight: -0.035006\n",
      "modules_list.0.feature_attention.W_q.bias: -0.037683\n",
      "modules_list.0.feature_attention.W_k.weight: -0.037529\n",
      "modules_list.0.feature_attention.W_k.bias: -0.033596\n",
      "modules_list.0.feature_attention.W_v.weight: -0.038815\n",
      "modules_list.0.feature_attention.W_v.bias: -0.015222\n",
      "modules_list.0.feature_attention.W_o.weight: -0.011895\n",
      "modules_list.0.feature_attention.W_o.bias: 0.157283\n",
      "modules_list.0.feature_norm.weight: 1.000000\n",
      "modules_list.0.feature_norm.bias: 0.000000\n",
      "modules_list.0.feature_ff.linear1.weight: -0.000022\n",
      "modules_list.0.feature_ff.linear1.bias: -0.000965\n",
      "modules_list.0.feature_ff.linear2.weight: 0.000093\n",
      "modules_list.0.feature_ff.linear2.bias: -0.003047\n",
      "modules_list.0.feature_ff_norm.weight: 1.000000\n",
      "modules_list.0.feature_ff_norm.bias: 0.000000\n",
      "modules_list.0.time_attention.W_q.weight: 0.031950\n",
      "modules_list.0.time_attention.W_q.bias: 0.015847\n",
      "modules_list.0.time_attention.W_k.weight: -0.019863\n",
      "modules_list.0.time_attention.W_k.bias: 0.202583\n",
      "modules_list.0.time_attention.W_v.weight: -0.123313\n",
      "modules_list.0.time_attention.W_v.bias: 0.033067\n",
      "modules_list.0.time_attention.W_o.weight: 0.063208\n",
      "modules_list.0.time_attention.W_o.bias: -0.015153\n",
      "modules_list.0.time_norm.weight: 1.000000\n",
      "modules_list.0.time_norm.bias: 0.000000\n",
      "modules_list.0.time_ff.linear1.weight: 0.003238\n",
      "modules_list.0.time_ff.linear1.bias: 0.003019\n",
      "modules_list.0.time_ff.linear2.weight: -0.000028\n",
      "modules_list.0.time_ff.linear2.bias: 0.006495\n",
      "modules_list.0.time_ff_norm.weight: 1.000000\n",
      "modules_list.0.time_ff_norm.bias: 0.000000\n",
      "modules_list.1.input_projection.weight: -0.002492\n",
      "modules_list.1.input_projection.bias: 0.006205\n",
      "modules_list.1.feature_attention.W_q.weight: -0.039921\n",
      "modules_list.1.feature_attention.W_q.bias: -0.098242\n",
      "modules_list.1.feature_attention.W_k.weight: -0.069274\n",
      "modules_list.1.feature_attention.W_k.bias: -0.058274\n",
      "modules_list.1.feature_attention.W_v.weight: 0.006953\n",
      "modules_list.1.feature_attention.W_v.bias: 0.112308\n",
      "modules_list.1.feature_attention.W_o.weight: 0.019958\n",
      "modules_list.1.feature_attention.W_o.bias: 0.022147\n",
      "modules_list.1.feature_norm.weight: 1.000000\n",
      "modules_list.1.feature_norm.bias: 0.000000\n",
      "modules_list.1.feature_ff.linear1.weight: 0.000250\n",
      "modules_list.1.feature_ff.linear1.bias: -0.002604\n",
      "modules_list.1.feature_ff.linear2.weight: -0.000020\n",
      "modules_list.1.feature_ff.linear2.bias: 0.000402\n",
      "modules_list.1.feature_ff_norm.weight: 1.000000\n",
      "modules_list.1.feature_ff_norm.bias: 0.000000\n",
      "modules_list.1.time_attention.W_q.weight: 0.025532\n",
      "modules_list.1.time_attention.W_q.bias: 0.019147\n",
      "modules_list.1.time_attention.W_k.weight: -0.011539\n",
      "modules_list.1.time_attention.W_k.bias: 0.231216\n",
      "modules_list.1.time_attention.W_v.weight: 0.063798\n",
      "modules_list.1.time_attention.W_v.bias: 0.068883\n",
      "modules_list.1.time_attention.W_o.weight: -0.061763\n",
      "modules_list.1.time_attention.W_o.bias: -0.066745\n",
      "modules_list.1.time_norm.weight: 1.000000\n",
      "modules_list.1.time_norm.bias: 0.000000\n",
      "modules_list.1.time_ff.linear1.weight: 0.000631\n",
      "modules_list.1.time_ff.linear1.bias: 0.001064\n",
      "modules_list.1.time_ff.linear2.weight: -0.000146\n",
      "modules_list.1.time_ff.linear2.bias: 0.005488\n",
      "modules_list.1.time_ff_norm.weight: 1.000000\n",
      "modules_list.1.time_ff_norm.bias: 0.000000\n",
      "output_ff.linear1.weight: -0.000792\n",
      "output_ff.linear1.bias: 0.005792\n",
      "output_ff.linear2.weight: 0.000029\n",
      "output_ff.linear2.bias: -0.008889\n",
      "output_norm.weight: 1.000000\n",
      "output_norm.bias: 0.000000\n",
      "feature_pool.weight: -0.101578\n",
      "feature_pool.bias: 0.134450\n",
      "regressor.weight: -0.012975\n",
      "regressor.bias: -0.206619\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.data.mean().item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6ec25439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.2570,  0.3088,  0.2686, -0.1744, -0.0968, -0.1702, -0.2498, -0.2376],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376b2e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3991],\n",
       "        [-0.6855],\n",
       "        [-0.6416]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(model.named_parameters()))[1][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "76aaf9aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3527, -0.2711, -0.3267], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6b00839b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,   0.0000,  -1.0000],\n",
       "          [ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,  -1.0000,  -1.0000],\n",
       "          [ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,  -1.0000,   0.3000],\n",
       "          ...,\n",
       "          [ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,  -1.0000,   2.7000],\n",
       "          [ -1.0000,  -1.0000,  34.0000,  ...,  -1.0000,  -1.0000,  -1.0000],\n",
       "          [ -1.0000,  -1.0000,  -1.0000,  ...,  10.0384,   4.1489,  14.7076]],\n",
       " \n",
       "         [[ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,   1.0000,  -1.0000],\n",
       "          [ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,  -1.0000,  -1.0000],\n",
       "          [ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,  -1.0000,  -1.0000],\n",
       "          ...,\n",
       "          [ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,   7.0000,  -1.0000],\n",
       "          [ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,  -1.0000,  -1.0000],\n",
       "          [110.3000,  -1.0000,  -1.0000,  ...,  35.3383,  10.8326,  16.3405]],\n",
       " \n",
       "         [[ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,  -1.0000,  -1.0000],\n",
       "          [ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,  -1.0000,  -1.0000],\n",
       "          [ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,  -1.0000,  -1.0000],\n",
       "          ...,\n",
       "          [ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,  -1.0000,  -1.0000],\n",
       "          [ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,  -1.0000,  14.0000],\n",
       "          [ -1.0000,  -1.0000,  -1.0000,  ...,  31.6246,  41.4605, 115.4848]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ -1.0000,   3.0000,  -1.0000,  ...,  -1.0000,  -1.0000,  -1.0000],\n",
       "          [ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,  -1.0000,  -1.0000],\n",
       "          [ -1.0000,   3.0000,  -1.0000,  ...,  -1.0000,  -1.0000,  -1.0000],\n",
       "          ...,\n",
       "          [ 20.4000,  21.0000,  -1.0000,  ...,  -1.0000,   0.0000,  -1.0000],\n",
       "          [ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,  -1.0000,  -1.0000],\n",
       "          [ -1.0000,  -1.0000,  -1.0000,  ...,  21.9386,  38.6920,  -1.0000]],\n",
       " \n",
       "         [[  1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,  -1.0000,  -1.0000],\n",
       "          [ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,  -1.0000,  -1.0000],\n",
       "          [ -1.0000,   1.0000,  -1.0000,  ...,  -1.0000,   1.0000,  -1.0000],\n",
       "          ...,\n",
       "          [ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,  -1.0000,  -1.0000],\n",
       "          [ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,  -1.0000,  -1.0000],\n",
       "          [ -1.0000,  -1.0000, 133.5674,  ...,  34.8879,  -1.0000,  -1.0000]],\n",
       " \n",
       "         [[ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,  -1.0000,  -1.0000],\n",
       "          [ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,  -1.0000,  -1.0000],\n",
       "          [ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,  -1.0000,  -1.0000],\n",
       "          ...,\n",
       "          [ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,  -1.0000,  12.9000],\n",
       "          [ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,  -1.0000,  -1.0000],\n",
       "          [ -1.0000,  -1.0000,  -1.0000,  ...,  -1.0000,  -1.0000,  -1.0000]]]),\n",
       " [tensor([ True,  True,  True, False,  True,  True, False, False,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "          False,  True, False,  True,  True,  True, False, False,  True, False,\n",
       "           True,  True, False,  True, False,  True, False,  True, False, False,\n",
       "          False,  True,  True,  True, False,  True,  True,  True, False,  True,\n",
       "           True, False, False, False, False, False,  True,  True,  True, False,\n",
       "           True,  True, False,  True,  True,  True, False,  True, False, False,\n",
       "           True, False, False, False,  True,  True, False,  True,  True,  True,\n",
       "          False,  True, False, False,  True, False,  True,  True,  True,  True,\n",
       "           True, False, False, False, False, False, False, False, False,  True,\n",
       "          False, False,  True,  True, False, False, False,  True, False,  True,\n",
       "          False,  True, False, False,  True,  True, False, False, False, False,\n",
       "           True, False,  True, False,  True, False, False, False, False,  True,\n",
       "           True, False, False, False, False, False, False,  True, False,  True,\n",
       "           True, False, False,  True, False, False, False, False, False, False,\n",
       "          False, False, False,  True,  True, False,  True,  True,  True, False,\n",
       "          False, False,  True,  True,  True, False, False, False, False, False,\n",
       "          False,  True,  True, False, False, False,  True, False,  True, False,\n",
       "          False,  True,  True, False,  True, False,  True, False,  True,  True,\n",
       "          False, False, False, False,  True,  True, False, False,  True, False]),\n",
       "  tensor([1.1010e+03, 5.3200e+02, 9.0000e+01, 1.8850e+03, 8.7300e+02, 1.1240e+03,\n",
       "          9.2500e+02, 4.9000e+02, 1.6860e+03, 2.0760e+03, 2.6000e+01, 4.8800e+02,\n",
       "          8.0000e+00, 4.3000e+01, 4.4500e+02, 2.4000e+01, 4.3900e+02, 2.4370e+03,\n",
       "          2.6270e+03, 6.9000e+01, 2.9300e+02, 0.0000e+00, 5.9000e+01, 7.7000e+01,\n",
       "          4.9850e+03, 2.1200e+02, 1.5600e+02, 4.4500e+02, 4.0000e+01, 3.5610e+03,\n",
       "          4.7000e+01, 4.0000e+00, 9.2900e+02, 0.0000e+00, 3.5710e+03, 1.8830e+03,\n",
       "          2.8000e+01, 3.4000e+01, 1.9100e+02, 1.8590e+03, 7.3200e+02, 1.9000e+01,\n",
       "          2.3600e+02, 1.2000e+01, 2.0600e+02, 2.2770e+03, 2.8400e+02, 4.8000e+02,\n",
       "          1.0000e+01, 4.0000e+01, 4.2000e+02, 4.2150e+03, 8.8000e+01, 4.7800e+02,\n",
       "          4.5300e+02, 2.6410e+03, 7.0000e+00, 1.9840e+03, 5.0000e+00, 7.7100e+02,\n",
       "          1.8300e+02, 3.2200e+02, 9.7900e+02, 2.1400e+02, 1.3000e+01, 7.1000e+01,\n",
       "          3.6800e+02, 7.7800e+02, 7.6000e+01, 3.7430e+03, 1.1580e+03, 2.2550e+03,\n",
       "          2.1610e+03, 4.0000e+00, 1.2700e+02, 6.8900e+02, 1.3340e+03, 5.3600e+02,\n",
       "          4.8200e+02, 4.5000e+01, 1.2600e+03, 1.5000e+01, 5.7700e+02, 4.8980e+03,\n",
       "          9.4000e+01, 2.0300e+02, 1.6480e+03, 6.4000e+01, 9.3700e+02, 0.0000e+00,\n",
       "          2.0510e+03, 2.1970e+03, 1.2430e+03, 4.2000e+02, 1.3800e+02, 5.6600e+02,\n",
       "          1.6110e+03, 1.8190e+03, 2.5890e+03, 7.0000e+00, 2.0000e+00, 2.6700e+03,\n",
       "          6.8000e+01, 3.5900e+02, 3.0870e+03, 2.1180e+03, 1.7400e+03, 1.7600e+02,\n",
       "          4.3300e+02, 3.6500e+02, 1.0320e+03, 5.4000e+01, 5.2000e+02, 2.8900e+02,\n",
       "          2.7400e+02, 6.2000e+01, 2.6380e+03, 1.4440e+03, 3.3810e+03, 3.2000e+03,\n",
       "          6.6000e+01, 2.1610e+03, 0.0000e+00, 4.1100e+02, 1.3210e+03, 3.5160e+03,\n",
       "          3.8300e+02, 7.1300e+02, 6.7900e+02, 1.6500e+02, 1.0000e+00, 4.9000e+01,\n",
       "          4.7000e+01, 3.2530e+03, 5.6000e+01, 1.7220e+03, 1.0760e+03, 1.8600e+02,\n",
       "          2.3220e+03, 3.7700e+02, 3.8900e+02, 1.2940e+03, 9.2100e+02, 1.8300e+02,\n",
       "          6.0210e+03, 1.5500e+02, 3.0000e+00, 1.0600e+02, 6.8400e+02, 2.1570e+03,\n",
       "          7.0500e+02, 1.3700e+02, 1.6170e+03, 8.4000e+01, 0.0000e+00, 5.6200e+02,\n",
       "          0.0000e+00, 4.5000e+01, 6.8000e+01, 3.4320e+03, 4.2190e+03, 4.7040e+03,\n",
       "          2.2800e+02, 4.9800e+02, 4.8500e+02, 3.5000e+01, 8.2300e+02, 6.4800e+02,\n",
       "          6.1560e+03, 5.0500e+02, 1.3850e+03, 2.8600e+02, 5.6800e+02, 4.3000e+02,\n",
       "          2.1700e+03, 3.0000e+01, 1.2800e+02, 1.4910e+03, 1.4800e+02, 1.0890e+03,\n",
       "          6.3300e+02, 1.6900e+02, 5.9400e+02, 8.0600e+02, 2.2590e+03, 2.1790e+03,\n",
       "          3.2000e+01, 1.6900e+02, 9.0000e+00, 1.9740e+03, 6.3600e+02, 1.0020e+03,\n",
       "          1.9700e+02, 9.9000e+01, 4.2000e+01, 6.8300e+02, 2.2760e+03, 1.4830e+03,\n",
       "          2.1810e+03, 2.5400e+02])]]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c1c8c2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 7.9429\n",
      "Validation loss: 7.938645362854004\n",
      "    Epoch: 000, Training loss: 7.94, Validation loss: 7.94\n",
      "Epoch 1, Batch 0, Loss: 7.9321\n",
      "Validation loss: 7.929367542266846\n",
      "    Epoch: 001, Training loss: 7.93, Validation loss: 7.93\n",
      "Epoch 2, Batch 0, Loss: 7.9429\n",
      "Validation loss: 7.925448894500732\n",
      "    Epoch: 002, Training loss: 7.94, Validation loss: 7.93\n",
      "Epoch 3, Batch 0, Loss: 7.9333\n",
      "Validation loss: 7.919982433319092\n",
      "    Epoch: 003, Training loss: 7.93, Validation loss: 7.92\n",
      "Epoch 4, Batch 0, Loss: 7.9579\n",
      "Validation loss: 7.92241096496582\n",
      "    Epoch: 004, Training loss: 7.96, Validation loss: 7.92\n",
      "Epoch 5, Batch 0, Loss: 7.9281\n",
      "Validation loss: 7.913344383239746\n",
      "    Epoch: 005, Training loss: 7.93, Validation loss: 7.91\n",
      "Epoch 6, Batch 0, Loss: 7.9329\n",
      "Validation loss: 7.931198596954346\n",
      "    Epoch: 006, Training loss: 7.93, Validation loss: 7.93\n",
      "Epoch 7, Batch 0, Loss: 7.9441\n",
      "Validation loss: 7.916887283325195\n",
      "    Epoch: 007, Training loss: 7.94, Validation loss: 7.92\n",
      "Epoch 8, Batch 0, Loss: 7.9496\n",
      "Validation loss: 7.916080951690674\n",
      "    Epoch: 008, Training loss: 7.95, Validation loss: 7.92\n",
      "Epoch 9, Batch 0, Loss: 7.9518\n",
      "Validation loss: 7.953993797302246\n",
      "    Epoch: 009, Training loss: 7.95, Validation loss: 7.95\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[117], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Train models\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m trained_model_reg, train_losses_reg, val_losses_reg, val_loss_reg, best_epoch_i_reg \u001b[38;5;241m=\u001b[39m train_model_simple(\n\u001b[0;32m     21\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     22\u001b[0m     dataloader_train \u001b[38;5;241m=\u001b[39m dataloader_train,\n\u001b[0;32m     23\u001b[0m     dataloader_val \u001b[38;5;241m=\u001b[39m dataloader_val, \n\u001b[0;32m     24\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer_reg,\n\u001b[0;32m     25\u001b[0m     n_epochs\u001b[38;5;241m=\u001b[39mEPOCHS,\n\u001b[0;32m     26\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     27\u001b[0m )\n",
      "Cell \u001b[1;32mIn[116], line 48\u001b[0m, in \u001b[0;36mtrain_model_simple\u001b[1;34m(model, dataloader_train, dataloader_val, optimizer, n_epochs, learning_rate, device)\u001b[0m\n\u001b[0;32m     44\u001b[0m loss \u001b[38;5;241m=\u001b[39m neg_partial_log_likelihood(log_hz, event, time, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;66;03m#.to(dtype=dtype, device=device, non_blocking=True)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m#print(f\"loss dtype: {loss.dtype}\")\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m#print(f\"loss cuda:  {loss.is_cuda}\")\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m#print(f\"weights dtype: {feature_weights.dtype, time_weights.dtype}\")\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     49\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     50\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;66;03m#.detach().to(\"cpu\").item()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ericf\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    628\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ericf\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ericf\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Init optimizer for Cox\n",
    "# AdamW is generally preferred over Adam for its weight decay regularization\n",
    "optimizer_noreg = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "optimizer_reg = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "\n",
    "\n",
    "# Initiate empty list to store the loss on the train and validation sets\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Get device and move model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = initmodel()\n",
    "model = model.to(device)\n",
    "# Train models\n",
    "trained_model_reg, train_losses_reg, val_losses_reg, val_loss_reg, best_epoch_i_reg = train_model_simple(\n",
    "    model=model,\n",
    "    dataloader_train = dataloader_train,\n",
    "    dataloader_val = dataloader_val, \n",
    "    optimizer=optimizer_reg,\n",
    "    n_epochs=EPOCHS,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "# model = initmodel().to(device)\n",
    "# trained_model_noreg, train_losses_noreg, val_losses_noreg, val_loss_noreg, best_epoch_i_noreg = train_model_simple(\n",
    "#     model=model,\n",
    "#     dataloader_train = dataloader_train,\n",
    "#     dataloader_val = dataloader_val, \n",
    "#     optimizer=optimizer_noreg,\n",
    "#     n_epochs=EPOCHS,\n",
    "#     device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# )\n",
    "\n",
    "\n",
    "# plot_losses(train_losses_reg, val_losses_reg, \"Cox\")\n",
    "# print(f\"  Best train and val loss: {min(train_losses_reg):0.3f}, {min(val_losses_reg):0.3f}\")\n",
    "\n",
    "# plot_losses(train_losses_noreg, val_losses_noreg, \"Cox\")\n",
    "# print(f\"  Best train and val loss: {min(train_losses_noreg):0.3f}, {min(val_losses_noreg):0.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
