{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052cc887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a Multi-Head Latent Attention (MLA) mechanism, inspired by\n",
    "    concepts aiming for reduced KV cache size through latent compression\n",
    "    of Keys and Values.\n",
    "\n",
    "    Attributes:\n",
    "        d_model (int): The dimensionality of the input and output features.\n",
    "        n_heads (int): The number of attention heads.\n",
    "        d_kv_comp (int): The latent dimension for compressed Keys and Values.\n",
    "                         This is the dimension that would be stored in the KV cache.\n",
    "        d_k (int): The dimensionality of each attention head for Query (d_model // n_heads).\n",
    "        W_q (nn.Linear): Linear layer to project the input query to the query space.\n",
    "        W_kv_down (nn.Linear): Linear layer to compress input key/value to the latent space.\n",
    "        W_k_up (nn.Linear): Linear layer to project latent key back to full key space.\n",
    "        W_v_up (nn.Linear): Linear layer to project latent value back to full value space.\n",
    "        W_o (nn.Linear): Linear layer to project the concatenated output of all attention heads.\n",
    "        dropout (nn.Dropout): Dropout layer applied to the attention weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, d_kv_comp, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the MultiHeadLatentAttention module.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The dimensionality of the input and output features.\n",
    "            n_heads (int): The number of attention heads.\n",
    "            d_kv_comp (int): The latent dimension for compressed Keys and Values.\n",
    "            dropout (float, optional): Dropout probability for the attention weights. Default is 0.1.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: If d_model is not divisible by n_heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_kv_comp = d_kv_comp\n",
    "        self.d_k = d_model // n_heads # Dimension per head for Query\n",
    "\n",
    "        # Query projection remains standard\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Key and Value compression (down-projection)\n",
    "        # These project from d_model (input feature dim) to d_kv_comp (latent dim)\n",
    "        self.W_kv_down = nn.Linear(d_model, d_kv_comp)\n",
    "\n",
    "        # Key and Value expansion (up-projection)\n",
    "        # These project from d_kv_comp (latent dim) back to d_model for attention computation\n",
    "        self.W_k_up = nn.Linear(d_kv_comp, d_model)\n",
    "        self.W_v_up = nn.Linear(d_kv_comp, d_model)\n",
    "\n",
    "        # Output projection remains standard\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the Multi-Head Latent Attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): The input query tensor of shape (batch_size, seq_len, d_model).\n",
    "            key (torch.Tensor): The input key tensor of shape (batch_size, seq_len, d_model).\n",
    "            value (torch.Tensor): The input value tensor of shape (batch_size, seq_len, d_model).\n",
    "            mask (torch.Tensor, optional): The mask tensor to apply to the attention scores. Default is None.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: A tuple containing:\n",
    "                - output (torch.Tensor): The output tensor of shape (batch_size, seq_len, d_model).\n",
    "                - attention_weights (torch.Tensor): The attention weights tensor of shape (batch_size, n_heads, seq_len, seq_len).\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        seq_len_q = query.size(1) # Sequence length for Query\n",
    "        seq_len_kv = key.size(1) # Sequence length for Key/Value\n",
    "\n",
    "        # 1. Project Query\n",
    "        # Q: (batch_size, seq_len_q, d_model)\n",
    "        Q = self.W_q(query)\n",
    "\n",
    "        # 2. Compress Key and Value into latent space\n",
    "        # K_latent, V_latent: (batch_size, seq_len_kv, d_kv_comp)\n",
    "        # This is the representation that would be cached during inference.\n",
    "        K_latent = self.W_kv_down(key)\n",
    "        V_latent = self.W_kv_down(value)\n",
    "\n",
    "        # 3. Up-project latent K and V for attention computation\n",
    "        # K_up, V_up: (batch_size, seq_len_kv, d_model)\n",
    "        # These are the full-dimensional K and V that will be split into heads\n",
    "        K_up = self.W_k_up(K_latent)\n",
    "        V_up = self.W_v_up(V_latent)\n",
    "\n",
    "        # 4. Reshape for Multi-Head Attention\n",
    "        # Q: (batch_size, n_heads, seq_len_q, d_k)\n",
    "        # K_up, V_up: (batch_size, n_heads, seq_len_kv, d_k)\n",
    "        Q = Q.view(batch_size, seq_len_q, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K_up = K_up.view(batch_size, seq_len_kv, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V_up = V_up.view(batch_size, seq_len_kv, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 5. Calculate Attention Scores\n",
    "        # scores: (batch_size, n_heads, seq_len_q, seq_len_kv)\n",
    "        scores = torch.matmul(Q, K_up.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # 6. Apply mask (e.g., causal mask for autoregressive models)\n",
    "        if mask is not None:\n",
    "            # Mask should be broadcastable to scores shape\n",
    "            # (batch_size, 1, seq_len_q, seq_len_kv) or (1, 1, seq_len_q, seq_len_kv)\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # 7. Apply Softmax and Dropout\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # 8. Compute weighted sum of Values\n",
    "        # output: (batch_size, n_heads, seq_len_q, d_k)\n",
    "        output = torch.matmul(attention_weights, V_up)\n",
    "\n",
    "        # 9. Concatenate heads and apply final linear layer\n",
    "        # output: (batch_size, seq_len_q, d_model)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len_q, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "# Example Usage to demonstrate similarity in style and functionality:\n",
    "if __name__ == \"__main__\":\n",
    "    d_model = 512\n",
    "    n_heads = 8\n",
    "    d_kv_comp = 128  # Choose a compression dimension, e.g., 1/4 of d_model\n",
    "    seq_len = 100\n",
    "    batch_size = 4\n",
    "\n",
    "    # Dummy input tensors (e.g., from a self-attention context where Q, K, V are same)\n",
    "    query = torch.randn(batch_size, seq_len, d_model)\n",
    "    key = torch.randn(batch_size, seq_len, d_model)\n",
    "    value = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # Example causal mask for self-attention\n",
    "    causal_mask = torch.tril(torch.ones(seq_len, seq_len)).bool()\n",
    "    causal_mask = causal_mask.unsqueeze(0).unsqueeze(0) # (1, 1, seq_len, seq_len)\n",
    "\n",
    "    print(\"--- Testing MultiHeadAttention (Original Style) ---\")\n",
    "    mha_block = MultiHeadAttention(d_model, n_heads)\n",
    "    mha_output, mha_attn_weights = mha_block(query, key, value, mask=causal_mask)\n",
    "    print(f\"MHA Output shape: {mha_output.shape}\")\n",
    "    print(f\"MHA Attention weights shape: {mha_attn_weights.shape}\")\n",
    "\n",
    "    print(\"\\n--- Testing MultiHeadLatentAttention (MLA Style) ---\")\n",
    "    mla_block = MultiHeadLatentAttention(d_model, n_heads, d_kv_comp)\n",
    "    mla_output, mla_attn_weights = mla_block(query, key, value, mask=causal_mask)\n",
    "    print(f\"MLA Output shape: {mla_output.shape}\")\n",
    "    print(f\"MLA Attention weights shape: {mla_attn_weights.shape}\")\n",
    "\n",
    "    print(f\"\\nMLA KV compression ratio (d_model/d_kv_comp): {d_model / d_kv_comp:.2f}x\")\n",
    "    print(f\"MLA KV cache size (relative to original): {d_kv_comp / d_model:.2f}\")\n",
    "\n",
    "    # Demonstrate cross-attention scenario where key/value sequence length differs from query\n",
    "    print(\"\\n--- Testing MLA with cross-attention (different KV sequence length) ---\")\n",
    "    encoder_output_len = 50\n",
    "    encoder_key = torch.randn(batch_size, encoder_output_len, d_model)\n",
    "    encoder_value = torch.randn(batch_size, encoder_output_len, d_model)\n",
    "    \n",
    "    # Mask for cross-attention, typically (batch_size, 1, query_len, key_len)\n",
    "    cross_attn_mask = torch.ones(batch_size, 1, seq_len, encoder_output_len).bool()\n",
    "\n",
    "    mla_cross_output, mla_cross_attn_weights = mla_block(query, encoder_key, encoder_value, mask=cross_attn_mask)\n",
    "    print(f\"MLA Cross-Attention Output shape: {mla_cross_output.shape}\")\n",
    "    print(f\"MLA Cross-Attention weights shape: {mla_cross_attn_weights.shape}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
