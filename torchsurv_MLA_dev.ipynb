{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Multi Head Latent Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "import ukko\n",
    "from ukko.core import MultiHeadAttention\n",
    "importlib.reload(ukko.core)\n",
    "\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a Multi-Head Latent Attention (MLA) mechanism, inspired by\n",
    "    concepts aiming for reduced KV cache size through latent compression\n",
    "    of Keys and Values.\n",
    "\n",
    "    Attributes:\n",
    "        d_model (int): The dimensionality of the input and output features.\n",
    "        n_heads (int): The number of attention heads.\n",
    "        d_kv_comp (int): The latent dimension for compressed Keys and Values.\n",
    "                         This is the dimension that would be stored in the KV cache.\n",
    "        d_k (int): The dimensionality of each attention head for Query (d_model // n_heads).\n",
    "        W_q (nn.Linear): Linear layer to project the input query to the query space.\n",
    "        W_kv_down (nn.Linear): Linear layer to compress input key/value to the latent space.\n",
    "        W_k_up (nn.Linear): Linear layer to project latent key back to full key space.\n",
    "        W_v_up (nn.Linear): Linear layer to project latent value back to full value space.\n",
    "        W_o (nn.Linear): Linear layer to project the concatenated output of all attention heads.\n",
    "        dropout (nn.Dropout): Dropout layer applied to the attention weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, d_kv_comp, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the MultiHeadLatentAttention module.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The dimensionality of the input and output features.\n",
    "            n_heads (int): The number of attention heads.\n",
    "            d_kv_comp (int): The latent dimension for compressed Keys and Values.\n",
    "            dropout (float, optional): Dropout probability for the attention weights. Default is 0.1.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: If d_model is not divisible by n_heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_kv_comp = d_kv_comp\n",
    "        self.d_k = d_model // n_heads # Dimension per head for Query\n",
    "\n",
    "        # Query projection remains standard\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Key and Value compression (down-projection)\n",
    "        # These project from d_model (input feature dim) to d_kv_comp (latent dim)\n",
    "        self.W_kv_down = nn.Linear(d_model, d_kv_comp)\n",
    "\n",
    "        # Key and Value expansion (up-projection)\n",
    "        # These project from d_kv_comp (latent dim) back to d_model for attention computation\n",
    "        self.W_k_up = nn.Linear(d_kv_comp, d_model)\n",
    "        self.W_v_up = nn.Linear(d_kv_comp, d_model)\n",
    "\n",
    "        # Output projection remains standard\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the Multi-Head Latent Attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): The input query tensor of shape (batch_size, seq_len, d_model).\n",
    "            key (torch.Tensor): The input key tensor of shape (batch_size, seq_len, d_model).\n",
    "            value (torch.Tensor): The input value tensor of shape (batch_size, seq_len, d_model).\n",
    "            mask (torch.Tensor, optional): The mask tensor to apply to the attention scores. Default is None.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: A tuple containing:\n",
    "                - output (torch.Tensor): The output tensor of shape (batch_size, seq_len, d_model).\n",
    "                - attention_weights (torch.Tensor): The attention weights tensor of shape (batch_size, n_heads, seq_len, seq_len).\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        seq_len_q = query.size(1) # Sequence length for Query\n",
    "        seq_len_kv = key.size(1) # Sequence length for Key/Value\n",
    "\n",
    "        # 1. Project Query\n",
    "        # Q: (batch_size, seq_len_q, d_model)\n",
    "        Q = self.W_q(query)\n",
    "\n",
    "        # 2. Compress Key and Value into latent space\n",
    "        # K_latent, V_latent: (batch_size, seq_len_kv, d_kv_comp)\n",
    "        # This is the representation that would be cached during inference.\n",
    "        K_latent = self.W_kv_down(key)\n",
    "        V_latent = self.W_kv_down(value)\n",
    "\n",
    "        # 3. Up-project latent K and V for attention computation\n",
    "        # K_up, V_up: (batch_size, seq_len_kv, d_model)\n",
    "        # These are the full-dimensional K and V that will be split into heads\n",
    "        K_up = self.W_k_up(K_latent)\n",
    "        V_up = self.W_v_up(V_latent)\n",
    "\n",
    "        # 4. Reshape for Multi-Head Attention\n",
    "        # Q: (batch_size, n_heads, seq_len_q, d_k)\n",
    "        # K_up, V_up: (batch_size, n_heads, seq_len_kv, d_k)\n",
    "        Q = Q.view(batch_size, seq_len_q, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K_up = K_up.view(batch_size, seq_len_kv, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V_up = V_up.view(batch_size, seq_len_kv, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 5. Calculate Attention Scores\n",
    "        # scores: (batch_size, n_heads, seq_len_q, seq_len_kv)\n",
    "        scores = torch.matmul(Q, K_up.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # 6. Apply mask (e.g., causal mask for autoregressive models)\n",
    "        if mask is not None:\n",
    "            # Mask should be broadcastable to scores shape\n",
    "            # (batch_size, 1, seq_len_q, seq_len_kv) or (1, 1, seq_len_q, seq_len_kv)\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # 7. Apply Softmax and Dropout\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # 8. Compute weighted sum of Values\n",
    "        # output: (batch_size, n_heads, seq_len_q, d_k)\n",
    "        output = torch.matmul(attention_weights, V_up)\n",
    "\n",
    "        # 9. Concatenate heads and apply final linear layer\n",
    "        # output: (batch_size, seq_len_q, d_model)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len_q, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "# Example Usage to demonstrate similarity in style and functionality:\n",
    "if __name__ == \"__main__\":\n",
    "    d_model = 512\n",
    "    n_heads = 8\n",
    "    d_kv_comp = 128  # Choose a compression dimension, e.g., 1/4 of d_model\n",
    "    seq_len = 100\n",
    "    batch_size = 4\n",
    "\n",
    "    # Dummy input tensors (e.g., from a self-attention context where Q, K, V are same)\n",
    "    query = torch.randn(batch_size, seq_len, d_model)\n",
    "    key = torch.randn(batch_size, seq_len, d_model)\n",
    "    value = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # Example causal mask for self-attention\n",
    "    causal_mask = torch.tril(torch.ones(seq_len, seq_len)).bool()\n",
    "    causal_mask = causal_mask.unsqueeze(0).unsqueeze(0) # (1, 1, seq_len, seq_len)\n",
    "\n",
    "    print(\"--- Testing MultiHeadAttention (Original Style) ---\")\n",
    "    mha_block = MultiHeadAttention(d_model, n_heads)\n",
    "    mha_output, mha_attn_weights = mha_block(query, key, value, mask=causal_mask)\n",
    "    print(f\"MHA Output shape: {mha_output.shape}\")\n",
    "    print(f\"MHA Attention weights shape: {mha_attn_weights.shape}\")\n",
    "\n",
    "    print(\"\\n--- Testing MultiHeadLatentAttention (MLA Style) ---\")\n",
    "    mla_block = MultiHeadLatentAttention(d_model, n_heads, d_kv_comp)\n",
    "    mla_output, mla_attn_weights = mla_block(query, key, value, mask=causal_mask)\n",
    "    print(f\"MLA Output shape: {mla_output.shape}\")\n",
    "    print(f\"MLA Attention weights shape: {mla_attn_weights.shape}\")\n",
    "\n",
    "    print(f\"\\nMLA KV compression ratio (d_model/d_kv_comp): {d_model / d_kv_comp:.2f}x\")\n",
    "    print(f\"MLA KV cache size (relative to original): {d_kv_comp / d_model:.2f}\")\n",
    "\n",
    "    # Demonstrate cross-attention scenario where key/value sequence length differs from query\n",
    "    print(\"\\n--- Testing MLA with cross-attention (different KV sequence length) ---\")\n",
    "    encoder_output_len = 50\n",
    "    encoder_key = torch.randn(batch_size, encoder_output_len, d_model)\n",
    "    encoder_value = torch.randn(batch_size, encoder_output_len, d_model)\n",
    "    \n",
    "    # Mask for cross-attention, typically (batch_size, 1, query_len, key_len)\n",
    "    cross_attn_mask = torch.ones(batch_size, 1, seq_len, encoder_output_len).bool()\n",
    "\n",
    "    mla_cross_output, mla_cross_attn_weights = mla_block(query, encoder_key, encoder_value, mask=cross_attn_mask)\n",
    "    print(f\"MLA Cross-Attention Output shape: {mla_cross_output.shape}\")\n",
    "    print(f\"MLA Cross-Attention weights shape: {mla_cross_attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Grouped Query Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Grouped-Query Attention (GQA), a variant of Multi-Head Attention\n",
    "    designed to improve inference speed and reduce KV cache memory.\n",
    "    In GQA, query heads are divided into groups, and each group shares a\n",
    "    single set of Key and Value projections.\n",
    "\n",
    "    Attributes:\n",
    "        d_model (int): The dimensionality of the input and output features.\n",
    "        n_heads (int): The total number of query attention heads.\n",
    "        n_kv_heads (int): The number of Key and Value heads. Must be\n",
    "                          less than or equal to n_heads and n_heads must be\n",
    "                          divisible by n_kv_heads.\n",
    "        d_k (int): The dimensionality of each attention head (d_model // n_heads for queries).\n",
    "        d_kv (int): The dimensionality of each Key/Value head (d_model // n_kv_heads).\n",
    "        W_q (nn.Linear): Linear layer to project the input query to the query space.\n",
    "        W_k (nn.Linear): Linear layer to project the input key to the key space (for n_kv_heads).\n",
    "        W_v (nn.Linear): Linear layer to project the input value to the value space (for n_kv_heads).\n",
    "        W_o (nn.Linear): Linear layer to project the concatenated output of all attention heads.\n",
    "        dropout (nn.Dropout): Dropout layer applied to the attention weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, n_kv_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the GroupedQueryAttention module.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The dimensionality of the input and output features.\n",
    "            n_heads (int): The total number of query attention heads.\n",
    "            n_kv_heads (int): The number of Key and Value heads.\n",
    "            dropout (float, optional): Dropout probability for the attention weights. Default is 0.1.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: If d_model is not divisible by n_heads,\n",
    "                            or if n_heads is not divisible by n_kv_heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        assert n_heads % n_kv_heads == 0, \"n_heads must be divisible by n_kv_heads for grouping\"\n",
    "        assert n_kv_heads <= n_heads, \"n_kv_heads cannot be greater than n_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.d_k = d_model // n_heads       # Dimension per Query head\n",
    "        self.d_kv = d_model // n_kv_heads   # Dimension per KV head (used for linear layer output)\n",
    "\n",
    "        # Query projection remains standard\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Output d_model for n_heads * d_k\n",
    "        # Key and Value projections produce n_kv_heads\n",
    "        self.W_k = nn.Linear(d_model, self.n_kv_heads * self.d_k) # Output n_kv_heads * d_k\n",
    "        self.W_v = nn.Linear(d_model, self.n_kv_heads * self.d_k) # Output n_kv_heads * d_k\n",
    "        # Output projection remains standard\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Init the attention weights with xavier (uniform or normal should not make much difference), becasue \n",
    "        #  - does varaince balancing and \n",
    "        #  - tranforms attention weight into probabilities for better interpretability and smoother training \n",
    "        # Kaiming also an option (default for nn.Liner) but we dont ahve ReLu in attentions, so all ok with xavier \n",
    "        nn.init.xavier_uniform_(self.W_q.weight)\n",
    "        nn.init.xavier_uniform_(self.W_k.weight)\n",
    "        nn.init.xavier_uniform_(self.W_v.weight)\n",
    "        nn.init.xavier_uniform_(self.W_o.weight)\n",
    "        # You might also initialize biases to zero or a small constant if present\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Calculate how many query heads per KV head\n",
    "        self.n_reps = self.n_heads // self.n_kv_heads\n",
    "\n",
    "    def _repeat_kv(self, x, n_reps):\n",
    "        \"\"\"\n",
    "        Repeats the K/V heads N_reps times to match the number of query heads.\n",
    "        Input x: (batch_size, n_kv_heads, seq_len, d_k)\n",
    "        Output: (batch_size, n_heads, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        batch_size, n_kv_heads, seq_len, d_k = x.shape\n",
    "        if n_reps == 1: # If n_heads == n_kv_heads, it's just MHA, no repetition needed\n",
    "            return x\n",
    "        return x[:, :, None, :, :].expand(batch_size, n_kv_heads, n_reps, seq_len, d_k).reshape(\n",
    "            batch_size, n_kv_heads * n_reps, seq_len, d_k\n",
    "        )\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the Grouped-Query Attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): The input query tensor of shape (batch_size, seq_len, d_model).\n",
    "            key (torch.Tensor): The input key tensor of shape (batch_size, seq_len, d_model).\n",
    "            value (torch.Tensor): The input value tensor of shape (batch_size, seq_len, d_model).\n",
    "            mask (torch.Tensor, optional): The mask tensor to apply to the attention scores. Default is None.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: A tuple containing:\n",
    "                - output (torch.Tensor): The output tensor of shape (batch_size, seq_len, d_model).\n",
    "                - attention_weights (torch.Tensor): The attention weights tensor of shape (batch_size, n_heads, seq_len, seq_len).\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        seq_len_q = query.size(1) # Sequence length for Query\n",
    "        seq_len_kv = key.size(1)  # Sequence length for Key/Value\n",
    "\n",
    "        # 1. Project Query, Key, Value\n",
    "        # Q: (batch_size, seq_len_q, d_model)\n",
    "        # K, V: (batch_size, seq_len_kv, n_kv_heads * d_k)\n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "\n",
    "        # 2. Reshape for Multi-Head Attention\n",
    "        # Q: (batch_size, n_heads, seq_len_q, d_k)\n",
    "        # K, V: (batch_size, n_kv_heads, seq_len_kv, d_k)\n",
    "        # Note: K and V are reshaped to self.n_kv_heads, not self.n_heads\n",
    "        Q = Q.view(batch_size, seq_len_q, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len_kv, self.n_kv_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len_kv, self.n_kv_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 3. Repeat K and V heads to match the number of query heads\n",
    "        # K, V after repeat: (batch_size, n_heads, seq_len_kv, d_k)\n",
    "        K = self._repeat_kv(K, self.n_reps)\n",
    "        V = self._repeat_kv(V, self.n_reps)\n",
    "\n",
    "        # 4. Calculate Attention Scores (standard dot product)\n",
    "        # scores: (batch_size, n_heads, seq_len_q, seq_len_kv)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # 5. Apply mask\n",
    "        if mask is not None:\n",
    "            # Mask should be broadcastable to scores shape\n",
    "            # (batch_size, 1, seq_len_q, seq_len_kv) or (1, 1, seq_len_q, seq_len_kv)\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # 6. Apply Softmax and Dropout\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # 7. Compute weighted sum of Values\n",
    "        # output: (batch_size, n_heads, seq_len_q, d_k)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        # 8. Concatenate heads and apply final linear layer\n",
    "        # output: (batch_size, seq_len_q, d_model)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len_q, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "# Example Usage:\n",
    "if __name__ == \"__main__\":\n",
    "    d_model = 512\n",
    "    n_heads = 8  # Total number of Query heads\n",
    "    n_kv_heads = 2 # Number of Key/Value heads. Must be a divisor of n_heads (8/2 = 4 groups)\n",
    "    seq_len = 100\n",
    "    batch_size = 4\n",
    "\n",
    "    # Dummy input tensors (e.g., for self-attention)\n",
    "    query = torch.randn(batch_size, seq_len, d_model)\n",
    "    key = torch.randn(batch_size, seq_len, d_model)\n",
    "    value = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # Example causal mask (for decoder self-attention)\n",
    "    causal_mask = torch.tril(torch.ones(seq_len, seq_len)).bool()\n",
    "    causal_mask = causal_mask.unsqueeze(0).unsqueeze(0) # (1, 1, seq_len, seq_len)\n",
    "\n",
    "    print(\"--- Testing GroupedQueryAttention (GQA Style) ---\")\n",
    "    gqa_attention = GroupedQueryAttention(d_model, n_heads, n_kv_heads)\n",
    "    gqa_output, gqa_attn_weights = gqa_attention(query, key, value, mask=causal_mask)\n",
    "\n",
    "    print(f\"GQA Output shape: {gqa_output.shape}\")\n",
    "    print(f\"GQA Attention weights shape: {gqa_attn_weights.shape}\")\n",
    "\n",
    "    # For comparison, MHA would have KV cache size proportional to 8 * d_k * seq_len\n",
    "    # GQA has KV cache size proportional to n_kv_heads * d_k * seq_len (2 * d_k * seq_len in this example)\n",
    "    print(f\"\\nNumber of Query heads: {n_heads}\")\n",
    "    print(f\"Number of KV heads: {n_kv_heads}\")\n",
    "    print(f\"KV cache reduction factor (compared to MHA with same n_heads): {n_heads / n_kv_heads:.1f}x\")\n",
    "    print(f\"Effective KV cache size (relative to MHA): {n_kv_heads / n_heads:.2f}\")\n",
    "\n",
    "    # Test MQA case (n_kv_heads = 1)\n",
    "    print(\"\\n--- Testing GroupedQueryAttention (MQA case: n_kv_heads=1) ---\")\n",
    "    mqa_attention = GroupedQueryAttention(d_model, n_heads, n_kv_heads=1)\n",
    "    mqa_output, mqa_attn_weights = mqa_attention(query, key, value, mask=causal_mask)\n",
    "    print(f\"MQA (GQA with n_kv_heads=1) Output shape: {mqa_output.shape}\")\n",
    "    print(f\"MQA (GQA with n_kv_heads=1) Attention weights shape: {mqa_attn_weights.shape}\")\n",
    "    print(f\"KV cache reduction factor (MQA): {n_heads / 1:.1f}x\")\n",
    "\n",
    "    # Test MHA case (n_kv_heads = n_heads)\n",
    "    print(\"\\n--- Testing GroupedQueryAttention (MHA case: n_kv_heads=n_heads) ---\")\n",
    "    mha_via_gqa_attention = GroupedQueryAttention(d_model, n_heads, n_kv_heads=n_heads)\n",
    "    mha_via_gqa_output, mha_via_gqa_attn_weights = mha_via_gqa_attention(query, key, value, mask=causal_mask)\n",
    "    print(f\"MHA (GQA with n_kv_heads=n_heads) Output shape: {mha_via_gqa_output.shape}\")\n",
    "    print(f\"MHA (GQA with n_kv_heads=n_heads) Attention weights shape: {mha_via_gqa_attn_weights.shape}\")\n",
    "    print(f\"KV cache reduction factor (MHA): {n_heads / n_heads:.1f}x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
