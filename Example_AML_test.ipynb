{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lifelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import ukko \n",
    "import importlib\n",
    "# For preprocessing\n",
    "print(\"Loading sklearn\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(\"Libraries loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Load an preprocess data\n",
    "\n",
    "Notes: \n",
    "- OSS_status: 1 means death  \n",
    "  `assign(OSS_status = lambda x: np.where( np.isnan(x[\"death_time\"].astype(\"float\")), 0, 1))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tidy data\n",
    "print(\"Loading tidy data\")\n",
    "df_xy = pd.read_csv(\"data/df_xy_synth_v1.csv\")\n",
    "\n",
    "# create train, validation and test datasets: IMPUTE nan: -1\n",
    "df_train = df_xy.fillna(-1)\n",
    "df_test = df_train.sample(frac=0.2)\n",
    "df_train = df_train.drop(df_test.index)\n",
    "df_val = df_train.sample(frac=0.2)\n",
    "df_train = df_train.drop(df_val.index)\n",
    "\n",
    "# Numerical feature columns for standardizer\n",
    "cols_standardize = df_xy.iloc[:,3:].columns\n",
    "\n",
    "\n",
    "#print(\"Loading torch\")\n",
    "#import torch # For building the networks \n",
    "#import torchtuples as tt # Some useful functionsci  swop\n",
    "\n",
    "#print(\"Loading pycox\")\n",
    "# from pycox.models import LogisticHazard\n",
    "# from pycox.models import PMF\n",
    "#->from pycox.models import DeepHitSingle\n",
    "#from pycox.models import CoxPH #CoxPH = DeepSurv\n",
    "#from pycox.evaluation import EvalSurv\n",
    "\n",
    "#cols_standardize = df_x.columns\n",
    "\n",
    "cols_standardize = df_xy.iloc[:,3:].columns\n",
    "standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "\n",
    "x_mapper = DataFrameMapper(standardize)\n",
    "x_train = x_mapper.fit_transform(df_train).astype('float32')\n",
    "x_val = x_mapper.transform(df_val).astype('float32')\n",
    "x_test = x_mapper.transform(df_test).astype('float32')\n",
    "df_xy\n",
    "\n",
    "print(f\"Original data: {df_xy.shape}, type: {type(df_xy)}\")\n",
    "mycols = df_xy.columns[:4].tolist() + ['...'] + df_xy.columns[-2:].tolist()\n",
    "print(f\"      Columns: {mycols}\")\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Training data:   {x_train.shape[0]}, type: {type(x_train)}\")\n",
    "print(f\"Test data:       {x_test.shape[0]}\")\n",
    "print(f\"Validation data: {x_val.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to convert df into 3-D numpy array\n",
    "def convert_to_3d_df(df):\n",
    "\n",
    "    # Convert column names to tuples, assuming this \"('feature', timepoint)\"\n",
    "    columns = [eval(col) for col in df.columns]\n",
    "    df.columns = columns\n",
    "    \n",
    "    # Extract unique features and timepoints\n",
    "    features = sorted(list(set([col[0] for col in columns])))\n",
    "    timepoints = sorted(list(set([col[1] for col in columns])))\n",
    "    \n",
    "    # Initialize a 3D numpy array\n",
    "    n_rows = df.shape[0]\n",
    "    n_features = len(features)\n",
    "    n_timepoints = len(timepoints)\n",
    "    data_3d = np.empty((n_rows, n_features, n_timepoints))\n",
    "    data_3d.fill(np.nan)\n",
    "    \n",
    "    # Map feature names and timepoints to indices\n",
    "    feature_indices = {feature: i for i, feature in enumerate(features)}\n",
    "    timepoint_indices = {timepoint: i for i, timepoint in enumerate(timepoints)}\n",
    "    \n",
    "    # Fill the 3D array with data from the DataFrame\n",
    "    for col in columns:\n",
    "        feature, timepoint = col\n",
    "        feature_idx = feature_indices[feature]\n",
    "        timepoint_idx = timepoint_indices[timepoint]\n",
    "        data_3d[:, feature_idx, timepoint_idx] = df[col]\n",
    "\n",
    "    # Create a MultiIndex for the columns of the 3D DataFrame\n",
    "    columns = pd.MultiIndex.from_product([features, timepoints], names=[\"Feature\", \"Timepoint\"])\n",
    "    \n",
    "    # Create the 3D DataFrame\n",
    "    df_multiindex = pd.DataFrame(data_3d.reshape(n_rows, -1), columns=columns)\n",
    "    \n",
    "    return df_multiindex, data_3d\n",
    "\n",
    "# Example usage\n",
    "data = {\n",
    "    \"('Basophils/100 leukocytes in Blood', -5.0)\": [1, 2, 3],\n",
    "    \"('Basophils/100 leukocytes in Blood', 0.0)\": [4, 5, 6],\n",
    "    \"('Platelets [#/volume] in Blood', -5.0)\": [7, 8, 9],\n",
    "    \"('Platelets [#/volume] in Blood', 21.0)\": [10, 11, 12]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df_multiindex, data_3d = convert_to_3d_df(df)\n",
    "\n",
    "print(\"Test:\")\n",
    "display(df)\n",
    "\n",
    "print('')\n",
    "print('Multiindex df:')\n",
    "print(df_multiindex.shape)\n",
    "display(df_multiindex)\n",
    "\n",
    "print(df_multiindex)\n",
    "\n",
    "\n",
    "print('')\n",
    "print('3D array [numpy]')\n",
    "print(data_3d.shape, type(data_3d))\n",
    "display(data_3d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert AML data to multiindex df\n",
    "df_x, data_3d = convert_to_3d_df(df_xy.iloc[:,3:].fillna(-1))\n",
    "df_y = df_xy.iloc[:,:3]\n",
    "display(df_x)\n",
    "display(df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 2 year survival\n",
    "\n",
    "def add_two_year_survival(df):\n",
    "    # Define a function to determine 2-year survival status\n",
    "    def survival_status(row):\n",
    "        if pd.isna(row['OSS_days']) or pd.isna(row['OSS_status']):\n",
    "            return np.nan\n",
    "        elif row['OSS_days'] >= 730:\n",
    "            # return 0 = alive if OSS is > years\n",
    "            return 0 \n",
    "        else:\n",
    "            # for OSS < 2 years: return 1 = dead if status = 1 (dead), nan (cencored) otherwise \n",
    "            return 1 if row['OSS_status'] == 1 else np.nan\n",
    "    \n",
    "    # Apply the function to each row\n",
    "    df['2_year_survival (1=death)'] = df.apply(survival_status, axis=1)\n",
    "    return df\n",
    "\n",
    "df_y = add_two_year_survival(df_y)\n",
    "display(df_y)\n",
    "\n",
    "# Tidy 2 year survival data (no nans):\n",
    "nanrows = df_y['2_year_survival (1=death)'].isna()\n",
    "df_x_2year = df_x[~nanrows]\n",
    "df_y_2year = df_y[~nanrows]\n",
    "data_3d_2year = data_3d[~nanrows,:,:]\n",
    "\n",
    "#print(df_y[(df_y['OSS_days']<730) & (df_y['OSS_status']==0)]) # double ckeck\n",
    "print(f\"{df_x.shape[0] - df_x_2year.shape[0]} patients with < 2 year censoring removed\")\n",
    "print(f\"{df_x_2year.shape[0]} patients remain for 2-year survival analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot survival\n",
    "import lifelines as ll\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_survival(df_y):\n",
    "    # X-axis for plotting:\n",
    "    timeline = [0, 365, 2*365, 5*365, 10*365, 15*365]\n",
    "    # Create a KaplanMeierFitter instance\n",
    "    kmf = KaplanMeierFitter()\n",
    "    # Fit the data. 1 (death) = event observed\n",
    "    kmf.fit(durations=df_y['OSS_days'], event_observed=df_y['OSS_status']==1)\n",
    "    # Plot the survival curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = kmf.plot_survival_function(show_censors=True, ci_show=True, at_risk_counts=False)\n",
    "    ll.plotting.add_at_risk_counts(kmf, ax=ax, xticks=timeline)\n",
    "    # Indicate 2-year survival\n",
    "    plt.axvline(x=5*365, color='r', linestyle='--', label='5-year mark')\n",
    "    # Annotate\n",
    "    plt.title('Overall Survival Curve')\n",
    "    ax.xaxis.set_ticks(timeline)\n",
    "    ax.set_xlabel('Time since diagnosis (days)')\n",
    "    ax.set_ylabel('Survival probability (OSS)')\n",
    "    #plt.ylabel('Survival Probability')\n",
    "    #plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_survival(df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 2 year survival\n",
    "# Dataloader from dataframe, multiindex\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df_x, df_y): #data_tensor, labels_df):\n",
    "        \n",
    "        # Convert MultiIndex DataFrame to NumPy array\n",
    "        # df should have multiindex features, timepoints\n",
    "        n_features = len(df_x.columns.levels[0])\n",
    "        n_timepoints = len(df_x.columns.levels[1])\n",
    "        data_np = df_x.values.reshape(df_x.shape[0], n_features, n_timepoints)\n",
    "        \n",
    "        # Convert NumPy array to PyTorch tensor\n",
    "        data_tensor = torch.tensor(data_np, dtype=torch.float32)\n",
    "        \n",
    "        # Keep column annotations\n",
    "        self.features = df_x.columns.levels[0]\n",
    "        self.timepoints = df_x.columns.levels[1]\n",
    "        self.samples = df_x.index\n",
    "        self.labelname = '2_year_survival (1=death)'\n",
    "        \n",
    "        self.data = data_tensor\n",
    "\n",
    "        # Keep survival data\n",
    "        self.df_x = df_x\n",
    "        self.df_y = df_y\n",
    "        \n",
    "        # Labels are 2-year survival (0=alive, 1=death)\n",
    "        # self.labels = df_y['2_year_survival (1=death)'].values\n",
    "        # Convert labels to PyTorch tensor\n",
    "        self.labels = torch.tensor(df_y['2_year_survival (1=death)'].values, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx].float() # Ensure features are float\n",
    "        label =  self.labels[idx].long() # Ensure targets are long integers for classification\n",
    "        return sample, label\n",
    "\n",
    "\n",
    "# Split the data into training and remaining sets (80% train, 20% remaining)\n",
    "X_train, X_remaining, y_train, y_remaining = train_test_split(df_x_2year, df_y_2year, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the remaining data into validation and test sets (50% validation, 50% test of the remaining 20%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_remaining, y_remaining, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print the shapes of the splits to verify\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape, y_val.shape)\n",
    "print(\"Test set shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "# dataset = CustomDataset(df_x_2year, df_y_2year)\n",
    "# dataloader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
    "dataset_train = CustomDataset(X_train, y_train)\n",
    "train_loader = DataLoader(dataset_train, batch_size=605, shuffle=True)\n",
    "dataset_val = CustomDataset(X_val, y_val)\n",
    "val_loader = DataLoader(dataset_val, batch_size=605, shuffle=False)\n",
    "dataset_test = CustomDataset(X_test, y_test)\n",
    "test_loader = DataLoader(dataset_test, batch_size=605, shuffle=False)\n",
    "\n",
    "\n",
    "x,y = dataset_test[1]\n",
    "print(x.shape)\n",
    "print(y)\n",
    "\n",
    "batch = next(iter(test_loader))\n",
    "print(batch[0].shape)\n",
    "print(batch[1].shape)\n",
    "\n",
    "# Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     for batch in dataloader:\n",
    "#         data, labels = batch\n",
    "#         print(\"Data:\", data)\n",
    "#         print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ukko)\n",
    "importlib.reload(ukko.data)\n",
    "importlib.reload(ukko.core)\n",
    "importlib.reload(ukko.test)\n",
    "\n",
    "# Create dictionary to store metrics\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "n_features = dataset_train.features.shape[0]\n",
    "timepoints = dataset_train.timepoints.shape[0]\n",
    "print(n_features)\n",
    "print(timepoints)\n",
    "\n",
    "# Quick data for testing\n",
    "dataset_train = CustomDataset(X_train, y_train)\n",
    "train_loader = DataLoader(dataset_train, batch_size=1000, shuffle=True)\n",
    "\n",
    "# Initialize model\n",
    "# DualAttentionClassifier(self, n_features, time_steps, n_classes, d_model=128, n_heads=8, dropout=0.1)\n",
    "model = ukko.core.DualAttentionClassifier1(\n",
    "    n_features=n_features,\n",
    "    time_steps=timepoints,\n",
    "    n_labels=1,  # 2-year surival\n",
    "    n_classes=2, # 2 classes: 0=alive, 1=death\n",
    "    d_model=32,\n",
    "    n_heads=4,\n",
    "    dropout=0.2,\n",
    "    n_modules=1\n",
    ")\n",
    "\n",
    "def train_sine_model(model, train_loader, val_loader, epochs=100, lr=0.001, device='cpu', verbose=True):\n",
    "    model = model.to(device)\n",
    "    model = model.float()\n",
    "    #criterion = nn.MSELoss()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            x = x.float().to(device)\n",
    "            y = y.long().to(device)\n",
    "\n",
    "            #print(f\"X: {x.shape}\")\n",
    "            #print(f\"y: {y.shape}\")\n",
    "            #print(f\"Type: {y.dtype}\")\n",
    "            y = y.long()  # Convert targets to long (required for CrossEntropyLoss)\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            output, _, _ = model(x)\n",
    "            #print(f\"Output: {output.shape}\")\n",
    "            #print(f\"Type: {output.dtype}\")\n",
    "            \n",
    "            # Squeeze n_labels dimension since we only have one label\n",
    "            output = output.squeeze(1)  # Shape: [batch_size, n_classes]\n",
    "           \n",
    "            #loss = criterion(output, y[:, :, 0])  # Compare with first time step of prediction\n",
    "            loss = criterion(output, y)  # y is only one timepoint 5 or 4 steps ahead\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.float().to(device), y.long().to(device)\n",
    "                output, _, _ = model(x)     # Shape: [batch_size, n_labels, n_classes]\n",
    "                # Becasue only 1 label:\n",
    "                output = output.squeeze(1)  # Shape: [batch_size, n_classes]\n",
    "                #val_loss += criterion(output, y[:, :, 0]).item()\n",
    "                val_loss += criterion(output, y).item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Log training progress:\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        if verbose:\n",
    "            if epoch%10==0:\n",
    "                print(f\"Epoch {epoch+1:3.0f}/{epochs}, Train Loss: {train_loss:1.4f}, Val Loss: {val_loss:1.4f}\")\n",
    "            #print(f\"Train Loss: {train_loss:1.4f}\")\n",
    "            #print(f\"Val Loss: {val_loss:1.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_sine_model.pt')\n",
    "\n",
    "# Train the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_sine_model(model, train_loader, val_loader, epochs=200, device=device)\n",
    "history1 = history\n",
    "\n",
    "# Visualize training\n",
    "ukko.core.plot_training_curves(\n",
    "    train_losses = history['train_loss'], \n",
    "    val_losses   = history['val_loss'],\n",
    "    figsize=(15, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ukko.core)\n",
    "#help(ukko.core.DualAttentionClassifier1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Good model:\n",
    "d_model=32,\n",
    "n_heads=4,\n",
    "dropout=0.2,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "ukko.core.plot_training_curves(\n",
    "    train_losses = history['train_loss'], \n",
    "    val_losses   = history['val_loss'],\n",
    "    figsize=(15, 5),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model, data_loader, device='cpu'):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x = x.float().to(device)\n",
    "            y = y.long().to(device)\n",
    "            \n",
    "            outputs, _, _ = model(x)\n",
    "            outputs = outputs.squeeze(1)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    \n",
    "    return cm\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# Load the best model\n",
    "best_model = model\n",
    "best_model.load_state_dict(torch.load('best_sine_model.pt'))\n",
    "best_model = best_model.to(device)\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"Validation Set Performance:\")\n",
    "val_cm = evaluate_model(best_model, val_loader, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
